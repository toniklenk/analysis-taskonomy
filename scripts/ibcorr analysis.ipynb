{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import os, sys, pickle\n",
    "from collections import OrderedDict\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "# stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.api import OLS\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "\n",
    "# plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# neural networks\n",
    "import torch, torch.utils.model_zoo # required to load nets\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "\n",
    "# thesis library\n",
    "from lib.functions_base_analysis        import *\n",
    "from lib.functions_second_analysis      import *\n",
    "from lib.ImageDataset                   import ImageDataset\n",
    "from lib.NetworkScorer                  import NetworkScorer\n",
    "from lib.PatternGenerator               import Pattern_Generator\n",
    "from lib.ActivationPattern              import Activation_Pattern\n",
    "from lib.transforms                     import VisualPriorRepresentation\n",
    "from lib.PatternGeneratorSearchlight    import PatternGeneratorSearchlight\n",
    "\n",
    "DATASET_NAMES               = ('places1', 'places2', 'oasis')\n",
    "SCALE_NAMES                 = ('scale2','scale4','scale8','scale16','scale32')\n",
    "STUDY_NAMES                 = (\"short presentation\", \"long presentation\", \"complexity order\")\n",
    "BEHAVIOUR_NAMES             = ('study1_places1_short.csv','study2_places1.csv','study3_places2.csv','study4_oasis.csv')\n",
    "\n",
    "PATH_IMAGES                 = '../images and ratings/imageversions_256'\n",
    "PATH_RATINGS                = '../images and ratings/ratings'\n",
    "PATH_INTEGRATION_VALUES     = '../data csv/integration' # !! correlations, invert sign for integration\n",
    "PATH_IB_CORRELATIONS        = '../data csv/ibcorr'\n",
    "PATH_IB_CORRELATIONS_BLOCKED= '../data csv/ibcorr blocked'\n",
    "\n",
    "PATH_RESULTS                = '../results'\n",
    "PATH_PLOTS                  = '../plots'\n",
    "\n",
    "#VisualPrior.viable_feature_tasks\n",
    "MODEL_NAMES = ('autoencoding','depth_euclidean','jigsaw','reshading',\n",
    "               'edge_occlusion','keypoints2d','room_layout', #'colorization' currently not working\n",
    "               'curvature','edge_texture','keypoints3d','segment_unsup2d',\n",
    "               'class_object','egomotion','nonfixated_pose','segment_unsup25d',\n",
    "               'class_scene','fixated_pose','normal','segment_semantic',\n",
    "               'denoising','inpainting','point_matching','vanishing_point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = './data_256x256'\n",
    "#BEHAVIOR_PATH = './behavior'\n",
    "#RESULTS_PATH = './data_integration_taskonomy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder structure for imageversions\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for scale_name in SCALE_NAMES:\n",
    "        for version_name in ('full','version1','version2'):\n",
    "            os.makedirs(os.path.join(PATH_IMAGES, dataset_name, scale_name, version_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder structure for results\n",
    "for model_name in MODEL_NAMES:\n",
    "    for dataset_name in DATASET_NAMES:\n",
    "        for scale_name in SCALE_NAMES:\n",
    "            dir_path = os.path.join(PATH_INTEGRATION_VALUES, model_name, dataset_name, scale_name)\n",
    "            os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder strucutre for Pytorch -> Matlab converted results\n",
    "for model_name in MODEL_NAMES:\n",
    "    os.makedirs(os.path.join('../data mat', 'ibcorr', model_name))\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    os.makedirs(os.path.join('../data mat', 'ibcorr blocked', model_name))\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    os.makedirs(os.path.join('../data mat', 'integration', model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder strucutre for Matlab -> Python converted results (analysis results)\n",
    "STUDY_NAMES = ('short presentation','long presentation','complexity order','oasis')\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    for study_name in STUDY_NAMES:\n",
    "        for scale_name in SCALE_NAMES:\n",
    "            os.makedirs(os.path.join(PATH, model_name, study_name, scale_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part-whole correlations taskonomy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in MODEL_NAMES:\n",
    "    print(model_name)\n",
    "\n",
    "    # Import taskonomy model...\n",
    "    VisualPriorRepresentation._load_unloaded_nets([model_name])\n",
    "    net = VisualPriorRepresentation.feature_task_to_net[model_name]\n",
    "    \n",
    "    # ...and create activation extractor from it\n",
    "    _, eval_nodes = get_graph_node_names(net)\n",
    "    return_nodes = { node:node for node in eval_nodes if \"conv\" in node or 'fc' in node}\n",
    "    activation_extractor = create_feature_extractor(net, return_nodes=return_nodes)\n",
    "\n",
    "\n",
    "    for dataset_name in DATASET_NAMES:\n",
    "        print(dataset_name)\n",
    "\n",
    "        for scale_name in SCALE_NAMES:\n",
    "            \n",
    "            dataset = ImageDataset(\n",
    "                os.path.join(PATH_IMAGES, dataset_name, scale_name))\n",
    "                        \n",
    "            correlations, selfsimilarity, l2norm = calculate_dataset_metrics(dataset, activation_extractor)\n",
    "            \n",
    "            correlations.fillna(correlations.mean())\n",
    "            selfsimilarity.fillna(selfsimilarity.mean())\n",
    "            l2norm.fillna(l2norm.mean())\n",
    "            \n",
    "            correlations.to_csv(os.path.join(PATH_INTEGRATION_VALUES, model_name, dataset_name, scale_name, 'correlations.csv'), index=False, header=False)\n",
    "            selfsimilarity.to_csv(os.path.join(PATH_INTEGRATION_VALUES, model_name, dataset_name, scale_name, 'selfsimilarity.csv'), index=False, header=False)           \n",
    "            l2norm.to_csv(os.path.join(PATH_INTEGRATION_VALUES, model_name, dataset_name, scale_name, 'l2norm.csv'), index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "data_list = []\n",
    "\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    for dataset_name in DATASET_NAMES:\n",
    "        for scale_name in SCALE_NAMES:\n",
    "\n",
    "            data = pd.read_csv(os.path.join(PATH_INTEGRATION_VALUES, model_name, dataset_name, scale_name, 'correlations.csv'), header=None)\n",
    "            data.insert(0, 'scale', scale_name)\n",
    "            data.insert(0, 'dataset',dataset_name)\n",
    "            data.insert(0, 'model', model_name)\n",
    "\n",
    "            data_list.append(data)\n",
    "            #selfsimilarity.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'selfsimilarity.csv'), index=False, header=False)           \n",
    "            #l2norm.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'l2norm.csv'), index=False, header=False)\n",
    "\n",
    "# convert correlation to integration\n",
    "df = - pd.concat(data_list).set_index(['model','dataset','scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block layers\n",
    "df = df.T.set_index(np.insert(np.repeat(range(2,17+1), 3), 0, 1)).T.groupby(level=0,axis=1).mean()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles, labels = df.loc['autoencoding'].groupby('scale').mean().transpose().plot().get_legend_handles_labels()\n",
    "order = [1, 3, 4, 0, 2]\n",
    "plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order], bbox_to_anchor=(1.25, 1.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load ibcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder strucutre for Matlab -> Python converted results (analysis results)\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    for study_name in STUDY_NAMES:\n",
    "        for scale_name in SCALE_NAMES:\n",
    "            os.makedirs(os.path.join('./analysis results taskonomy', model_name, study_name, scale_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ib_list, ss_list, ibss_list = [],[],[]\n",
    "ib_list = []\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    for study_name in STUDY_NAMES:\n",
    "        for scale_name in SCALE_NAMES:\n",
    "            ib = pd.read_csv(os.path.join(PATH_IB_CORRELATIONS, model_name, study_name, scale_name, 'ib_correlations.csv'), header=None).transpose()\n",
    "            ib.insert(0, 'scale', scale_name)\n",
    "            ib.insert(0, 'study',study_name)\n",
    "            ib.insert(0, 'model', model_name)\n",
    "            ib_list.append(ib)\n",
    "\n",
    "\n",
    "            # ss = pd.read_csv(os.path.join(PATH_IB_CORRELATIONS, model_name, study_name, scale_name, 'self_similarity.csv'), header=None).transpose()\n",
    "            # ss.insert(0, 'scale', scale_name)\n",
    "            # ss.insert(0, 'study',study_name)\n",
    "            # ss.insert(0, 'model', model_name)\n",
    "            # ss_list.append(ss)\n",
    "\n",
    "\n",
    "            # ibss = pd.read_csv(os.path.join(PATH_IB_CORRELATIONS, model_name, study_name, scale_name, 'ib_correlation_ss_partialed.csv'), header=None).transpose()\n",
    "            # ibss.insert(0, 'scale', scale_name)\n",
    "            # ibss.insert(0, 'study',study_name)\n",
    "            # ibss.insert(0, 'model', model_name)\n",
    "            # ibss_list.append(ibss)\n",
    "\n",
    "            #selfsimilarity.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'selfsimilarity.csv'), index=False, header=False)           \n",
    "            #l2norm.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'l2norm.csv'), index=False, header=False)\n",
    "\n",
    "ib_df = pd.concat(ib_list).set_index(['model','study','scale']).rename(columns=lambda x:x+1)\n",
    "# ss_df = pd.concat(ss_list).set_index(['model','study','scale']).rename(columns=lambda x:x+1)\n",
    "# ibss_df = pd.concat(ibss_list).set_index(['model','study','scale']).rename(columns=lambda x:x+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
