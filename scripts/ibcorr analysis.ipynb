{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import os, sys, pickle\n",
    "from itertools import combinations, combinations_with_replacement, product\n",
    "\n",
    "# stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# neural networks\n",
    "import torch, torch.utils.model_zoo  # required to load nets\n",
    "from torchvision.models.feature_extraction import (\n",
    "    get_graph_node_names,\n",
    "    create_feature_extractor,\n",
    ")\n",
    "\n",
    "# thesis library\n",
    "from lib.functions_base_analysis import *\n",
    "from lib.functions_second_analysis import *\n",
    "from lib.functions_scripting import *\n",
    "from lib.ImageDataset import ImageDataset\n",
    "from lib.NetworkScorer import NetworkScorer\n",
    "from lib.PatternGenerator import Pattern_Generator\n",
    "from lib.ActivationPattern import Activation_Pattern\n",
    "from lib.transforms import VisualPriorRepresentation\n",
    "\n",
    "\n",
    "PATH_IMAGES = \"../images and ratings/imageversions_256\"\n",
    "PATH_RATINGS = \"../images and ratings/ratings\"\n",
    "\n",
    "PATH_INTEGRATION = \"../data csv/integration\"\n",
    "PATH_INTEGRATION_AVERAGE = \"../data csv/integration average\"\n",
    "PATH_INTEGRATION_MAXPOOL = \"../data csv/integration maxpool\"\n",
    "\n",
    "PATH_IBCORR = \"../data csv/ibcorr\"\n",
    "PATH_IBCORR_AVERAGE = \"../data csv/ibcorr average\"\n",
    "PATH_IBCORR_MAXPOOL = \"../data csv/ibcorr maxpool\"\n",
    "\n",
    "PATH_RESULTS = \"../results\"\n",
    "PATH_PLOTS = \"../plots\"\n",
    "\n",
    "\n",
    "# uniform blocking across this script to avoid bugs\n",
    "BLOCKING_GROUPS = np.insert(np.repeat(range(1, 16 + 1), 3), 0, 0)\n",
    "\n",
    "\n",
    "_study = \"short presentation\"\n",
    "_scale = \"scale16\"\n",
    "_dataset = \"places1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ibcorr analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in MODEL_NAMES:\n",
    "    print(model_name)\n",
    "\n",
    "    # Import taskonomy model...\n",
    "    VisualPriorRepresentation._load_unloaded_nets([model_name])\n",
    "    net = VisualPriorRepresentation.feature_task_to_net[model_name]\n",
    "\n",
    "    # ...and create activation extractor from it\n",
    "    _, eval_nodes = get_graph_node_names(net)\n",
    "    return_nodes = {node: node for node in eval_nodes if \"conv\" in node or \"fc\" in node}\n",
    "    activation_extractor = create_feature_extractor(net, return_nodes=return_nodes)\n",
    "\n",
    "    for dataset_name in DATASET_NAMES:\n",
    "        print(dataset_name)\n",
    "\n",
    "        for scale_name in SCALE_NAMES:\n",
    "\n",
    "            dataset = ImageDataset(os.path.join(PATH_IMAGES, dataset_name, scale_name))\n",
    "\n",
    "            correlations, selfsimilarity, l2norm = calculate_dataset_metrics(\n",
    "                dataset, activation_extractor\n",
    "            )\n",
    "\n",
    "            correlations.fillna(correlations.mean())\n",
    "            selfsimilarity.fillna(selfsimilarity.mean())\n",
    "            l2norm.fillna(l2norm.mean())\n",
    "\n",
    "            correlations.to_csv(\n",
    "                os.path.join(\n",
    "                    PATH_INTEGRATION,\n",
    "                    model_name,\n",
    "                    dataset_name,\n",
    "                    scale_name,\n",
    "                    \"correlations.csv\",\n",
    "                ),\n",
    "                index=False,\n",
    "                header=False,\n",
    "            )\n",
    "            selfsimilarity.to_csv(\n",
    "                os.path.join(\n",
    "                    PATH_INTEGRATION,\n",
    "                    model_name,\n",
    "                    dataset_name,\n",
    "                    scale_name,\n",
    "                    \"selfsimilarity.csv\",\n",
    "                ),\n",
    "                index=False,\n",
    "                header=False,\n",
    "            )\n",
    "            l2norm.to_csv(\n",
    "                os.path.join(\n",
    "                    PATH_INTEGRATION, model_name, dataset_name, scale_name, \"l2norm.csv\"\n",
    "                ),\n",
    "                index=False,\n",
    "                header=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load, average, maxpool, create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = load_integration(PATH_INTEGRATION)\n",
    "dfi_a = load_integration(PATH_INTEGRATION_AVERAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_ibc = load_ibcorr(PATH_IBCORR)\n",
    "#df_ibc_b = load_ibcorr(PATH_IBCORR_AVERAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = load_pvalues(PATH_IBCORR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integration\n",
    "dfi_a = dfi.unstack(\"layer\").groupby(BLOCKING_GROUPS, axis=1).mean()\n",
    "#dfi_a.groupby(\"dataset\").mean().T.plot() # control sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc_a = -dfi_a\n",
    "for da, sc, mo in product(DATASET_NAMES, SCALE_NAMES, MODEL_NAMES):\n",
    "    dfc_a.loc[(mo, da, sc)].to_csv(\n",
    "        os.path.join(PATH_INTEGRATION_AVERAGE, mo, da, sc, \"correlations.csv\"),\n",
    "        index=False,\n",
    "        header=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ibc\n",
    "df_ibc_a = df_ibc.unstack(\"layer\").groupby(BLOCKING_GROUPS, axis=1).mean().stack().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for st, sc, mo in product(STUDY_NAMES, SCALE_NAMES, MODEL_NAMES):\n",
    "    df_ibc_a.loc[(mo, st, sc)].to_csv(\n",
    "        os.path.join(PATH_IBCORR_AVERAGE, mo, da, sc, \"ib_correlations.csv\"),\n",
    "        index=False,\n",
    "        header=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ibc\n",
    "df_ibc_m = (\n",
    "    df_ibc.unstack(\"layer\")\n",
    "    .groupby(BLOCKING_GROUPS, axis=1)\n",
    "    .max()\n",
    "    .stack()\n",
    "    .to_frame()\n",
    "    .rename({0: \"ibcorr\"}, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for st, sc, mo in product(STUDY_NAMES, SCALE_NAMES, MODEL_NAMES):\n",
    "    df_ibc_m.loc[(mo, st, sc)].to_csv(\n",
    "        os.path.join(PATH_IBCORR_MAXPOOL, mo, st, sc, \"ib_correlations.csv\"),\n",
    "        index=False,\n",
    "        header=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integration\n",
    "def maxpool_integration(__study):\n",
    "    \"\"\"df_ibc and dfi need to be fully stacked in the workspace (have only 1 column)\"\"\"\n",
    "\n",
    "    bestlayers = (\n",
    "        df_ibc.unstack(\"layer\")\n",
    "        .T.droplevel(None)\n",
    "        .groupby(BLOCKING_GROUPS)\n",
    "        .idxmax()\n",
    "        .T.reorder_levels([\"study\", \"scale\", \"model\"])\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    bestlayers.columns.name = \"layer\"\n",
    "    bestlayers = (\n",
    "        bestlayers.stack(\"layer\")\n",
    "        .loc[__study]\n",
    "        .reset_index()\n",
    "        .set_index(\"layer\")\n",
    "        .values.tolist()\n",
    "    )\n",
    "\n",
    "    dfi_study = (\n",
    "        dfi.reorder_levels([\"dataset\", \"scale\", \"model\", \"img\", \"layer\"])\n",
    "        .loc[studyname2datasetname(__study)]\n",
    "        .unstack(\"img\")\n",
    "        .loc[bestlayers, :]\n",
    "    )\n",
    "\n",
    "    new_layer_idx = dfi_study.groupby([\"scale\", \"model\"]).cumcount()\n",
    "\n",
    "    dfi_study = (\n",
    "        dfi_study.assign(blocklayer=new_layer_idx)\n",
    "        .reset_index()\n",
    "        .drop(\"layer\", axis=1)\n",
    "        .rename({\"blocklayer\": \"layer\"}, axis=1)\n",
    "        .set_index([\"scale\", \"model\", \"layer\"])\n",
    "        .unstack(\"layer\")\n",
    "        .stack(\"img\")\n",
    "    )\n",
    "\n",
    "    return dfi_study\n",
    "\n",
    "\n",
    "for st in STUDY_NAMES:\n",
    "    # save as correlation, not integration\n",
    "    dfc = -maxpool_integration(st)\n",
    "    da = studyname2datasetname(st)\n",
    "\n",
    "    for mo, sc in product(MODEL_NAMES, SCALE_NAMES):\n",
    "        dfc.loc[(sc, mo)].to_csv(\n",
    "            os.path.join(PATH_INTEGRATION_MAXPOOL, mo, da, sc, \"correlations.csv\"),\n",
    "            index=False,\n",
    "            header=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi_m = load_integration(PATH_INTEGRATION_MAXPOOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_integration(PATH_INTEGRATION_MAXPOOL).unstack(\"layer\").groupby([\"scale\"]).mean().T.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imageversions\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    for scale_name in SCALE_NAMES:\n",
    "        for version_name in (\"full\", \"version1\", \"version2\"):\n",
    "            os.makedirs(\n",
    "                os.path.join(PATH_IMAGES, dataset_name, scale_name, version_name)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data csv\n",
    "# integration\n",
    "for model_name in MODEL_NAMES:\n",
    "    for dataset_name in DATASET_NAMES:\n",
    "        for scale_name in SCALE_NAMES:\n",
    "            dir_path = os.path.join(\n",
    "                PATH_INTEGRATION_MAXPOOL, model_name, dataset_name, scale_name\n",
    "            )\n",
    "            os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ibcorr\n",
    "for model_name in MODEL_NAMES:\n",
    "    for study_name in STUDY_NAMES:\n",
    "        for scale_name in SCALE_NAMES:\n",
    "            os.makedirs(\n",
    "                os.path.join(PATH_IBCORR_MAXPOOL, model_name, study_name, scale_name)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data mat\n",
    "for model_name in MODEL_NAMES:\n",
    "    os.makedirs(os.path.join(\"../data mat\", \"ibcorr\", model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in MODEL_NAMES:\n",
    "    os.makedirs(os.path.join(\"../data mat\", \"ibcorr blocked\", model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in MODEL_NAMES:\n",
    "    os.makedirs(os.path.join(\"../data mat\", \"ibcorr blocked_integr\", model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in MODEL_NAMES:\n",
    "    os.makedirs(os.path.join(\"../data mat\", \"integration\", model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in MODEL_NAMES:\n",
    "    os.makedirs(os.path.join(\"../data mat\", \"integration blocked\", model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initial results & scale selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # unblocked ibcorr\n",
    "# df1 = (\n",
    "#     df_ibc.loc[(slice(None), _study, slice(None)), :]\n",
    "#     .droplevel([\"study\", \"scale\"])\n",
    "#     .fillna(0)\n",
    "# )\n",
    "\n",
    "# # unblocked integration\n",
    "# df2 = (\n",
    "#     dfi.loc[(slice(None), _dataset, _scale), :]\n",
    "#     .droplevel([\"dataset\", \"scale\"])\n",
    "#     .fillna(0)\n",
    "# )\n",
    "\n",
    "# # unblocked pvalues\n",
    "# df3 = (\n",
    "#     dfp.loc[(slice(None), _study, slice(None)), :]\n",
    "#     .droplevel([\"study\", \"scale\"])\n",
    "#     .fillna(0)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot only significant pixels, bonferroni MCC\n",
    "adjusted_pvalue = 0.05 / 49\n",
    "df_ibc_s = df_ibc.copy()\n",
    "df_ibc_s.iloc[(dfp > adjusted_pvalue).values] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better visualizaiton, sort models by integration\n",
    "# order = dfi.abs().groupby(\"model\").sum().sum(axis=1).argsort()\n",
    "order = df_ibc.abs().groupby(\"model\").sum().sum(axis=1).argsort()\n",
    "\n",
    "# model names to numbers mapping\n",
    "modelnum_mapping = order.sort_values()\n",
    "modelnum_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale x study grid of all-net heatmaps\n",
    "fig, axes = plt.subplots(\n",
    "    3, 2, figsize=(24, 24), sharex=True, sharey=True, constrained_layout=True\n",
    ")\n",
    "x_ticks = np.concatenate(\n",
    "    [np.arange(0, df_ibc.columns[-1], 10), np.array([df_ibc.columns[-1]])]\n",
    ")\n",
    "title_fontsize = 45\n",
    "axislabel_fontsize = 12\n",
    "\n",
    "\n",
    "for axidx, scale in enumerate(SCALE_NAMES):\n",
    "    ax = axes[axidx % 3, axidx // 3]\n",
    "\n",
    "    c = df_ibc_s.loc[(slice(None), _study, scale),].iloc[order[::-1], :]\n",
    "    im = ax.imshow(c, vmin=df_ibc_s.min().min(), vmax=df_ibc_s.max().max(), cmap=\"BuPu\")\n",
    "\n",
    "    # p = df3.loc[(slice(None), study, scale), :].droplevel([\"scale\", \"study\"])\n",
    "    # ax.contour(p, levels=[0.05], colors='orange', linewidths=5, corner_mask = False)\n",
    "\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_yticks(modelnum_mapping)\n",
    "    ax.set_yticklabels(modelnum_mapping[::-1], fontsize=axislabel_fontsize)\n",
    "    ax.set_title(scale, fontsize=title_fontsize)\n",
    "\n",
    "fig.colorbar(im, ax=axes, orientation=\"vertical\", fraction=0.2, pad=0.03, shrink=0.5)\n",
    "# plt.subplots_adjust(hspace=0.2, wspace=.005)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale x study grid of all-net heatmaps\n",
    "fig, axes = plt.subplots(\n",
    "    5, 3, figsize=(24, 24), sharex=True, sharey=True, constrained_layout=True\n",
    ")\n",
    "x_ticks = np.concatenate(\n",
    "    [np.arange(0, df_ibc.columns[-1], 10), np.array([df_ibc.columns[-1]])]\n",
    ")\n",
    "title_fontsize = 30\n",
    "axislabel_fontsize = 12\n",
    "\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list(\"blue_magenta\", [\"blue\", \"magenta\"])\n",
    "\n",
    "for idsc, scale in enumerate(SCALE_NAMES):\n",
    "    for idst, study in enumerate(STUDY_NAMES[:-1]):\n",
    "        ax = axes[idsc, idst]\n",
    "\n",
    "        c = df_ibc_s.loc[(slice(None), study, scale),].iloc[order[::-1], :]\n",
    "        im = ax.imshow(\n",
    "            c, vmin=df_ibc_s.min().min(), vmax=df_ibc_s.max().max(), cmap=\"BuPu\"\n",
    "        )\n",
    "\n",
    "        # p = df3.loc[(slice(None), study, scale), :].droplevel([\"scale\", \"study\"])\n",
    "        # ax.contour(p, levels=[0.05], colors='orange', linewidths=5, corner_mask = False)\n",
    "\n",
    "        ax.set_xticks(x_ticks)\n",
    "        ax.set_yticks(modelnum_mapping)\n",
    "        ax.set_yticklabels(modelnum_mapping[::-1], fontsize=axislabel_fontsize)\n",
    "        ax.set_title(study + \" \" + scale)\n",
    "\n",
    "fig.colorbar(im, ax=axes, orientation=\"vertical\", fraction=0.2, pad=0.03, shrink=0.5)\n",
    "# plt.subplots_adjust(hspace=0.2, wspace=.005)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation of best predicting layer in each network, scale4 vs scale16\n",
    "title_fontsize = 30\n",
    "axislabel_fontsize = 20\n",
    "\n",
    "ax = (\n",
    "    df_ibc.loc[(slice(None), _study, [\"scale4\", \"scale16\"])]\n",
    "    .droplevel(\"study\")\n",
    "    .max(axis=1)\n",
    "    .unstack(\"scale\")\n",
    "    .iloc[order, :]\n",
    "    .plot.bar(stacked=False, color={\"scale4\": \"magenta\", \"scale16\": \"blue\"})\n",
    ")\n",
    "ax.set_ylabel(\"maximum integration-beauty-correlation\")\n",
    "ax.set_xticklabels(modelnum_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blocking (average vs maxpool)\n",
    "for  scale4 (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot1: show oscillations\n",
    "explain why blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfi.loc[(slice(None), \"places1\", \"scale16\"), :].droplevel([\"scale\", \"dataset\"])\n",
    "df2 = df_ibcorr.loc[(slice(None), \"short presentation\", \"scale16\"), :].droplevel(\n",
    "    [\"scale\", \"study\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by std of specific layers\n",
    "df_maxidx = (\n",
    "    df1.iloc[:, slice(27, 43 + 1)]\n",
    "    .std(axis=1)\n",
    "    .groupby(\"model\")\n",
    "    .idxmax()\n",
    "    .apply(lambda tup: tup[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by std in all layers\n",
    "df_maxidx = df1.std(axis=1).groupby(\"model\").idxmax().apply(lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integration\n",
    "fig, axes = plt.subplots(1, 3, figsize=(33, 23), sharey=True)\n",
    "layers = slice(0, 49)  # first to last layers to plot\n",
    "yscaling = 3\n",
    "axtitle_fontsize = 38\n",
    "axeslabel_fontsize = 30\n",
    "x_ticks = np.concatenate([np.arange(0, data.index[-1], 10), np.array([data.index[-1]])])\n",
    "\n",
    "# individual images\n",
    "ax = axes[0]\n",
    "for y, model in enumerate(MODEL_NAMES):\n",
    "    maxidx = df_maxidx.loc[model]\n",
    "    data = df1.loc[(model, maxidx), :].iloc[layers]\n",
    "    ax.plot(yscaling * data + 2 * y + 3)\n",
    "    ax.axhline(\n",
    "        y=2 * y + 1, color=\"grey\", linestyle=\"--\", linewidth=0.5, xmin=0.05, xmax=0.95\n",
    "    )\n",
    "for layer in range(27, 45, 3):\n",
    "    ax.axvline(layer, color=\"thistle\", ymin=0.01, ymax=0.99)\n",
    "ax.set_title(\"single image integration highest std.\", fontsize=axtitle_fontsize)\n",
    "ax.set_xlabel(\"network layer\", fontsize=axeslabel_fontsize)\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_ticks, fontsize=axeslabel_fontsize)\n",
    "# shared y-axis\n",
    "ax.set_ylim(0, 23 * 2 + 3)\n",
    "ax.set_yticks(range(1, 23 * 2, 2))\n",
    "ax.set_yticklabels(range(1, 23 + 1), fontsize=axeslabel_fontsize)\n",
    "ax.set_ylabel(\"taskonomy network (see dict below)\", fontsize=axeslabel_fontsize)\n",
    "\n",
    "\n",
    "# network median\n",
    "ax = axes[1]\n",
    "tmp = df1.groupby(\"model\").median()\n",
    "for y, model in enumerate(MODEL_NAMES):\n",
    "    ax.plot(yscaling * tmp.iloc[y, layers] + 2 * y + 3)\n",
    "    ax.axhline(\n",
    "        y=2 * y + 1, color=\"grey\", linestyle=\"--\", linewidth=0.5, xmin=0.05, xmax=0.95\n",
    "    )\n",
    "for layer in range(27, 45, 3):\n",
    "    ax.axvline(layer, color=\"thistle\", ymin=0.01, ymax=0.99)\n",
    "ax.set_title(\"mean spatial integration\", fontsize=axtitle_fontsize)\n",
    "ax.set_xlabel(\"network layer\", fontsize=axeslabel_fontsize)\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_ticks, fontsize=axeslabel_fontsize)\n",
    "\n",
    "\n",
    "# ibcorr\n",
    "ax = axes[2]\n",
    "for y, model in enumerate(MODEL_NAMES):\n",
    "    ax.plot(2 * yscaling * df2.loc[model] + 2 * y)\n",
    "    ax.axhline(\n",
    "        y=2 * y + 1, color=\"grey\", linestyle=\"--\", linewidth=0.5, xmin=0.05, xmax=0.95\n",
    "    )\n",
    "for layer in range(26, 45, 3):\n",
    "    ax.axvline(layer, color=\"thistle\", ymin=0.01, ymax=0.99)\n",
    "ax.set_title(\"integration-beauty correlation\", fontsize=axtitle_fontsize)\n",
    "ax.set_xlabel(\"network layer\", fontsize=axeslabel_fontsize)\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_ticks, fontsize=axeslabel_fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ibcorr\n",
    "# unblocked view of scale16\n",
    "\n",
    "tmp = df2.loc[(slice(None), study, \"scale16\"), :]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 23))\n",
    "\n",
    "\n",
    "# for idst, study in enumerate(STUDY_NAMES):\n",
    "#     corr= df_ib.loc[(slice(None),study, 'scale16'),:].droplevel(['scale','study'])\n",
    "#     p   = df_p.loc[(slice(None), study,scale),:].droplevel(['scale','study'])\n",
    "#     ax = axes[idst]\n",
    "\n",
    "#     ax.set_yticks(range(len(corr.index)))\n",
    "#     #ax.set_yticklabels(corr.index)\n",
    "#     ax.set_title(study + ' ' + scale)\n",
    "\n",
    "for y, model in enumerate(MODEL_NAMES):\n",
    "    ax.plot(10 * tmp.iloc[y, slice(None)] + 2 * y + 1)\n",
    "    ax.axhline(y=2 * y + 1, color=\"grey\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "ax.set_ylim(-3, 23 * 2 + 3)\n",
    "ax.set_yticks(range(1, 23 * 2, 2))\n",
    "ax.set_yticklabels(range(1, 23 + 1))\n",
    "\n",
    "# Plot each line with its offset\n",
    "# ax.plot(x, y1, label='sin(x)')\n",
    "# ax.plot(x, y2_offset, label=f'cos(x) + {offset}')\n",
    "# ax.plot(x, y3_offset, label=f'exp(x/10) + {2 * offset}')\n",
    "\n",
    "# # Optionally, add horizontal lines to indicate the offset\n",
    "# ax.axhline(y=0, color='grey', linestyle='--', linewidth=0.5)\n",
    "# ax.axhline(y=offset, color='grey', linestyle='--', linewidth=0.5)\n",
    "# ax.axhline(y=2 * offset, color='grey', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# # Add legends, labels, and title\n",
    "# ax.legend()\n",
    "# ax.set_xlabel('X-axis')\n",
    "# ax.set_ylabel('Y-axis')\n",
    "# ax.set_title('Multiple Lines with Y-Offset')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot2.1: inside block layer structure of oscillations\n",
    "resnet50 architecture explained before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot2.2: ibcorr difference from blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unblocked ibcorr\n",
    "df1 = df_ibc.loc[(slice(None), \"short presentation\", _scale)].fillna(0)\n",
    "\n",
    "\n",
    "# unblocked integration\n",
    "df3 = dfi.loc[(slice(None), \"places1\", _scale)].fillna(0)\n",
    "\n",
    "\n",
    "# maxpool ibcorr\n",
    "df5 = df1.T.groupby(BLOCKING_GROUPS).max().T\n",
    "\n",
    "# maxpool integration\n",
    "# per block layer with best beauty prediction\n",
    "bl = df1.T.groupby(BLOCKING_GROUPS).idxmax().T\n",
    "bl_stacked = (\n",
    "    bl.stack()\n",
    "    .droplevel(None)\n",
    "    .to_frame()\n",
    "    .rename({0: \"layer\"}, axis=1)\n",
    "    .reset_index()\n",
    "    .values\n",
    ")\n",
    "\n",
    "# format d3 for indexing\n",
    "df3.columns.name = \"layer\"\n",
    "df3_stacked = df3.stack(\"layer\").reorder_levels([\"model\", \"layer\", \"img\"]).to_frame()\n",
    "\n",
    "# indices for indexing\n",
    "num_blocks = np.unique(BLOCKING_GROUPS).size\n",
    "num_images = df3.index.get_level_values(\"img\").unique().size\n",
    "num_models = df3.index.get_level_values(\"model\").unique().size\n",
    "blocked_layer_idx = np.tile(\n",
    "    np.repeat(\n",
    "        np.arange(num_blocks),\n",
    "        num_images,\n",
    "    ),\n",
    "    num_models,\n",
    ")\n",
    "\n",
    "# index best layer per block\n",
    "df6 = (\n",
    "    df3_stacked.reset_index(level=\"img\")\n",
    "    .loc[bl_stacked.tolist(), :]\n",
    "    .droplevel(\"layer\")\n",
    "    .assign(layer=blocked_layer_idx)\n",
    "    .reset_index()\n",
    "    .set_index([\"model\", \"img\", \"layer\"])\n",
    "    .unstack(\"layer\")\n",
    "    .droplevel(None, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save maxpooled\n",
    "# for now, this is only for data subset\n",
    "\n",
    "# integration\n",
    "for mo, da, sc in product(MODEL_NAMES, [_dataset], [_scale]):\n",
    "    df6.loc[(mo, slice(None))].to_csv(\n",
    "        os.path.join(PATH_INTEGRATION_MAXPOOL, mo, da, sc, \"correlations.csv\"),\n",
    "        header=None,\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# ibcorr\n",
    "for mo, st, sc in product(MODEL_NAMES, [_study], [_scale]):\n",
    "    df5.loc[mo].to_csv(\n",
    "        os.path.join(PATH_IBCORR_MAXPOOL, mo, st, sc, \"ib_correlations.csv\"),\n",
    "        header=None,\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's better: significant difference between average and max\n",
    "df_s = pd.DataFrame(index=MODEL_NAMES, columns=df3.columns)\n",
    "for mo in MODEL_NAMES:\n",
    "    for layer in df2.columns:\n",
    "        df_s.loc[mo, layer] = compare_corr(\n",
    "            r_yx1=df2.loc[mo, layer],  # avg_ibc\n",
    "            r_yx2=df5.loc[mo, layer],  # max_ibc\n",
    "            X1=df4.loc[(mo, slice(None)), layer],  # avg_i\n",
    "            X2=df6.loc[(mo, slice(None)), layer],  # max_i\n",
    "        )\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "alpha_mcc = (0.05 / 17) * 2\n",
    "\n",
    "upper = (df_s > norm.ppf(1 - alpha_mcc / 2)).sum().sum()\n",
    "lower = (df_s < norm.ppf(alpha_mcc / 2)).sum().sum()\n",
    "\n",
    "print(f\"There are {upper+lower} layers in all networks with a significant difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp, yp = 4, 6\n",
    "fig, axes = plt.subplots(yp, xp, figsize=(40, 60), sharex=True, sharey=True)\n",
    "axtitle_fontsize = 50\n",
    "axeslabel_fontsize = 40\n",
    "linewidth = 4.5\n",
    "x_ticks = np.concatenate(\n",
    "    [np.arange(0, df1.columns[-1], 10), np.array([df1.columns[-1]])]\n",
    ")\n",
    "\n",
    "# plot the blocked value in the middle index of the unblocked values, eg 2, 2, 3, ->3<-, 3, 4, 4, ...\n",
    "block_plot_indices = [0] + list(range(2, 49, 3))\n",
    "block_plot_indices\n",
    "\n",
    "for ax, mo, idx in zip(axes.flat, MODEL_NAMES, range(len(MODEL_NAMES))):\n",
    "    ax.axhline(0, color=\"grey\", linestyle=\"--\", linewidth=0.5, xmin=0.05, xmax=0.95)\n",
    "    ax.plot(\n",
    "        df1.loc[mo], alpha=0.85, c=\"lightblue\", linewidth=linewidth\n",
    "    )  # unblocked ibcorr\n",
    "    ax.plot(\n",
    "        block_plot_indices,\n",
    "        df5.loc[mo],\n",
    "        label=\"maximum corr.\",\n",
    "        c=\"magenta\",\n",
    "        linewidth=linewidth,\n",
    "    )  # max ibcorr\n",
    "    ax.plot(\n",
    "        block_plot_indices,\n",
    "        df2.loc[mo],\n",
    "        label=\"corr. from avg int.\",\n",
    "        c=\"blue\",\n",
    "        linewidth=linewidth,\n",
    "    )  # avg ibcorr\n",
    "\n",
    "    ax.set_title(mo, fontsize=axtitle_fontsize)\n",
    "\n",
    "    if idx % xp == 0:\n",
    "        ax.set_ylabel(\"correlation\", fontsize=axeslabel_fontsize)\n",
    "        ax.tick_params(axis=\"y\", labelsize=axeslabel_fontsize)\n",
    "\n",
    "    if idx > 19:\n",
    "        ax.set_xlabel(\"network layer\", fontsize=axeslabel_fontsize)\n",
    "        ax.set_xticks(x_ticks)\n",
    "        ax.set_xticklabels(x_ticks, fontsize=axeslabel_fontsize)\n",
    "\n",
    "    # plot significant difference between avg and max and indicate which's better\n",
    "    # ax.\n",
    "\n",
    "    # ax.legend(title=\"correlation with ... integration\", loc=\"lower center\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax = axes.flat[-1]\n",
    "ax.legend(\n",
    "    handles, labels, fontsize=45, loc=\"center\", title=\"blockwise\", title_fontsize=60\n",
    ")\n",
    "ax.set_xticks([])\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "for s in ax.spines.values():\n",
    "    s.set_visible(False)\n",
    "# for ax in axes.flat[len(MODEL_NAMES) :]:\n",
    "#    ax.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predictive layers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unblocked ibcorr\n",
    "df1 = df_ibc.loc[(slice(None), \"short presentation\", _scale)].fillna(0)\n",
    "\n",
    "# unblocked integration\n",
    "df2 = dfi.loc[(slice(None), \"places1\", _scale)].fillna(0)\n",
    "# format d2 for indexing\n",
    "df2.columns.name = \"layer\"\n",
    "df2_stacked = df2.stack(\"layer\").reorder_levels([\"model\", \"layer\", \"img\"]).to_frame()\n",
    "\n",
    "# maxpooled ibcorr\n",
    "df3 = df1.T.groupby(BLOCKING_GROUPS).max().T\n",
    "\n",
    "# per block layer with best beauty prediction\n",
    "bl = df1.T.groupby(BLOCKING_GROUPS).idxmax().T\n",
    "bl_stacked = (\n",
    "    bl.stack()\n",
    "    .droplevel(None)\n",
    "    .to_frame()\n",
    "    .rename({0: \"layer\"}, axis=1)\n",
    "    .reset_index()\n",
    "    .values\n",
    ")\n",
    "# indices for indexing\n",
    "num_blocks = np.unique(BLOCKING_GROUPS).size\n",
    "num_images = df2.index.get_level_values(\"img\").unique().size\n",
    "num_models = df2.index.get_level_values(\"model\").unique().size\n",
    "blocked_layer_idx = np.tile(\n",
    "    np.repeat(\n",
    "        np.arange(num_blocks),\n",
    "        num_images,\n",
    "    ),\n",
    "    num_models,\n",
    ")\n",
    "\n",
    "# maxpool integration  (index best layer per block)\n",
    "df4 = (\n",
    "    df2_stacked.reset_index(level=\"img\")\n",
    "    .loc[bl_stacked.tolist(), :]\n",
    "    .droplevel(\"layer\")\n",
    "    .assign(layer=blocked_layer_idx)\n",
    "    .reset_index()\n",
    "    .set_index([\"model\", \"img\", \"layer\"])\n",
    "    .unstack(\"layer\")\n",
    "    .droplevel(None, axis=1)\n",
    ")\n",
    "\n",
    "# unblocked pvalues\n",
    "df5 = dfp.loc[(slice(None), \"short presentation\", _scale)].fillna(0)\n",
    "\n",
    "\n",
    "blocked_layer_idx = np.tile(\n",
    "    np.arange(num_blocks),\n",
    "    num_models,\n",
    ")\n",
    "# maxpooled pvalues\n",
    "df6 = (\n",
    "    df5.stack(\"layer\")\n",
    "    .loc[bl_stacked.tolist(), :]\n",
    "    .droplevel(\"layer\")\n",
    "    .assign(layer=blocked_layer_idx)\n",
    "    .reset_index()\n",
    "    .set_index([\"model\", \"layer\"])\n",
    "    .unstack(\"layer\")\n",
    "    .droplevel(None, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot only significant pixels, bonferroni MCC\n",
    "adjusted_pvalue = 0.05 / 17\n",
    "mask = (df6 > adjusted_pvalue).values\n",
    "df3.iloc[mask] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better visualizaiton, sort models by integration\n",
    "# order = dfi.abs().groupby(\"model\").sum().sum(axis=1).argsort()\n",
    "order = df_ibc.abs().groupby(\"model\").sum().sum(axis=1).argsort()\n",
    "\n",
    "# model names to numbers mapping\n",
    "modelnum_mapping = order.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale x study grid of all-net heatmaps\n",
    "fig, ax = plt.subplots(\n",
    "    1,\n",
    "    1,\n",
    "    figsize=(13, 8),  # constrained_layout=False\n",
    ")\n",
    "x_ticks = np.concatenate(\n",
    "    [np.arange(0, df3.columns[-1], 6), np.array([df3.columns[-1]])]\n",
    ")\n",
    "title_fontsize = 40\n",
    "axislabel_fontsize = 20\n",
    "\n",
    "\n",
    "c = df3.iloc[order[::-1], :]\n",
    "im = ax.imshow(c, vmin=c.min().min(), vmax=c.max().max(), cmap=\"BuPu\", aspect=\"auto\")\n",
    "\n",
    "# annotations\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(x_ticks, fontsize=axislabel_fontsize)\n",
    "ax.set_xlabel(\"layer\", fontsize=axislabel_fontsize)\n",
    "ax.set_yticks(modelnum_mapping)\n",
    "ax.set_yticklabels(modelnum_mapping[::-1], fontsize=axislabel_fontsize)\n",
    "ax.set_ylabel(\"model\", fontsize=axislabel_fontsize)\n",
    "ax.set_title(_scale, fontsize=title_fontsize)\n",
    "ax.grid(which=\"major\")\n",
    "fig.colorbar(im, ax=ax, orientation=\"vertical\", fraction=0.2, pad=0.03, shrink=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### further plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare this to selected networks, scale, dataset, layers\n",
    "df_ibcorr.loc[(slice(None), \"short presentation\", slice(None)), :].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate column means and standard deviations\n",
    "means = df3.mean()\n",
    "stds = df3.std()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(means.index, means, label=\"Mean\", color=\"blue\")\n",
    "\n",
    "# Add shaded area (std) around the mean\n",
    "plt.fill_between(\n",
    "    means.index,\n",
    "    means - stds,\n",
    "    means + stds,\n",
    "    color=\"blue\",\n",
    "    alpha=0.15,\n",
    "    label=\"Â±1 Std Dev\",\n",
    ")\n",
    "\n",
    "## Add hatching to the shaded area\n",
    "# plt.fill_between(means.index, means - stds, means + stds, color='none', hatch='/', edgecolor='blue', alpha=0.3)\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Column Means with Shaded Standard Deviation\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test: heatmap significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.pad(np.random.random((10, 10)), 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.pad(np.random.random((10, 10)), 1)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16, 16))\n",
    "\n",
    "# Example significance mask (True for significant, False for not)\n",
    "# Let's assume that any value greater than 0.8 is significant\n",
    "significance_mask = data > 0.8\n",
    "\n",
    "# Plotting the heatmap\n",
    "ax[0, 0].imshow(data, cmap=\"viridis\")\n",
    "\n",
    "# Overlay the contours for significance\n",
    "ax[0, 0].contour(data, levels=[0.5], colors=\"orange\", linewidths=5, corner_mask=False)\n",
    "\n",
    "# Add colorbar\n",
    "# plt.colorbar()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
