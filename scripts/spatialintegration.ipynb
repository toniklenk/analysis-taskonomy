{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial integration analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import os, sys, pickle\n",
    "from typing import Dict\n",
    "from collections import OrderedDict\n",
    "from itertools import product, combinations, combinations_with_replacement\n",
    "\n",
    "# stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.api import OLS\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "\n",
    "# plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# neural networks\n",
    "import torch, torch.utils.model_zoo # required to load nets\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "\n",
    "# thesis library\n",
    "from lib.functions_base_analysis        import *\n",
    "from lib.functions_second_analysis      import *\n",
    "from lib.ImageDataset                   import ImageDataset\n",
    "from lib.NetworkScorer                  import NetworkScorer\n",
    "from lib.PatternGenerator               import Pattern_Generator\n",
    "from lib.ActivationPattern              import Activation_Pattern\n",
    "from lib.transforms                     import VisualPriorRepresentation\n",
    "from lib.PatternGeneratorSearchlight    import PatternGeneratorSearchlight\n",
    "\n",
    "DATASET_NAMES               = ('places1', 'places2', 'oasis')\n",
    "SCALE_NAMES                 = ('scale2','scale4','scale8','scale16','scale32')\n",
    "STUDY_NAMES                 = ('short presentation', 'long presentation', 'complexity order', 'oasis')\n",
    "BEHAVIOUR_NAMES             = ('study1_places1_short.csv','study2_places1.csv','study3_places2.csv','study4_oasis.csv')\n",
    "\n",
    "PATH_IMAGES                 = '../images and ratings/imageversions_256'\n",
    "PATH_RATINGS                = '../images and ratings/ratings'\n",
    "PATH_INTEGRATION_VALUES     = '../data csv/integration'\n",
    "PATH_IB_CORRELATIONS        = '../data csv/ibcorr'\n",
    "PATH_IB_CORRELATIONS_BLOCKED= '../data csv/ibcorr blocked'\n",
    "\n",
    "PATH_RESULTS                = '../results'\n",
    "PATH_PLOTS                  = '../plots'\n",
    "\n",
    "#VisualPrior.viable_feature_tasks\n",
    "MODEL_NAMES = ('autoencoding','depth_euclidean','jigsaw','reshading',\n",
    "               'edge_occlusion','keypoints2d','room_layout', #'colorization' currently not working\n",
    "               'curvature','edge_texture','keypoints3d','segment_unsup2d',\n",
    "               'class_object','egomotion','nonfixated_pose','segment_unsup25d',\n",
    "               'class_scene','fixated_pose','normal','segment_semantic',\n",
    "               'denoising','inpainting','point_matching','vanishing_point')\n",
    "\n",
    "def study2behaviour(st):\n",
    "    return BEHAVIOUR_NAMES[STUDY_NAMES.index(st)]\n",
    "\n",
    "def study2dataset(st):\n",
    "    if st in STUDY_NAMES[:2]:\n",
    "        return DATASET_NAMES[0]\n",
    "    if st == STUDY_NAMES[2]:\n",
    "        return DATASET_NAMES[1]\n",
    "    if st == STUDY_NAMES[3]:\n",
    "        return DATASET_NAMES[2]\n",
    "\n",
    "\n",
    "# storing a NetworkScorer object for saving backprojected scores\n",
    "# TODO needs to be adapted when analysing multiple nets\n",
    "BACKPROJECTED_SCORES_FOLDER = './backprojected_scores'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do for each studys:\n",
    "\n",
    "1. For every network: RDM (imageXimage) of searchlight integration scoring. Best ibcorr-performing layer in study.\n",
    "\n",
    "3. netXnet RDM of RDMs from (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ibcorr_for_spatialanalysis(mo, sc):\n",
    "    \"\"\" load ibcorr for one net\"\"\"\n",
    "    l = []\n",
    "    for st in STUDY_NAMES:\n",
    "        d = pd.read_csv(os.path.join(PATH_IB_CORRELATIONS, mo, st, sc, 'ib_correlations.csv'), header=None)\n",
    "        d.insert(0, 'study', st)\n",
    "        d = d.reset_index().rename(columns={\"index\":\"layer\", 0:\"ibcorr\"})\n",
    "        l.append(d)\n",
    "    return pd.concat(l).reset_index(drop=True)\n",
    "\n",
    "#setup nets for extracting activations from best layer\n",
    "def setup_singlelayer(model_name: str, layer_idx: int):\n",
    "    \"\"\" Setup activation extractor for a single layer of a tasnomomy network\n",
    "    \"\"\"\n",
    "    VisualPriorRepresentation._load_unloaded_nets([model_name])\n",
    "    net = VisualPriorRepresentation.feature_task_to_net[model_name]\n",
    "\n",
    "    _, eval_nodes = get_graph_node_names(net)\n",
    "    return_nodes = {node:node for node in eval_nodes if \"conv\" in node or 'fc' in node}\n",
    "    \n",
    "    layer_name = list(return_nodes.keys())[layer_idx]\n",
    "    return create_feature_extractor(net, return_nodes={layer_name:layer_name}), layer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_spatialintegration_analysis_bestlayer(mo, st, sc):\n",
    "    # setup best-layer activation extractor\n",
    "    df_ibcorr = load_ibcorr_for_spatialanalysis(mo, sc)\n",
    "    best_ibcorr = (df_ibcorr\n",
    "                   .iloc[df_ibcorr.groupby('study').ibcorr.idxmax(),:]\n",
    "                   .set_index('study'))\n",
    "    idx = best_ibcorr.loc[st].layer.astype(int)\n",
    "    activation_extractor, layername = setup_singlelayer(mo, idx)\n",
    "\n",
    "    # setup Pattern Generator\n",
    "    dataset = ImageDataset(os.path.join(PATH_IMAGES, study2dataset(st), sc))\n",
    "    dummy_image = next(iter(dataset))\n",
    "    net_activation = activation_extractor(dummy_image[0])\n",
    "    activation_shape = taskonomy_activation_layer_shapes(net_activation)\n",
    "    activation_shape = torch.Size(d for d in activation_shape[layername] if d != 1)\n",
    "    pat = PatternGeneratorSearchlight(activation_shape, layername)\n",
    "    \n",
    "    # setup storing results\n",
    "    # layer x image x subset\n",
    "    num_layers = 1\n",
    "    num_images = dataset.img_count\n",
    "    num_subsets = pat.num_subsets\n",
    "    integration = np.full([num_layers,num_images,num_subsets], np.nan, dtype=np.float64)\n",
    "\n",
    "    # run analysis\n",
    "    #cnt = 0\n",
    "    for img_id, (img_full, img_v1, img_v2) in enumerate(iter(dataset)):\n",
    "        #if cnt % 30 == 0: print(cnt, end= ' ')\n",
    "        #cnt=cnt+1\n",
    "        \n",
    "        # activations as tensors\n",
    "        act_full = activation_extractor(img_full)[layername].squeeze()\n",
    "        act_v1 = activation_extractor(img_v1)[layername].squeeze()\n",
    "        act_v2 = activation_extractor(img_v2)[layername].squeeze()\n",
    "        act_avg = (act_v1 + act_v2) / 2.\n",
    "\n",
    "        # iterate 3D positions\n",
    "        pat_it = iter(pat)\n",
    "        for subset_num, roi_mask in pat_it:\n",
    "            \n",
    "            subset_act_full = act_full[roi_mask]\n",
    "            subset_act_avg = act_avg[roi_mask]\n",
    "\n",
    "            # calculate integration and store it\n",
    "            subset_integration = pearsonr(subset_act_full.flatten(), subset_act_avg.flatten())[0]\n",
    "            integration[:,img_id, subset_num] = subset_integration\n",
    "\n",
    "    return integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_study = \"long presentation\"\n",
    "_scale = \"scale4\"\n",
    "\n",
    "\n",
    "for mo in MODEL_NAMES:\n",
    "    print(mo)\n",
    "    integration_subsets = run_spatialintegration_analysis_bestlayer(mo, _study, _scale)\n",
    "    integration = pd.DataFrame(integration_subsets.squeeze().T)\n",
    "    integration.to_csv(\n",
    "        os.path.join(\n",
    "            PATH_RESULTS,\n",
    "            \"spatial integration\",\n",
    "            _scale,\n",
    "            _study + \" subsetdata\",\n",
    "            mo + \".csv\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.789565</td>\n",
       "      <td>0.822378</td>\n",
       "      <td>0.933830</td>\n",
       "      <td>0.921553</td>\n",
       "      <td>0.963331</td>\n",
       "      <td>0.823563</td>\n",
       "      <td>0.839624</td>\n",
       "      <td>0.795302</td>\n",
       "      <td>0.958589</td>\n",
       "      <td>0.920836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438066</td>\n",
       "      <td>0.845780</td>\n",
       "      <td>0.903340</td>\n",
       "      <td>0.929214</td>\n",
       "      <td>0.620008</td>\n",
       "      <td>0.883785</td>\n",
       "      <td>0.913007</td>\n",
       "      <td>0.823557</td>\n",
       "      <td>0.878925</td>\n",
       "      <td>0.963137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.708518</td>\n",
       "      <td>0.845675</td>\n",
       "      <td>0.942570</td>\n",
       "      <td>0.895499</td>\n",
       "      <td>0.944936</td>\n",
       "      <td>0.855701</td>\n",
       "      <td>0.885716</td>\n",
       "      <td>0.854953</td>\n",
       "      <td>0.930072</td>\n",
       "      <td>0.869773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.656696</td>\n",
       "      <td>0.815259</td>\n",
       "      <td>0.866278</td>\n",
       "      <td>0.900904</td>\n",
       "      <td>0.656427</td>\n",
       "      <td>0.856804</td>\n",
       "      <td>0.796750</td>\n",
       "      <td>0.759617</td>\n",
       "      <td>0.913485</td>\n",
       "      <td>0.948846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666026</td>\n",
       "      <td>0.775316</td>\n",
       "      <td>0.850162</td>\n",
       "      <td>0.812954</td>\n",
       "      <td>0.758249</td>\n",
       "      <td>0.759467</td>\n",
       "      <td>0.812634</td>\n",
       "      <td>0.688789</td>\n",
       "      <td>0.775883</td>\n",
       "      <td>0.550010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406312</td>\n",
       "      <td>0.758702</td>\n",
       "      <td>0.671209</td>\n",
       "      <td>0.696034</td>\n",
       "      <td>0.513749</td>\n",
       "      <td>0.794002</td>\n",
       "      <td>0.513979</td>\n",
       "      <td>0.537180</td>\n",
       "      <td>0.698741</td>\n",
       "      <td>0.781612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.635995</td>\n",
       "      <td>0.649550</td>\n",
       "      <td>0.832780</td>\n",
       "      <td>0.779286</td>\n",
       "      <td>0.769203</td>\n",
       "      <td>0.847171</td>\n",
       "      <td>0.843160</td>\n",
       "      <td>0.711602</td>\n",
       "      <td>0.762562</td>\n",
       "      <td>0.365617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397083</td>\n",
       "      <td>0.821731</td>\n",
       "      <td>0.708546</td>\n",
       "      <td>0.701664</td>\n",
       "      <td>0.389897</td>\n",
       "      <td>0.777546</td>\n",
       "      <td>0.593664</td>\n",
       "      <td>0.431560</td>\n",
       "      <td>0.681521</td>\n",
       "      <td>0.688238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.780789</td>\n",
       "      <td>0.533344</td>\n",
       "      <td>0.865568</td>\n",
       "      <td>0.773644</td>\n",
       "      <td>0.808846</td>\n",
       "      <td>0.900123</td>\n",
       "      <td>0.867244</td>\n",
       "      <td>0.701487</td>\n",
       "      <td>0.809309</td>\n",
       "      <td>0.656558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370777</td>\n",
       "      <td>0.908834</td>\n",
       "      <td>0.769267</td>\n",
       "      <td>0.808429</td>\n",
       "      <td>0.463270</td>\n",
       "      <td>0.889772</td>\n",
       "      <td>0.738970</td>\n",
       "      <td>0.628863</td>\n",
       "      <td>0.792633</td>\n",
       "      <td>0.718332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24691</th>\n",
       "      <td>0.410811</td>\n",
       "      <td>-0.106083</td>\n",
       "      <td>0.033703</td>\n",
       "      <td>0.716069</td>\n",
       "      <td>0.362532</td>\n",
       "      <td>0.284008</td>\n",
       "      <td>0.401079</td>\n",
       "      <td>0.544075</td>\n",
       "      <td>0.651219</td>\n",
       "      <td>0.649127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050257</td>\n",
       "      <td>0.644140</td>\n",
       "      <td>0.905792</td>\n",
       "      <td>0.290449</td>\n",
       "      <td>0.558072</td>\n",
       "      <td>-0.001639</td>\n",
       "      <td>-0.082489</td>\n",
       "      <td>-0.108256</td>\n",
       "      <td>0.194868</td>\n",
       "      <td>0.337830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24692</th>\n",
       "      <td>0.604702</td>\n",
       "      <td>0.056296</td>\n",
       "      <td>0.003916</td>\n",
       "      <td>0.758142</td>\n",
       "      <td>0.516417</td>\n",
       "      <td>0.538283</td>\n",
       "      <td>0.511134</td>\n",
       "      <td>0.634556</td>\n",
       "      <td>0.617044</td>\n",
       "      <td>0.599259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038689</td>\n",
       "      <td>0.578432</td>\n",
       "      <td>0.798848</td>\n",
       "      <td>0.329612</td>\n",
       "      <td>0.604866</td>\n",
       "      <td>0.282032</td>\n",
       "      <td>0.157823</td>\n",
       "      <td>-0.057153</td>\n",
       "      <td>0.345047</td>\n",
       "      <td>0.512277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24693</th>\n",
       "      <td>0.640885</td>\n",
       "      <td>0.085503</td>\n",
       "      <td>-0.035659</td>\n",
       "      <td>0.632084</td>\n",
       "      <td>0.589960</td>\n",
       "      <td>0.672916</td>\n",
       "      <td>0.834496</td>\n",
       "      <td>0.555944</td>\n",
       "      <td>0.461628</td>\n",
       "      <td>0.539368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168896</td>\n",
       "      <td>0.534773</td>\n",
       "      <td>0.621770</td>\n",
       "      <td>0.470264</td>\n",
       "      <td>0.519247</td>\n",
       "      <td>0.311254</td>\n",
       "      <td>0.278814</td>\n",
       "      <td>-0.014633</td>\n",
       "      <td>0.566752</td>\n",
       "      <td>0.562143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24694</th>\n",
       "      <td>0.546563</td>\n",
       "      <td>0.045963</td>\n",
       "      <td>-0.162492</td>\n",
       "      <td>0.542275</td>\n",
       "      <td>0.578382</td>\n",
       "      <td>0.694877</td>\n",
       "      <td>0.853557</td>\n",
       "      <td>0.388634</td>\n",
       "      <td>0.360721</td>\n",
       "      <td>0.367691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111435</td>\n",
       "      <td>0.420816</td>\n",
       "      <td>0.380698</td>\n",
       "      <td>0.347294</td>\n",
       "      <td>0.353153</td>\n",
       "      <td>0.169157</td>\n",
       "      <td>0.406306</td>\n",
       "      <td>0.109531</td>\n",
       "      <td>0.385657</td>\n",
       "      <td>0.465950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24695</th>\n",
       "      <td>0.401050</td>\n",
       "      <td>-0.050019</td>\n",
       "      <td>-0.019193</td>\n",
       "      <td>0.446578</td>\n",
       "      <td>0.424845</td>\n",
       "      <td>0.758910</td>\n",
       "      <td>0.864447</td>\n",
       "      <td>0.314618</td>\n",
       "      <td>0.267070</td>\n",
       "      <td>0.341459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025972</td>\n",
       "      <td>0.377425</td>\n",
       "      <td>0.257974</td>\n",
       "      <td>0.445250</td>\n",
       "      <td>0.230002</td>\n",
       "      <td>-0.162388</td>\n",
       "      <td>0.359667</td>\n",
       "      <td>0.009941</td>\n",
       "      <td>0.353079</td>\n",
       "      <td>0.307333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24696 rows Ã— 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.789565  0.822378  0.933830  0.921553  0.963331  0.823563  0.839624   \n",
       "1      0.708518  0.845675  0.942570  0.895499  0.944936  0.855701  0.885716   \n",
       "2      0.666026  0.775316  0.850162  0.812954  0.758249  0.759467  0.812634   \n",
       "3      0.635995  0.649550  0.832780  0.779286  0.769203  0.847171  0.843160   \n",
       "4      0.780789  0.533344  0.865568  0.773644  0.808846  0.900123  0.867244   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24691  0.410811 -0.106083  0.033703  0.716069  0.362532  0.284008  0.401079   \n",
       "24692  0.604702  0.056296  0.003916  0.758142  0.516417  0.538283  0.511134   \n",
       "24693  0.640885  0.085503 -0.035659  0.632084  0.589960  0.672916  0.834496   \n",
       "24694  0.546563  0.045963 -0.162492  0.542275  0.578382  0.694877  0.853557   \n",
       "24695  0.401050 -0.050019 -0.019193  0.446578  0.424845  0.758910  0.864447   \n",
       "\n",
       "              7         8         9  ...       240       241       242  \\\n",
       "0      0.795302  0.958589  0.920836  ...  0.438066  0.845780  0.903340   \n",
       "1      0.854953  0.930072  0.869773  ...  0.656696  0.815259  0.866278   \n",
       "2      0.688789  0.775883  0.550010  ...  0.406312  0.758702  0.671209   \n",
       "3      0.711602  0.762562  0.365617  ...  0.397083  0.821731  0.708546   \n",
       "4      0.701487  0.809309  0.656558  ...  0.370777  0.908834  0.769267   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "24691  0.544075  0.651219  0.649127  ...  0.050257  0.644140  0.905792   \n",
       "24692  0.634556  0.617044  0.599259  ...  0.038689  0.578432  0.798848   \n",
       "24693  0.555944  0.461628  0.539368  ...  0.168896  0.534773  0.621770   \n",
       "24694  0.388634  0.360721  0.367691  ...  0.111435  0.420816  0.380698   \n",
       "24695  0.314618  0.267070  0.341459  ... -0.025972  0.377425  0.257974   \n",
       "\n",
       "            243       244       245       246       247       248       249  \n",
       "0      0.929214  0.620008  0.883785  0.913007  0.823557  0.878925  0.963137  \n",
       "1      0.900904  0.656427  0.856804  0.796750  0.759617  0.913485  0.948846  \n",
       "2      0.696034  0.513749  0.794002  0.513979  0.537180  0.698741  0.781612  \n",
       "3      0.701664  0.389897  0.777546  0.593664  0.431560  0.681521  0.688238  \n",
       "4      0.808429  0.463270  0.889772  0.738970  0.628863  0.792633  0.718332  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "24691  0.290449  0.558072 -0.001639 -0.082489 -0.108256  0.194868  0.337830  \n",
       "24692  0.329612  0.604866  0.282032  0.157823 -0.057153  0.345047  0.512277  \n",
       "24693  0.470264  0.519247  0.311254  0.278814 -0.014633  0.566752  0.562143  \n",
       "24694  0.347294  0.353153  0.169157  0.406306  0.109531  0.385657  0.465950  \n",
       "24695  0.445250  0.230002 -0.162388  0.359667  0.009941  0.353079  0.307333  \n",
       "\n",
       "[24696 rows x 250 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_study = \"short presentation\"\n",
    "_scale = \"scale4\"\n",
    "mo = \"normal\"\n",
    "\n",
    "pd.read_csv(\n",
    "    os.path.join(\n",
    "        PATH_RESULTS, \"spatial integration\", _scale, _study + \" subsetdata\", mo + \".csv\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMDs from subsetdata\n",
    "\n",
    "_study = \"short presentation\"\n",
    "_scale = \"scale4\"\n",
    "\n",
    "\n",
    "for mo in MODEL_NAMES[5:]:\n",
    "    integration_subsets = pd.read_csv(\n",
    "        os.path.join(\n",
    "            PATH_RESULTS,\n",
    "            \"spatial integration\",\n",
    "            _scale,\n",
    "            _study + \" subsetdata\",\n",
    "            mo + \".csv\",\n",
    "        ),\n",
    "        usecols=lambda col: col!=\"Unnamed: 0\"\n",
    "    )\n",
    "    rdm = calculate_rdm(integration_subsets)\n",
    "    rdm.to_csv(\n",
    "        os.path.join(PATH_RESULTS, \"spatial integration\", _scale, _study, mo + \".csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing code\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "# with Pool() as pool:\n",
    "#     l1 = pool.starmap(run_spatialintegration_analysis_bestlayer, zip(MODEL_NAMES[5:], [\"short presentation\"] * len(MODEL_NAMES[5:]), [\"scale4\"] * len(MODEL_NAMES[5:]))   )\n",
    "\n",
    "#d1 = {k: d1[k] for k in _models}\n",
    "\n",
    "# for mo, data in l1:\n",
    "#     df = pd.DataFrame(data.squeeze().T)\n",
    "#     calculate_rdm(df)\n",
    "#     df.to_csv(os.path.join(PATH_RESULTS, \"spatial integration\", \"scale4\", \"study1\", mo + \".csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = 'depth_euclidean'\n",
    "st = STUDY_NAMES[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_ibcorr(mo, sc = SCALE_NAME):\n",
    "    \"\"\" load ibcorr for one net\"\"\"\n",
    "    l = []\n",
    "    for st in STUDY_NAMES:\n",
    "        d = pd.read_csv(os.path.join(PATH_IB_CORRELATIONS, mo, st, sc, 'ib_correlations.csv'), header=None)\n",
    "        d.insert(0, 'study', st)\n",
    "        d = d.reset_index().rename(columns={\"index\":\"layer\", 0:\"ibcorr\"})\n",
    "        l.append(d)\n",
    "    return pd.concat(l).reset_index(drop=True)\n",
    "\n",
    "df_ibcorr = load_ibcorr(mo)\n",
    "df_ibcorr\n",
    "\n",
    "# visualize this: add lineplot for ibcorr of 4 studies for per net\n",
    "best_ibcorr = df_ibcorr.iloc[df_ibcorr.groupby('study').ibcorr.idxmax(),:].set_index('study')\n",
    "best_ibcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup nets for extracting activations from best layer\n",
    "def setup_singlelayer(model_name: str, layer_idx: int):\n",
    "    \"\"\" Setup activation extractor for a single layer of a tasnomomy network\n",
    "    \"\"\"\n",
    "    VisualPriorRepresentation._load_unloaded_nets([model_name])\n",
    "    net = VisualPriorRepresentation.feature_task_to_net[model_name]\n",
    "\n",
    "    _, eval_nodes = get_graph_node_names(net)\n",
    "    return_nodes = {node:node for node in eval_nodes if \"conv\" in node or 'fc' in node}\n",
    "    \n",
    "    layer_name = list(return_nodes.keys())[layer_idx]\n",
    "    return create_feature_extractor(net, return_nodes={layer_name:layer_name}), layer_name\n",
    "\n",
    "idx = best_ibcorr.loc[st].layer.astype(int)\n",
    "activation_extractor, layername = setup_singlelayer(mo, idx)\n",
    "layername"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load net and activation shape (of best layer)\n",
    "dataset = ImageDataset(os.path.join(PATH_IMAGES, study2dataset(st), SCALE_NAME))\n",
    "dummy_image = next(iter(dataset))\n",
    "net_activation = activation_extractor(dummy_image[0])\n",
    "activation_shape = taskonomy_activation_layer_shapes(net_activation)\n",
    "activation_shape = torch.Size(d for d in activation_shape[layername] if d != 1)\n",
    "\n",
    "pat = PatternGeneratorSearchlight(activation_shape, layername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer x image x subset\n",
    "num_layers = 1\n",
    "num_images = dataset.img_count\n",
    "num_subsets = pat.num_subsets\n",
    "integration = np.full([num_layers,num_images,num_subsets], np.nan, dtype=np.float64)\n",
    "\n",
    "# run analysis\n",
    "cnt = 0\n",
    "\n",
    "for img_id, (img_full, img_v1, img_v2) in enumerate(iter(dataset)):\n",
    "    if cnt % 30 == 0: print(cnt, end= ' ')\n",
    "    cnt=cnt+1\n",
    "    \n",
    "    # activations as tensors\n",
    "    act_full = activation_extractor(img_full)[layername].squeeze()\n",
    "    act_v1 = activation_extractor(img_v1)[layername].squeeze()\n",
    "    act_v2 = activation_extractor(img_v2)[layername].squeeze()\n",
    "    act_avg = (act_v1 + act_v2) / 2.\n",
    "\n",
    "    # iterate 3D positions in layer\n",
    "    pat_it = iter(pat)\n",
    "    for subset_num, roi_mask in pat_it:\n",
    "        \n",
    "        subset_act_full = act_full[roi_mask]\n",
    "        subset_act_avg = act_avg[roi_mask]\n",
    "\n",
    "        # calculate integration and store it\n",
    "        subset_integration = pearsonr(subset_act_full.flatten(), subset_act_avg.flatten())[0]\n",
    "        integration[:,img_id, subset_num] = subset_integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_RESULTS, RESULTS_SEMANTIC_PLACES1, \"semantic_integration.npy\"), 'rb') as f:\n",
    "    integration = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load beauty ratings\n",
    "beauty_ratings = pd.read_csv(os.path.join(PATH_RATINGS, study2behaviour(st)), header=None).mean(axis=1)\n",
    "\n",
    "# correlate integration with beauty\n",
    "scores = correlate_integration_beauty(integration, beauty_ratings)\n",
    "\n",
    "# convert integration back to 3D layer space\n",
    "ns = NetworkScorer({layername:activation_shape})\n",
    "ns.map_back_scores(scores, pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best predicting nets\n",
    "\n",
    "Representative net from each category:\n",
    "\n",
    "semantic:   segment semantic \\\n",
    "2D:         segment unsup 2d \\\n",
    "3D:         depth euclidean\n",
    "\n",
    "> from each class of networks take the one layer with highest integration beauty score across networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load integration-beauty correlation for 3 nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_NAME = \"scale4\"\n",
    "MODEL_SEMANTIC = \"segment_semantic\"\n",
    "MODEL_2D = \"segment_unsup2d\"\n",
    "MODEL_3D = \"depth_euclidean\"\n",
    "\n",
    "\n",
    "def load_ibcorr(m):\n",
    "    l = []\n",
    "    for s in STUDY_NAMES:\n",
    "        d = pd.read_csv(os.path.join(PATH_IB_CORRELATIONS, m, s, SCALE_NAME, 'ib_correlations.csv'), header=None)\n",
    "        d.insert(0, 'study', s)\n",
    "        d = d.reset_index().rename(columns={\"index\":\"layer\", 0:\"ibcorr\"})\n",
    "        l.append(d)\n",
    "    \n",
    "    return pd.concat(l).reset_index(drop=True)\n",
    "\n",
    "df_segment_semantic = load_ibcorr(MODEL_SEMANTIC)\n",
    "df_segment_unsup2d = load_ibcorr(MODEL_2D)\n",
    "df_depth_euclidean = load_ibcorr(MODEL_3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find layers with highest integration-beauty correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scale and layer num of layer with highest correlation\n",
    "best_semantic = df_segment_semantic.iloc[df_segment_semantic.ibcorr.idxmax(),:]\n",
    "best_2d = df_segment_unsup2d.iloc[df_segment_unsup2d.ibcorr.idxmax(),:]\n",
    "best_3d = df_depth_euclidean.iloc[df_depth_euclidean.ibcorr.idxmax(),:]\n",
    "print(best_semantic)\n",
    "print(best_2d)\n",
    "print(best_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup nets for extracting activations from best layer\n",
    "def setup_singlelayer(model_name: str, layer_idx: int):\n",
    "    \"\"\" Setup activation extractor for a single layer of a tasnomomy network\n",
    "    \"\"\"\n",
    "    VisualPriorRepresentation._load_unloaded_nets([model_name])\n",
    "    net = VisualPriorRepresentation.feature_task_to_net[model_name]\n",
    "\n",
    "    _, eval_nodes = get_graph_node_names(net)\n",
    "    return_nodes = {node:node for node in eval_nodes if \"conv\" in node or 'fc' in node}\n",
    "    \n",
    "    layer_name = list(return_nodes.keys())[layer_idx]\n",
    "    return create_feature_extractor(net, return_nodes={layer_name:layer_name}), layer_name\n",
    "\n",
    "activation_extractor_semantic, layername_best_semantic = setup_singlelayer(MODEL_SEMANTIC, best_semantic.layer)\n",
    "activation_extractor_2d, layername_best_2d = setup_singlelayer(MODEL_2D, best_2d.layer)\n",
    "activation_extractor_3d, layername_best_3d = setup_singlelayer(MODEL_3D, best_3d.layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## searchlight scoring\n",
    "\n",
    "Do integration beauty scoring with node pools sweeping along 3D axis\n",
    "- full width 2D layer pools along all three axis\n",
    "- 3D cubic pool along all three axis (vary in size)\n",
    "\n",
    "Effectively just need to change the PatternGenerator, and switch from iterating subset num to iterating the pattern generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_SEMANTIC_PLACES1 = './searchlight/singlelayer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### semantic net\n",
    "\n",
    "semantic, places1, best layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load images & activation shape (semantic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(os.path.join(PATH_IMAGES, \"places1\", best_semantic.scale))\n",
    "\n",
    "num_images = dataset.img_count\n",
    "\n",
    "# activation shape as Dict\n",
    "dummy_image = next(iter(dataset))\n",
    "net_activation = activation_extractor_semantic(dummy_image[0])\n",
    "net_activation = OrderedDict(net_activation)\n",
    "activation_shapes = taskonomy_activation_layer_shapes(net_activation)\n",
    "\n",
    "# activation shape as Tensor\n",
    "activation_shape_semantic = list(activation_shapes.values())[0][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single image(s)\n",
    "\n",
    "can only map back integration-beauty score for whole dataset, however, its possible, that in every image something different gets integrated, therefore it wouldn't make sense to map back a score for the whole dataset to individual nodes.\n",
    "\n",
    "Since we know that (at least some) integration, and subsets integration, is meaningfull, we map back integration of individual images onto individual nodes, without correlating this integration with beauty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SETUP VARIABLES ---\n",
    "pat_s = PatternGeneratorSearchlight(\n",
    "    activation_shape_semantic,\n",
    "    layername_best_semantic\n",
    "    )\n",
    "\n",
    "num_layers = 1 # only take activations of best performing layer from each net\n",
    "num_images = 1\n",
    "num_subsets = pat_s.num_subsets\n",
    "\n",
    "# layer x image x subset\n",
    "integration = np.full([num_layers, num_images, num_subsets], np.nan, dtype=np.float64)\n",
    "cnt = 0\n",
    "\n",
    "    \n",
    "# load single image\n",
    "img_id = 0\n",
    "img_full, img_v1, img_v2 = next(iter(dataset))\n",
    "\n",
    "# --- ANALYSIS ---\n",
    "\n",
    "act_full = activation_extractor_semantic(img_full)[layername_best_semantic].squeeze()\n",
    "act_v1 = activation_extractor_semantic(img_v1)[layername_best_semantic].squeeze()\n",
    "act_v2 = activation_extractor_semantic(img_v2)[layername_best_semantic].squeeze()\n",
    "act_avg = (act_v1 + act_v2) / 2.\n",
    "\n",
    "# iterate 3D positions\n",
    "pat_it = iter(pat_s)\n",
    "for subset_num, roi_mask in pat_it:\n",
    "    \n",
    "    subset_act_full = act_full[roi_mask]\n",
    "    subset_act_avg = act_avg[roi_mask]\n",
    "\n",
    "    # calculate integration and store it\n",
    "    subset_integration = -pearsonr(subset_act_full.flatten(), subset_act_avg.flatten())[0]\n",
    "    integration[:,img_id, subset_num] = subset_integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration = integration.squeeze(0)\n",
    "integration.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_shapes['layer3.4.conv2'] = activation_shape_semantic\n",
    "ns = NetworkScorer(activation_shapes)\n",
    "ns.map_back_scores(integration, pat_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer x image x subset\n",
    "integration = np.full([num_layers,num_images,num_subsets], np.nan, dtype=np.float64)\n",
    "cnt = 0\n",
    "\n",
    "# iterate image set\n",
    "for img_id, (img_full, img_v1, img_v2) in enumerate(iter(dataset)):\n",
    "    if cnt % 30 == 0: print(cnt, end= ' ')\n",
    "    cnt=cnt+1\n",
    "    \n",
    "    # activations as tensors\n",
    "    act_full = activation_extractor_semantic(img_full)[layername_best_semantic].squeeze()\n",
    "    act_v1 = activation_extractor_semantic(img_v1)[layername_best_semantic].squeeze()\n",
    "    act_v2 = activation_extractor_semantic(img_v2)[layername_best_semantic].squeeze()\n",
    "    act_avg = (act_v1 + act_v2) / 2.\n",
    "\n",
    "    # iterate 3D positions in layer\n",
    "    pat_it = iter(pat_s)\n",
    "\n",
    "    for subset_num, roi_mask in pat_it:\n",
    "        \n",
    "        subset_act_full = act_full[roi_mask]\n",
    "        subset_act_avg = act_avg[roi_mask]\n",
    "\n",
    "        # calculate integration and store it\n",
    "        subset_integration = pearsonr(subset_act_full.flatten(), subset_act_avg.flatten())[0]\n",
    "        integration[:,img_id, subset_num] = subset_integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_RESULTS, RESULTS_SEMANTIC_PLACES1, \"semantic_integration.npy\"), \"wb\") as f:\n",
    "    np.save(f, integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_RESULTS, RESULTS_SEMANTIC_PLACES1, \"semantic_integration.npy\"), 'rb') as f:\n",
    "    integration = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load beauty ratings\n",
    "beauty_ratings = ImageDataset(\n",
    "    os.path.join(DATA_PATH, DATASET_NAMES[0], SCALE_NAMES[0]),\n",
    "    beauty_ratings_path='behavior/ratings_study1.csv').beauty_ratings\n",
    "\n",
    "# correlate integration with beauty\n",
    "scores = correlate_integration_beauty(integration, beauty_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert integration back to 3D layer space\n",
    "activation_shapes['layer3.4.conv2'] = activation_shape_semantic\n",
    "ns = NetworkScorer(activation_shapes)\n",
    "ns.map_back_scores(scores, pat_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize spatial integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO plot all heatmaps with the same absoloute color scale\n",
    "with PdfPages('integration heatmap semantic img0.pdf') as pdf:\n",
    "    for vertical_slice in ns.scores[layername_best_semantic].squeeze():\n",
    "        sns.heatmap(vertical_slice)\n",
    "        pdf.savefig()\n",
    "        plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ns.scores[layername_best_semantic]\n",
    "t.min(), t.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.flatten()\n",
    "num_bins = 30\n",
    "hist = torch.histc(t, bins=num_bins, min=torch.min(t).item(), max=torch.max(t).item())\n",
    "\n",
    "# Define the edges of the bins\n",
    "bin_edges = torch.linspace(torch.min(t).item(), torch.max(t).item(), steps=num_bins + 1)\n",
    "\n",
    "# Plot the histogram using Matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(t.numpy(), bins=bin_edges.numpy(), edgecolor='black')\n",
    "plt.title('Histogram of Tensor Values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inte = ns.scores[layername_best_semantic].squeeze()\n",
    "\n",
    "# plot only highest integrating nodes\n",
    "with PdfPages('integration heatmap semantic img0 best.pdf') as pdf:\n",
    "    for vertical_slice in inte:\n",
    "        sns.heatmap(vertical_slice)\n",
    "        pdf.savefig()\n",
    "        plt.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation testing\n",
    "\n",
    "Repeat analysis for randomly shuffeled integration values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "integration = integration.squeeze(0)\n",
    "integration.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_pt = torch.tensor(integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_pt.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_randomized = integration_pt[torch.randperm(integration_pt.nelement())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_randomized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = NetworkScorer(activation_shapes)\n",
    "ns.map_back_scores(integration_randomized.unsqueeze(0), pat_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot random pattern heatmaps for visual comparison\n",
    "# TODO generate permutation distribution\n",
    "with PdfPages('integration heatmap random scores.pdf') as pdf:\n",
    "    for vertical_slice in ns.scores[layername_best_semantic]:\n",
    "        sns.heatmap(vertical_slice)\n",
    "        pdf.savefig()\n",
    "        plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_shapes[\"layer2.3.conv1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are integration beauty scores meaningfull ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do the top nodes integration predict beauty better than the bottom nodes (or any other random 10%) ? \\\n",
    "- Use spatially coherent subsets to calculate scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (old) random pattern scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_singlelayer_analysis(dataset, activation_extractor, num_subsets, frac):\n",
    "    num_layers = 1 # only take activations of best performing layer from each net\n",
    "    num_images = dataset.img_count\n",
    "\n",
    "    # get shapes of activation\n",
    "    dummy_image = next(iter(dataset))\n",
    "    net_activation = activation_extractor(dummy_image[0])\n",
    "    net_activation = OrderedDict(net_activation)\n",
    "    activation_shapes = taskonomy_activation_layer_shapes(net_activation)\n",
    "    \n",
    "    pat = Pattern_Generator(\n",
    "        num_subsets,\n",
    "        activation_shapes,\n",
    "        frac=frac\n",
    "        )\n",
    "\n",
    "    # layer x image x subset\n",
    "    integration = np.full([num_layers,num_images,num_subsets], np.nan, dtype=np.float64)\n",
    "    cnt = 0\n",
    "\n",
    "    # iterate image set\n",
    "    for img_id, (img_full, img_v1, img_v2) in enumerate(iter(dataset)):\n",
    "        if cnt % 30 == 0: print(cnt, end= ' ')\n",
    "        cnt=cnt+1\n",
    "        \n",
    "        act_full = Activation_Pattern(activation_extractor(img_full))\n",
    "        act_v1 = Activation_Pattern(activation_extractor(img_v1))\n",
    "        act_v2 = Activation_Pattern(activation_extractor(img_v2))\n",
    "        act_avg = Activation_Pattern.average(act_v1, act_v2)\n",
    "\n",
    "        # iterate node subsets\n",
    "        for subset_num in range(num_subsets):\n",
    "            subset_mask = pat.get_subset_pattern(subset_num)\n",
    "            \n",
    "            subset_act_full = act_full[subset_mask]\n",
    "            subset_act_avg = act_avg[subset_mask]\n",
    "\n",
    "            # calculate integration and store it\n",
    "            subset_integration = Activation_Pattern.calculate_integration_coeff(subset_act_full, subset_act_avg)\n",
    "            integration[:,img_id, subset_num] = subset_integration\n",
    "    \n",
    "    ns = NetworkScorer(activation_shapes)\n",
    "    ns.map_back_scores(integration, pat)\n",
    "\n",
    "    return integration, ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subsets = 10000\n",
    "# run analysis for semantic\n",
    "dataset = ImageDataset(os.path.join(DATA_PATH, \"places1\", best_semantic.scale))\n",
    "\n",
    "integration_semantic_frac01, ns_semantic_frac01 = run_singlelayer_analysis(\n",
    "    dataset,\n",
    "    activation_extractor_semantic,\n",
    "    num_subsets,\n",
    "    frac=0.01\n",
    "    )\n",
    "\n",
    "integration_semantic_frac10, ns_semantic_frac10 = run_singlelayer_analysis(\n",
    "    dataset,\n",
    "    activation_extractor_semantic,\n",
    "    num_subsets,\n",
    "    frac=0.10\n",
    "    )\n",
    "\n",
    "integration_semantic_frac90, ns_semantic_frac90= run_singlelayer_analysis(\n",
    "    dataset,\n",
    "    activation_extractor_semantic,\n",
    "    num_subsets,\n",
    "    frac=0.90\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_semantic_frac01.shape, ns_semantic_frac01.scores['layer3.4.conv2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(integration_semantic_frac01), type(ns_semantic_frac01.scores['layer3.4.conv2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subsets = 10000\n",
    "\n",
    "# run analysis for semantic\n",
    "dataset = ImageDataset(os.path.join(DATA_PATH, \"places1\", best_semantic.scale))\n",
    "\n",
    "integration_semantic_frac01, ns_semantic_frac01 = run_singlelayer_analysis(\n",
    "    dataset,\n",
    "    activation_extractor_semantic,\n",
    "    num_subsets,\n",
    "    frac=0.01\n",
    "    )\n",
    "\n",
    "integration_semantic_frac10, ns_semantic_frac10 = run_singlelayer_analysis(\n",
    "    dataset,\n",
    "    activation_extractor_semantic,\n",
    "    num_subsets,\n",
    "    frac=0.10\n",
    "    )\n",
    "\n",
    "integration_semantic_frac90, ns_semantic_frac90= run_singlelayer_analysis(\n",
    "    dataset,\n",
    "    activation_extractor_semantic,\n",
    "    num_subsets,\n",
    "    frac=0.90\n",
    "    )\n",
    "\n",
    "# run analysis for 2d\n",
    "dataset = ImageDataset(os.path.join(DATA_PATH, \"places1\", best_2d.scale))\n",
    "\n",
    "integration_2d_frac01, ns_2d_frac01 = run_singlelayer_analysis(\n",
    "    dataset,\n",
    "    activation_extractor_2d,\n",
    "    num_subsets,\n",
    "    frac=0.01\n",
    "    )\n",
    "\n",
    "integration_2d_frac10, ns_2d_frac10 = run_singlelayer_analysis(\n",
    "    dataset,\n",
    "    activation_extractor_2d,\n",
    "    num_subsets,\n",
    "    frac=0.10\n",
    "    )\n",
    "\n",
    "integration_2d_frac90, ns_2d_frac90= run_singlelayer_analysis(\n",
    "    dataset,\n",
    "    activation_extractor_2d,\n",
    "    num_subsets,\n",
    "    frac=0.90\n",
    "    )\n",
    "\n",
    "# run analysis for 3d\n",
    "dataset = ImageDataset(os.path.join(DATA_PATH, \"places1\", best_3d.scale))\n",
    "\n",
    "integration_3d_frac01, ns_3d_frac01 = run_singlelayer_analysis(\n",
    "    dataset,\n",
    "    activation_extractor_3d,\n",
    "    num_subsets,\n",
    "    frac=0.01\n",
    "    )\n",
    "\n",
    "integration_3d_frac10, ns_3d_frac10 = run_singlelayer_analysis(\n",
    "    dataset,\n",
    "    activation_extractor_3d,\n",
    "    num_subsets,\n",
    "    frac=0.10\n",
    "    )\n",
    "\n",
    "integration_3d_frac90, ns_3d_frac90= run_singlelayer_analysis(\n",
    "    dataset,\n",
    "    activation_extractor_3d,\n",
    "    num_subsets,\n",
    "    frac=0.90\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_SINGLELAYER_ANALYSIS = './analysis results singlelayer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "files_i = [\"integration\"+netclass+fracsize+\".npy\"\n",
    "           for netclass, fracsize\n",
    "           in product([\"_semantic\",\"_2d\",\"_3d\"], [\"_frac01\",\"_frac10\",\"_frac90\"])]\n",
    "files_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_n = [\"integration\"+netclass+fracsize+\".pkl\"\n",
    "           for netclass, fracsize\n",
    "           in product([\"_semantic\",\"_2d\",\"_3d\"], [\"_frac01\",\"_frac10\",\"_frac90\"])]\n",
    "files_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_integration = [integration_semantic_frac01, integration_semantic_frac10, integration_semantic_frac90,\n",
    "                      integration_2d_frac01, integration_2d_frac10, integration_2d_frac90,\n",
    "                      integration_3d_frac01, integration_3d_frac10, integration_3d_frac90]\n",
    "\n",
    "objects_nodescoring = [ns_semantic_frac01, ns_semantic_frac10, ns_semantic_frac90,\n",
    "                       ns_2d_frac01, ns_2d_frac10, ns_2d_frac90,\n",
    "                       ns_3d_frac01, ns_3d_frac10, ns_3d_frac90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_i, file_n, array_i, array_n in zip(files_i, files_n, arrays_integration, objects_nodescoring):\n",
    "    with open(os.path.join(FOLDER_SINGLELAYER_ANALYSIS, file_i), \"wb\") as f1:\n",
    "        np.save(f1, array_i)\n",
    "    \n",
    "    with open(os.path.join(FOLDER_SINGLELAYER_ANALYSIS, file_n), \"wb\") as f2:\n",
    "        pickle.dump(array_n, f2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (old) manipulating integratability and watch response of network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "gradually perform image transformations shown to break integration to:\n",
    "- test where in the network integration takes place\n",
    "- test whether different manipulations break different clusters first\n",
    "\n",
    "\n",
    "\n",
    "Do integration beauty scoring with node pools sweeping along 3D axis\n",
    "- full width 2D layer pools along all three axis\n",
    "- 3D cubic pool along all three axis (vary in size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "permutation test for integration: generate permutation distribution by calculating integration at each location in the layer to random input. input real image, calculate clusters of significant integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do semantic / 2d / 3d nets integrate different features (semantic vs structural) or are they essentially integrating the same features but in a different way ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do different manipulations break integration in different ways ?\n",
    "Look at:\n",
    "    a) How much manipulation does is need to break integration ?\n",
    "    b) In which networks / layers / scales / datasets is integration broken in which order ?\n",
    "    c) Which image's integration is broken first ?\n",
    "\n",
    "\n",
    "Collect different ways of manipulating images to break integration.\n",
    "    - Do these\n",
    "    a) all break integration the same way\n",
    "    b) cluster into groups in the way in which they break integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pixelated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(os.path.join(DATA_PATH, \"places1\", best_semantic.scale))\n",
    "it_dataset = iter(dataset)\n",
    "\n",
    "im = next(it_dataset)[0].squeeze()\n",
    "im2 = next(it_dataset)[2].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.min(), im.max(), im2.min(), im2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = im + im.min().abs()\n",
    "im = im / im.max()\n",
    "#d2 = 1-d2\n",
    "im.min(), im.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a fraction of the pixels on the 256x256 grid and permute them\n",
    "pix_degree = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pix = torch.rand((256,256)) < pix_degree\n",
    "mask_original = ~mask_pix\n",
    "\n",
    "# number of pixels that are pixeled, this is not exactly the pix degree due to randomness\n",
    "n_pix = im[:,mask_pix].shape[1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_im = torch.full_like(im, np.nan)\n",
    "\n",
    "# non-pixeled part\n",
    "pix_im[:,mask_original] = im[:,mask_original]\n",
    "\n",
    "# pixeled part\n",
    "pix_im[:,mask_pix] = im[:,mask_pix][:,torch.randperm(n_pix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelate_image(im, pix_degree: float):\n",
    "    \"\"\" Pixelate a 256x256 image\"\"\"\n",
    "    mask_pix = torch.rand((256,256)) < pix_degree\n",
    "    mask_original = ~mask_pix\n",
    "\n",
    "    # number of pixels that are pixeled, this is not exactly the pix degree due to randomness\n",
    "    n_pix = im[:,mask_pix].shape[1] \n",
    "\n",
    "    pix_im = torch.full_like(im, np.nan)\n",
    "\n",
    "    # non-pixeled part\n",
    "    pix_im[:,mask_original] = im[:,mask_original]\n",
    "\n",
    "    # pixeled part\n",
    "    pix_im[:,mask_pix] = im[:,mask_pix][:,torch.randperm(n_pix)]\n",
    "\n",
    "    return pix_im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_print(im):\n",
    "    im = im + im.min().abs()\n",
    "    im = im / im.max()\n",
    "    plt.imshow(im.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_print(pixelate_image(im, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: since at 0.25 theres already a strong (subjective) visual disturbance,\n",
    "# maybe apply a logarithmic scale with image pixelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: as annother gradual transformation, implement a gradual low-pass filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imageversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(127/127.5-1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[1,0],[0,1]]).repeat_interleave(2,dim=0).repeat_interleave(2,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkerboard(scale, size=256):\n",
    "    msc = torch.tensor([[1, 0] * (scale//2), [0, 1] * (scale//2)] * (scale//2))\n",
    "    return msc.repeat_interleave(size//scale, dim=0).repeat_interleave(size//scale, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_checkerboard(64).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get activations for pixelated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_extractor_semantic().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_extractor_semantic(torch.rand_like(dummy_image[0]))['layer3.4.conv2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  get distribution of integration / ib-correlation of random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each image\n",
    "ac = activation_extractor_semantic(torch.rand_like(dummy_image[0]))['layer3.4.conv2']\n",
    "ac = ac.squeeze()\n",
    "ac.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get previous one at each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1, l2 in zip(ac_full, ac_avg):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visual inspection\n",
    "are there maybe any clusters of integration to random images, already revealing a integration structure in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get activation of actual image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test where there are significant clusters of integration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
