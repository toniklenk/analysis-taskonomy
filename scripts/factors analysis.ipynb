{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import os, sys, pickle\n",
    "from itertools import combinations_with_replacement, combinations, product\n",
    "from collections import OrderedDict\n",
    "\n",
    "# stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "\n",
    "# neural networks\n",
    "import torch, torch.utils.model_zoo  # required to load nets\n",
    "from torchvision.models.feature_extraction import (\n",
    "    get_graph_node_names,\n",
    "    create_feature_extractor,\n",
    ")\n",
    "\n",
    "# analysis code\n",
    "from lib.transforms import VisualPriorRepresentation\n",
    "from lib.functions_second_analysis import *\n",
    "from lib.functions_scripting import *\n",
    "    # DATASET_NAMES,\n",
    "    # SCALE_NAMES,\n",
    "    # STUDY_NAMES,\n",
    "    # BEHAVIOUR_NAMES,\n",
    "    # MODEL_NAMES,\n",
    "    # load_integration,\n",
    "    # load_ibcorr,\n",
    "    # load_pvalues,\n",
    "    # load_ratings,\n",
    "    # studyname2datasetname,\n",
    "    # NETS_SEMANTIC,\n",
    "    # NETS_2D,\n",
    "    # NETS_3D,\n",
    "    # NETS_ALL,\n",
    "#)\n",
    "\n",
    "PATH_IMAGES = \"../images and ratings/imageversions_256\"\n",
    "PATH_RATINGS = \"../images and ratings/ratings\"\n",
    "\n",
    "# !! correlations, invert sign for integration\n",
    "PATH_INTEGRATION = \"../data csv/integration\"\n",
    "PATH_INTEGRATION_MAXPOOL = \"../data csv/integration maxpool\"\n",
    "PATH_INTEGRATION_AVERAGE = \"../data csv/integration average\"\n",
    "\n",
    "PATH_IBCORR = \"../data csv/ibcorr\"\n",
    "PATH_IBCORR_AVERAGE = \"../data csv/ibcorr average\"\n",
    "PATH_IBCORR_MAXPOOL = \"../data csv/ibcorr maxpool\"\n",
    "\n",
    "PATH_RESULTS = \"../results\"\n",
    "PATH_PLOTS = \"../plots\"\n",
    "\n",
    "_models = MODEL_NAMES\n",
    "_study = \"short presentation\"\n",
    "_dataset = \"places1\"\n",
    "_scale = \"scale4\"\n",
    "\n",
    "beauty_ratings = load_ratings(PATH_RATINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt to subselect models\n",
    "models_to_remove = set(\n",
    "    [\"edge_occlusion\", \"inpainting\", \"keypoints\", \"normal\", \"jigsaw\"]\n",
    ")\n",
    "_models = list(set(MODEL_NAMES).difference(models_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load & subselect unblocked data\n",
    "_blocking = None\n",
    "_layers = slice(None)\n",
    "\n",
    "dfi = load_integration(PATH_INTEGRATION)\n",
    "dfibc = load_ibcorr(PATH_IBCORR)\n",
    "\n",
    "dfi = dfi.sort_index().loc[(_models, _dataset, _scale, slice(None), _layers)]\n",
    "dfi = dfi.droplevel([\"dataset\",\"scale\"])\n",
    "\n",
    "dfibc = dfibc.sort_index().loc[(_models, _study, _scale, _layers)]\n",
    "dfibc = dfibc.droplevel([\"study\",\"scale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load & subselect averaged data\n",
    "_blocking = \"average\"\n",
    "_layers = slice(None)\n",
    "\n",
    "dfi = load_integration(PATH_INTEGRATION_AVERAGE)\n",
    "dfibc = load_ibcorr(PATH_IBCORR_AVERAGE)\n",
    "\n",
    "dfi = dfi.sort_index().loc[(_models, _dataset, _scale, slice(None), _layers)]\n",
    "dfi = dfi.droplevel([\"dataset\", \"scale\"])\n",
    "\n",
    "dfibc = dfibc.sort_index().loc[(_models, _study, _scale, _layers)]\n",
    "dfibc = dfibc.droplevel([\"study\", \"scale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load & subselect maxpooled data\n",
    "_blocking = \"maxpool\"\n",
    "_layers = slice(6, 14)\n",
    "\n",
    "dfi = load_integration(PATH_INTEGRATION_MAXPOOL)\n",
    "dfibc = load_ibcorr(PATH_IBCORR_MAXPOOL)\n",
    "\n",
    "dfi = dfi.sort_index().loc[(_models, _dataset, _scale, slice(None), _layers)]\n",
    "dfi = dfi.droplevel([\"dataset\", \"scale\"])\n",
    "\n",
    "dfibc = dfibc.sort_index().loc[(_models, _study, _scale, _layers)]\n",
    "dfibc = dfibc.droplevel([\"study\", \"scale\"])\n",
    "\n",
    "# dfi_m = load_integration(PATH_INTEGRATION_MAXPOOL)\n",
    "# dfibc_m = load_ibcorr(PATH_IBCORR_MAXPOOL)\n",
    "\n",
    "# dfi_m = dfi_m.sort_index().loc[(slice(None), _dataset, _scale, slice(None), _layers_blocked)]\n",
    "# dfi_m = dfi_m.loc[_models]\n",
    "# dfi_m = dfi_m.droplevel([\"dataset\",\"scale\"])\n",
    "\n",
    "# dfibc_m = dfibc_m.sort_index().loc[(slice(None),_study, _scale, _layers_blocked)]\n",
    "# dfibc_m = dfibc_m.loc[_models]\n",
    "# dfibc_m = dfibc_m.droplevel([\"study\",\"scale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_predictors_target_rdms(\n",
    "    path_integration, path_ibcorr, _models, _study, _scale, _layers\n",
    "):\n",
    "    _dataset = studyname2datasetname(_study)\n",
    "    blocking = None\n",
    "\n",
    "    dfi = load_integration(path_integration)\n",
    "    dfibc = load_ibcorr(path_ibcorr)\n",
    "\n",
    "    dfi = dfi.sort_index().loc[(_models, _dataset, _scale, slice(None), _layers)]\n",
    "    dfi = dfi.droplevel([\"dataset\", \"scale\"])\n",
    "\n",
    "    dfibc = dfibc.sort_index().loc[(_models, _study, _scale, _layers)]\n",
    "    dfibc = dfibc.droplevel([\"study\", \"scale\"])\n",
    "\n",
    "    # --- target ---\n",
    "    target_rdm = pd.DataFrame(0, index=_models, columns=_models)\n",
    "    df = dfibc.unstack(\"layer\")\n",
    "    for c1, c2 in combinations(_models, 2):\n",
    "        target_rdm.loc[c1, c2] = (df.loc[c1] - df.loc[c2]).abs().sum()\n",
    "        target_rdm.loc[c2, c1] = (df.loc[c1] - df.loc[c2]).abs().sum()\n",
    "\n",
    "    # --- predictor 1 ---\n",
    "    model_rdm = pd.DataFrame(\n",
    "        np.full((len(NETS_ALL), len(NETS_ALL)), np.nan),\n",
    "        columns=NETS_ALL,\n",
    "        index=NETS_ALL,\n",
    "    )\n",
    "    for combi in combinations_with_replacement(NETS_ALL, 2):\n",
    "        if (\n",
    "            combi in combinations_with_replacement(NETS_SEMANTIC, 2)\n",
    "            or combi in combinations_with_replacement(NETS_2D, 2)\n",
    "            or combi in combinations_with_replacement(NETS_3D, 2)\n",
    "        ):\n",
    "            model_rdm.loc[combi] = 1\n",
    "            model_rdm.loc[tuple(reversed(combi))] = 1\n",
    "        else:\n",
    "            model_rdm.loc[combi] = 0\n",
    "            model_rdm.loc[tuple(reversed(combi))] = 0\n",
    "\n",
    "    predictor_rdm_1 = pd.DataFrame(0, index=_models, columns=_models)\n",
    "    for c1, c2 in combinations_with_replacement(_models, 2):\n",
    "        if c1 in model_rdm.index and c2 in model_rdm.index:\n",
    "            predictor_rdm_1.loc[c1, c2] = 1 - model_rdm.loc[c1, c2]\n",
    "            predictor_rdm_1.loc[c2, c1] = 1 - model_rdm.loc[c2, c1]\n",
    "        else:\n",
    "            predictor_rdm_1.loc[c1, c2] = 0\n",
    "            predictor_rdm_1.loc[c2, c1] = 0\n",
    "    \n",
    "    # --- predictor 2---\n",
    "    bestlayers = dfibc.groupby(\"model\").idxmax().ibcorr\n",
    "    df_bestpredicting_integration = (\n",
    "        dfi.unstack(\"img\").loc[bestlayers].droplevel(\"layer\").T.droplevel(0)\n",
    "    )\n",
    "    predictor_rdm_2 = calculate_rdm(df_bestpredicting_integration, \"spearman\")\n",
    "\n",
    "    # ---predictor 3---\n",
    "    try:\n",
    "        rdms = {}\n",
    "        for mo in _models:\n",
    "            rdm = pd.read_csv(\n",
    "                os.path.join(\n",
    "                    PATH_RESULTS,\n",
    "                    \"spatial integration\",\n",
    "                    _scale,\n",
    "                    _study,\n",
    "                    mo + \".csv\",\n",
    "                ),\n",
    "                header=None,\n",
    "            )\n",
    "            rdm = rdm.iloc[1:, 1:].reset_index(drop=True).T.reset_index(drop=True).T\n",
    "            rdms[mo] = rdm\n",
    "        predictor_rdm_3 = calculate_rdm(flatten_concat_rdms(rdms))\n",
    "    except Exception as e:\n",
    "        print(\"Spatial analysis not available for these parameters, returing emptry predictor 3\")\n",
    "        predictor_rdm_3 = None\n",
    "\n",
    "    return target_rdm, predictor_rdm_1, predictor_rdm_2, predictor_rdm_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target 01: differences in ib-corr\n",
    "\n",
    "absoloute difference in correlation in each layer, summed up. <br>\n",
    "Optional: Normalized with 2 (spearman correlation range) * num_layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 1: absoloute differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfibc.unstack(\"layer\")\n",
    "rdm_target_absdiff = pd.DataFrame(0, index=_models, columns=_models)\n",
    "\n",
    "for c1, c2 in combinations(_models, 2):\n",
    "    # sum of absoloute differences (easier, no correlation of correlated correlation coefficients)\n",
    "    rdm_target_absdiff.loc[c1, c2] = (df.loc[c1] - df.loc[c2]).abs().sum()\n",
    "    rdm_target_absdiff.loc[c2, c1] = (df.loc[c1] - df.loc[c2]).abs().sum()\n",
    "\n",
    "# norming dissimilarity: sum_abs_diff / (num_layers * 2)\n",
    "sns.heatmap(\n",
    "    rdm_target_absdiff,\n",
    "    xticklabels=rdm_target_absdiff.columns,\n",
    "    yticklabels=rdm_target_absdiff.index,\n",
    ")\n",
    "target_rdm = rdm_target_absdiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 2: correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add correlate correlations\n",
    "df = dfibc.unstack(\"layer\")\n",
    "rdm_target_corrwise = pd.DataFrame(0, index=_models, columns=_models)\n",
    "\n",
    "for c1, c2 in combinations(_models, 2):\n",
    "    rdm_target_corrwise.loc[c1, c2] = pearsonr(df.loc[c1], df.loc[c2])[0]\n",
    "    rdm_target_corrwise.loc[c2, c1] = pearsonr(df.loc[c1], df.loc[c2])[0]\n",
    "\n",
    "# norming dissimilarity: sum_abs_diff / (num_layers * 2)\n",
    "sns.heatmap(\n",
    "    rdm_target_corrwise,\n",
    "    xticklabels=rdm_target_corrwise.columns,\n",
    "    yticklabels=rdm_target_corrwise.index,\n",
    ")\n",
    "target_rdm = rdm_target_corrwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor 1: semantic-2d-3d\n",
    "Network grouping according to Radek paper <br>\n",
    "[Finished predictor RDM ](#predictor-rdm-semantic-2d-3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add network classes to data\n",
    "\n",
    "# integration\n",
    "dfi = dfi.loc[NETS_ALL]\n",
    "dfi[\"class\"] = dfi.reset_index(level=\"model\").model.apply(modelname2class).values\n",
    "dfi = dfi.set_index(\"class\", append=True)\n",
    "\n",
    "dfi_m = dfi_m.loc[NETS_ALL]\n",
    "dfi_m[\"class\"] = dfi_m.reset_index(level=\"model\").model.apply(modelname2class).values\n",
    "dfi_m = dfi_m.set_index(\"class\", append=True)\n",
    "\n",
    "\n",
    "# ibcorr\n",
    "dfibc = dfibc.loc[NETS_ALL]\n",
    "dfibc[\"class\"] = dfibc.reset_index(level=\"model\").model.apply(modelname2class).values\n",
    "dfibc = dfibc.set_index(\"class\", append=True)\n",
    "\n",
    "dfibc_m = dfibc_m.loc[NETS_ALL]\n",
    "dfibc_m[\"class\"] = (\n",
    "    dfibc_m.reset_index(level=\"model\").model.apply(modelname2class).values\n",
    ")\n",
    "dfibc_m = dfibc_m.set_index(\"class\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model RDM\n",
    "model_rdm = pd.DataFrame(\n",
    "    np.full((len(NETS_ALL), len(NETS_ALL)), np.nan), columns=NETS_ALL, index=NETS_ALL\n",
    ")\n",
    "\n",
    "for combi in combinations_with_replacement(NETS_ALL, 2):\n",
    "    if (\n",
    "        combi in combinations_with_replacement(NETS_SEMANTIC, 2)\n",
    "        or combi in combinations_with_replacement(NETS_2D, 2)\n",
    "        or combi in combinations_with_replacement(NETS_3D, 2)\n",
    "    ):\n",
    "        model_rdm.loc[combi] = 1\n",
    "        model_rdm.loc[tuple(reversed(combi))] = 1\n",
    "    else:\n",
    "        model_rdm.loc[combi] = 0\n",
    "        model_rdm.loc[tuple(reversed(combi))] = 0\n",
    "\n",
    "sns.heatmap(model_rdm, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance partitioning\n",
    "\n",
    "MODEL RDM as target,  steps as predictors <br>\n",
    "shared variance between any of the models, subsequent added explained variance by each of the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get beauty predictions for images from best predicting model in each layer for each class\n",
    "bestnets = (\n",
    "    dfibc_m.unstack(\"layer\")\n",
    "    .groupby(\"class\")\n",
    "    .aggregate(lambda c: c.idxmax()[0])\n",
    "    .stack(\"layer\")\n",
    "    .rename({\"ibcorr\": \"net\"}, axis=1)\n",
    "    .reset_index()\n",
    "    .values.tolist()\n",
    ")\n",
    "dfi_m_best = (\n",
    "    dfi_m.unstack(\"img\")\n",
    "    .reorder_levels([\"class\", \"layer\", \"model\"])\n",
    "    .loc[bestnets, :]\n",
    "    .droplevel(\"model\")\n",
    "    .stack(\"img\")\n",
    "    #    .unstack(\"layer\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi_m_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibcorr_from_classmaxpool = (\n",
    "    dfi_m_best.groupby([\"layer\", \"class\"])\n",
    "    .aggregate(lambda i: pearsonr(-i, beauty_ratings[\"study1_places1_short.csv\"])[0][0])\n",
    "    .unstack(\"class\")\n",
    ")\n",
    "ibcorr_from_classmaxpool.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average beauty prediction (i.e. integration) for each image from each class\n",
    "# since the ib-correlation is the spearman correlation. Just do OLS variance partitioning for now an then talk to daniel about it.\n",
    "ibcorr_from_classmaxpool = (\n",
    "    dfi_m_best.groupby([\"layer\", \"class\"])\n",
    "    .aggregate(lambda i: pearsonr(-i, beauty_ratings[\"study1_places1_short.csv\"])[0][0])\n",
    "    .unstack(\"class\")\n",
    ")\n",
    "ibcorr_from_classmaxpool.plot()\n",
    "\n",
    "ibcorr_from_classavg = (\n",
    "    dfi_m.groupby([\"class\", \"layer\", \"img\"])\n",
    "    .mean()\n",
    "    .groupby([\"layer\", \"class\"])\n",
    "    .aggregate(lambda i: pearsonr(-i, beauty_ratings[\"study1_places1_short.csv\"])[0][0])\n",
    "    .unstack(\"class\")\n",
    ")\n",
    "ibcorr_from_classavg.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi_m_classavg = dfi_m.groupby([\"class\", \"layer\", \"img\"]).mean().unstack(\"class\")\n",
    "dfi_m_classavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beauty_ratings[\"study1_places1_short.csv\"].values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi_m_classavg.loc[:, (slice(None), \"2d\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_all = dfi_m_classavg.groupby([\"layer\"]).apply(\n",
    "    lambda X: OLS(beauty_ratings[\"study1_places1_short.csv\"].values.squeeze(), X.values)\n",
    "    .fit()\n",
    "    .rsquared\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_semantic = r2_all - dfi_m_classavg.loc[:, (slice(None), [\"2d\", \"3d\"])].groupby(\n",
    "    [\"layer\"]\n",
    ").apply(\n",
    "    lambda X: OLS(beauty_ratings[\"study1_places1_short.csv\"].values.squeeze(), X.values)\n",
    "    .fit()\n",
    "    .rsquared\n",
    ")\n",
    "\n",
    "r2_2d = r2_all - dfi_m_classavg.loc[:, (slice(None), [\"semantic\", \"3d\"])].groupby(\n",
    "    [\"layer\"]\n",
    ").apply(\n",
    "    lambda X: OLS(beauty_ratings[\"study1_places1_short.csv\"].values.squeeze(), X.values)\n",
    "    .fit()\n",
    "    .rsquared\n",
    ")\n",
    "\n",
    "\n",
    "r2_3d = r2_all - dfi_m_classavg.loc[:, (slice(None), [\"semantic\", \"2d\"])].groupby(\n",
    "    [\"layer\"]\n",
    ").apply(\n",
    "    lambda X: OLS(beauty_ratings[\"study1_places1_short.csv\"].values.squeeze(), X.values)\n",
    "    .fit()\n",
    "    .rsquared\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r2_3d, label=\"3d\")\n",
    "plt.plot(r2_2d, label=\"2d\")\n",
    "plt.plot(r2_semantic, label=\"semantic\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create linear model\n",
    "# single study & layer\n",
    "dataset = \"places1\"\n",
    "layer_idx = 48\n",
    "\n",
    "df_icr.loc[dataset, layer_idx]\n",
    "\n",
    "OLS(Y, X).fit().rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_variance_partitioning():\n",
    "    # do variance partitioning for one layer\n",
    "    # i.e. for all unique, shared and full combinations of the three predictors\n",
    "    # return dataframe with all R2 values\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## semantic, 2d, 3d along layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw integration amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get beauty predictions for images from best predicting model in each layer for each class\n",
    "bestnets = (\n",
    "    dfibc_m.unstack(\"layer\")\n",
    "    .groupby(\"class\")\n",
    "    .aggregate(lambda c: c.idxmax()[0])\n",
    "    .stack(\"layer\")\n",
    "    .rename({\"ibcorr\": \"net\"}, axis=1)\n",
    "    .reset_index()\n",
    "    .values.tolist()\n",
    ")\n",
    "dfi_m_best = (\n",
    "    dfi_m.unstack(\"img\")\n",
    "    .reorder_levels([\"class\", \"layer\", \"model\"])\n",
    "    .loc[bestnets, :]\n",
    "    .droplevel(\"model\")\n",
    "    .stack(\"img\")\n",
    "    #    .unstack(\"layer\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi_m_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibcorr_from_classmaxpool = (\n",
    "    dfi_m_best.groupby([\"layer\", \"class\"])\n",
    "    .aggregate(lambda i: pearsonr(-i, beauty_ratings[\"study1_places1_short.csv\"])[0][0])\n",
    "    .unstack(\"class\")\n",
    ")\n",
    "ibcorr_from_classmaxpool.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average beauty prediction (i.e. integration) for each image from each class\n",
    "# since the ib-correlation is the spearman correlation. Just do OLS variance partitioning for now an then talk to daniel about it.\n",
    "ibcorr_from_classmaxpool = (\n",
    "    dfi_m_best.groupby([\"layer\", \"class\"])\n",
    "    .aggregate(lambda i: pearsonr(-i, beauty_ratings[\"study1_places1_short.csv\"])[0][0])\n",
    "    .unstack(\"class\")\n",
    ")\n",
    "ibcorr_from_classmaxpool.plot()\n",
    "\n",
    "ibcorr_from_classavg = (\n",
    "    dfi_m.groupby([\"class\", \"layer\", \"img\"])\n",
    "    .mean()\n",
    "    .groupby([\"layer\", \"class\"])\n",
    "    .aggregate(lambda i: pearsonr(-i, beauty_ratings[\"study1_places1_short.csv\"])[0][0])\n",
    "    .unstack(\"class\")\n",
    ")\n",
    "ibcorr_from_classavg.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi_m_classavg = dfi_m.groupby([\"class\", \"layer\", \"img\"]).mean().unstack(\"class\")\n",
    "dfi_m_classavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beauty_ratings[\"study1_places1_short.csv\"].values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi_m_classavg.loc[:, (slice(None), \"2d\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_all = dfi_m_classavg.groupby([\"layer\"]).apply(\n",
    "    lambda X: OLS(beauty_ratings[\"study1_places1_short.csv\"].values.squeeze(), X.values)\n",
    "    .fit()\n",
    "    .rsquared\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_semantic = r2_all - dfi_m_classavg.loc[:, (slice(None), [\"2d\", \"3d\"])].groupby(\n",
    "    [\"layer\"]\n",
    ").apply(\n",
    "    lambda X: OLS(beauty_ratings[\"study1_places1_short.csv\"].values.squeeze(), X.values)\n",
    "    .fit()\n",
    "    .rsquared\n",
    ")\n",
    "\n",
    "r2_2d = r2_all - dfi_m_classavg.loc[:, (slice(None), [\"semantic\", \"3d\"])].groupby(\n",
    "    [\"layer\"]\n",
    ").apply(\n",
    "    lambda X: OLS(beauty_ratings[\"study1_places1_short.csv\"].values.squeeze(), X.values)\n",
    "    .fit()\n",
    "    .rsquared\n",
    ")\n",
    "\n",
    "\n",
    "r2_3d = r2_all - dfi_m_classavg.loc[:, (slice(None), [\"semantic\", \"2d\"])].groupby(\n",
    "    [\"layer\"]\n",
    ").apply(\n",
    "    lambda X: OLS(beauty_ratings[\"study1_places1_short.csv\"].values.squeeze(), X.values)\n",
    "    .fit()\n",
    "    .rsquared\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r2_3d, label=\"3d\")\n",
    "plt.plot(r2_2d, label=\"2d\")\n",
    "plt.plot(r2_semantic, label=\"semantic\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create linear model\n",
    "# single study & layer\n",
    "dataset = \"places1\"\n",
    "layer_idx = 48\n",
    "\n",
    "df_icr.loc[dataset, layer_idx]\n",
    "\n",
    "OLS(Y, X).fit().rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_variance_partitioning():\n",
    "    # do variance partitioning for one layer\n",
    "    # i.e. for all unique, shared and full combinations of the three predictors\n",
    "    # return dataframe with all R2 values\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## semantic, 2d, 3d along layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw integration amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average integration of layers\n",
    "df_int_netavg = df1.groupby(\"model\").mean().transpose()\n",
    "handles, labels = df_int_netavg.plot().get_legend_handles_labels()\n",
    "\n",
    "# already order legend by classes\n",
    "order = [labels.index(netname) for netname in NETS_ALL]\n",
    "plt.legend(\n",
    "    [handles[idx] for idx in order],\n",
    "    [labels[idx] for idx in order],\n",
    "    loc=\"center right\",\n",
    "    bbox_to_anchor=(1.5, 0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average integration, grouped by semantic-2d-3d\n",
    "colors = (\n",
    "    len(NETS_SEMANTIC) * [\"green\"]\n",
    "    + len(NETS_2D) * [\"purple\"]\n",
    "    + len(NETS_3D) * [\"orange\"]\n",
    ")\n",
    "\n",
    "for (netname, int_netavg), color in zip(df_int_netavg.iloc[:, order].items(), colors):\n",
    "    if netname in NETS_SEMANTIC:\n",
    "        alpha = 0.7\n",
    "    else:\n",
    "        alpha = 0.3\n",
    "    plt.plot(int_netavg, label=netname, color=color, alpha=alpha)\n",
    "    plt.legend(loc=\"center right\", bbox_to_anchor=(1.5, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dev: single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_id = 48\n",
    "\n",
    "# fitler relevant data\n",
    "layer_df = pd.DataFrame(df.loc[NETS_ALL, \"places1\", \"scale8\"][layer_id]).reset_index()\n",
    "# needed for pivot into wide format\n",
    "layer_df[\"img_id\"] = layer_df.groupby(\"model\").cumcount()\n",
    "\n",
    "# pivot\n",
    "layer_df = layer_df.pivot(columns=\"model\", index=\"img_id\", values=layer_id)\n",
    "\n",
    "# reorder columns according to semantic-2D-3D nets\n",
    "layer_df = layer_df[NETS_ALL]\n",
    "\n",
    "rdm = calculate_rdm(layer_df, correlation_type=\"spearman\")\n",
    "\n",
    "pearsonr(rdm.values.flatten(), model_rdm.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(rdm.values.flatten(), model_rdm.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(rdm, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdm = rdm[rdm > 0.142].fillna(0)\n",
    "sns.heatmap(xdm, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdm = rdm[rdm < 0].fillna(0)\n",
    "sns.heatmap(xdm, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_correlations = []\n",
    "model_pvalues = []\n",
    "# iterate layers\n",
    "for layer_name, layer_series in df.loc[:, \"places1\", \"scale8\"].items():\n",
    "\n",
    "    # put data back into DataFrame\n",
    "    layer_df = pd.DataFrame(layer_series).reset_index()\n",
    "\n",
    "    # needed for pivot into wide format\n",
    "    layer_df[\"img_id\"] = layer_df.groupby(\"model\").cumcount()\n",
    "\n",
    "    # pivot\n",
    "    layer_df = layer_df.pivot(columns=\"model\", index=\"img_id\", values=layer_name)\n",
    "\n",
    "    # reorder columns according to semantic-2D-3D nets\n",
    "    layer_df = layer_df[NETS_ALL]\n",
    "\n",
    "    rdm = calculate_rdm(layer_df, correlation_type=\"spearman\")\n",
    "\n",
    "    model_correlations.append(\n",
    "        pearsonr(rdm.values.flatten(), model_rdm.values.flatten())[0]\n",
    "    )\n",
    "    model_pvalues.append(pearsonr(rdm.values.flatten(), model_rdm.values.flatten())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "sns.lineplot(data=model_correlations)\n",
    "plt.suptitle(\"Similarity in what is integrated\")\n",
    "plt.title(\"Correlation of taskonomy RDM with model (semantic-2D-3D) RDM\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"pearson correlation\")\n",
    "\n",
    "\n",
    "for x, layer_pvalue in enumerate(model_pvalues):\n",
    "    if layer_pvalue < alpha:\n",
    "        plt.scatter(x, 0, color=\"cyan\", s=100, marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ibcorr differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inspect class average integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per class best and worst prediction from individual nets\n",
    "Y1 = dfibc_m.groupby([\"layer\", \"class\"]).agg([min, max])\n",
    "Y2 = (\n",
    "    dfi_m.groupby([\"class\", \"layer\", \"img\"])\n",
    "    .mean()\n",
    "    .groupby([\"layer\", \"class\"])\n",
    "    .aggregate(lambda i: spearmanr(i, beauty_ratings[\"study1_places1_short.csv\"])[0])\n",
    "    .unstack(\"class\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 3, figsize=(12, 3), sharey=True)\n",
    "\n",
    "modelclass = \"semantic\"\n",
    "axes[0].fill_between(\n",
    "    Y1.loc[(slice(None), modelclass), :].index.get_level_values(\"layer\"),\n",
    "    Y1.loc[(slice(None), modelclass), (slice(None), \"min\")].values.flat,\n",
    "    Y1.loc[(slice(None), modelclass), (slice(None), \"max\")].values.flat,\n",
    ")\n",
    "axes[0].plot(-Y2.loc[:, (slice(None), modelclass)], c=\"red\")\n",
    "\n",
    "modelclass = \"2d\"\n",
    "axes[1].fill_between(\n",
    "    Y1.loc[(slice(None), modelclass), :].index.get_level_values(\"layer\"),\n",
    "    Y1.loc[(slice(None), modelclass), (slice(None), \"min\")].values.flat,\n",
    "    Y1.loc[(slice(None), modelclass), (slice(None), \"max\")].values.flat,\n",
    ")\n",
    "axes[1].plot(-Y2.loc[:, (slice(None), modelclass)], c=\"red\")\n",
    "\n",
    "modelclass = \"3d\"\n",
    "axes[2].fill_between(\n",
    "    Y1.loc[(slice(None), modelclass), :].index.get_level_values(\"layer\"),\n",
    "    Y1.loc[(slice(None), modelclass), (slice(None), \"min\")].values.flat,\n",
    "    Y1.loc[(slice(None), modelclass), (slice(None), \"max\")].values.flat,\n",
    ")\n",
    "axes[2].plot(-Y2.loc[:, (slice(None), modelclass)], c=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictor RDM (semantic-2d-3d)\n",
    "\n",
    "Extend [model rdm](#model-rdm) to contain all models to use as predictor. Fill values for models not belonging to any class with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_rdm_1 = pd.DataFrame(0, index=_models, columns=_models)\n",
    "for c1, c2 in combinations_with_replacement(_models, 2):\n",
    "    if c1 in model_rdm.index and c2 in model_rdm.index:\n",
    "        predictor_rdm_1.loc[c1, c2] = 1 - model_rdm.loc[c1, c2]\n",
    "        predictor_rdm_1.loc[c2, c1] = 1 - model_rdm.loc[c2, c1]\n",
    "    else:\n",
    "        predictor_rdm_1.loc[c1, c2] = 0\n",
    "        predictor_rdm_1.loc[c2, c1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor 2:  integration profile across layers\n",
    "\n",
    "RDM of RDM's that correlate integration ratings of each different layers inside each network.\n",
    "\n",
    "[Finished predictor RDM](#predictor-rdm-layer-layer-similarity-inside-networks)\n",
    "\n",
    "TODO: this essentially the same thing as absoloute correlation differences alone ?\n",
    "copy code for each models layerXlayer RDM\n",
    "correlate correlations using daniels code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 1: layer X layer RDM for each network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfi.loc[(slice(None), slice(None), _scale, slice(None)), :].droplevel([\"scale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_rdms_p1, v1_rdms_p2, v1_rdms_oa = {}, {}, {}\n",
    "for mo, df_mo in df1.groupby(\"model\"):\n",
    "    v1_rdms_p1[mo] = calculate_rdm(df_mo.loc[(slice(None), \"places1\"), :], \"spearman\")\n",
    "    v1_rdms_p2[mo] = calculate_rdm(df_mo.loc[(slice(None), \"places2\"), :], \"spearman\")\n",
    "    v1_rdms_oa[mo] = calculate_rdm(df_mo.loc[(slice(None), \"oasis\"), :], \"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join(PATH_RESULTS, \"layer profile\", \"version 1\")\n",
    "with open(os.path.join(PATH, \"rmds places1.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(v1_rdms_p1, f)\n",
    "\n",
    "with open(os.path.join(PATH, \"rmds places2.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(v1_rdms_p2, f)\n",
    "\n",
    "with open(os.path.join(PATH, \"rmds oasis.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(v1_rdms_oa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join(PATH_RESULTS, \"layer profile\", \"version 1\")\n",
    "with open(os.path.join(PATH, \"rmds places1.pkl\"), \"rb\") as f:\n",
    "    v1_rdms_p1 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PATH, \"rmds places2.pkl\"), \"rb\") as f:\n",
    "    v1_rdms_p2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PATH, \"rmds oasis.pkl\"), \"rb\") as f:\n",
    "    v1_rdms_oa = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 2: integration in best layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ordering of images by integration in best predicting layer\n",
    "\n",
    "\"what is integrated\", alternatively average of correlation between in each layer, howevery layers may not correspond to each other, therefore best predicting layer is more general <br> <br>\n",
    "\n",
    "Interpretation: The differences in absolout values correspond to how similar the \"integration mechanism\" in both networks are.<br> If we assume that beauty perception depends on a specific stage of processing and not the whole processing stream, then the best predicting layer of a network can be interpreted as the point, where the network best mimics the aspects of the processing that determine beauty. <br> \n",
    "\n",
    "If the a similar The value in Is there a single or are there different ways of predicting beauty ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best layer per model\n",
    "bestlayers = dfibc_m.groupby(\"model\").idxmax().ibcorr\n",
    "df_bestpredicting_integration = (\n",
    "    dfi_m.unstack(\"img\").loc[bestlayers].droplevel(\"layer\").T.droplevel(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictor RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1: correlate network RDMs\n",
    "# RDMs into columns\n",
    "# throw out zeros on diagonal to avoid skewing correlation (standard RSA procedure)\n",
    "RDMs_places1 = pd.DataFrame(columns=_models)\n",
    "\n",
    "for network_name, rdm in v1_rdms_p1.items():\n",
    "    # mark diagonal values (all zeros)for removal\n",
    "    np.fill_diagonal(rdm.values, np.nan)\n",
    "    RDMs_places1.loc[:, network_name] = rdm.values.flatten()\n",
    "\n",
    "# removed marked diagonal values\n",
    "RDMs_places1 = RDMs_places1.dropna()\n",
    "\n",
    "print(\"Should be (2353, 15)\")\n",
    "print(RDMs_places1.shape)\n",
    "\n",
    "predictor_rdm_2 = calculate_rdm(RDMs_places1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2: only best layer\n",
    "predictor_rdm_2 = calculate_rdm(df_bestpredicting_integration, \"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor 3: spatial integration\n",
    "\n",
    "\"how\"\n",
    "\n",
    "\"where\" or alternatively \"what\",  which is the same because its spatial integration. Check for correlation between the what (represented by the integration ratings).\n",
    "\n",
    "DONE IN SEPERATE NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import results\n",
    "# with open(os.path.join(PATH_RESULTS, \"spatial integration\", \"study1.pkl\"), \"rb\") as f:\n",
    "#     d1 = pickle.load(f)\n",
    "\n",
    "# with open(os.path.join(PATH_RESULTS, \"spatial integration\", \"study2.pkl\"), \"rb\") as f:\n",
    "#     d2 = pickle.load(f)\n",
    "\n",
    "# with open(os.path.join(PATH_RESULTS, \"spatial integration\", \"study3.pkl\"), \"rb\") as f:\n",
    "#     d3 = pickle.load(f)\n",
    "\n",
    "# with open(os.path.join(PATH_RESULTS, \"spatial integration\", \"study4.pkl\"), \"rb\") as f:\n",
    "#     d4 = pickle.load(f)\n",
    "\n",
    "\n",
    "# rdm_study1 = calculate_rdm(flatten_concat(d1))\n",
    "# rdm_study2 = calculate_rdm(flatten_concat(d2))\n",
    "# rdm_study3 = calculate_rdm(flatten_concat(d3))\n",
    "# rdm_study4 = calculate_rdm(flatten_concat(d4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_study = \"short presentation\"\n",
    "_scale = \"scale4\"\n",
    "\n",
    "\n",
    "rdms = {}\n",
    "\n",
    "for mo in _models:\n",
    "    rdm = pd.read_csv(\n",
    "        os.path.join(\n",
    "            PATH_RESULTS,\n",
    "            \"spatial integration\",\n",
    "            _scale,\n",
    "            _study,\n",
    "            mo + \".csv\",\n",
    "        ),\n",
    "        header=None,\n",
    "    )\n",
    "    rdm = rdm.iloc[1:, 1:].reset_index(drop=True).T.reset_index(drop=True).T\n",
    "\n",
    "    rdms[mo] = rdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(spatial_rdm, xticklabels=spatial_rdm.index, yticklabels=spatial_rdm.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## integration is localized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize node score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize within layer heatmaps\n",
    "\n",
    "# exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial correlation per image per net, correlate these netXnet\n",
    "# test if integration scores are still correlating to beauty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predctor RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_rdm_3 = spatial_rdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictors-Target analysis\n",
    "do for each study and each scale, to check if there is some consistency in which factors always comes out on top\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_predictors_target_rdms(\n",
    "    path_integration, path_ibcorr, _models, _study, _scale, _layers\n",
    "):\n",
    "    _dataset = studyname2datasetname(_study)\n",
    "    blocking = None\n",
    "\n",
    "    dfi = load_integration(path_integration)\n",
    "    dfibc = load_ibcorr(path_ibcorr)\n",
    "\n",
    "    dfi = dfi.sort_index().loc[(_models, _dataset, _scale, slice(None), _layers)]\n",
    "    dfi = dfi.droplevel([\"dataset\", \"scale\"])\n",
    "\n",
    "    dfibc = dfibc.sort_index().loc[(_models, _study, _scale, _layers)]\n",
    "    dfibc = dfibc.droplevel([\"study\", \"scale\"])\n",
    "\n",
    "    # --- target ---\n",
    "    target_rdm = pd.DataFrame(0, index=_models, columns=_models)\n",
    "    df = dfibc.unstack(\"layer\")\n",
    "    for c1, c2 in combinations(_models, 2):\n",
    "        target_rdm.loc[c1, c2] = (df.loc[c1] - df.loc[c2]).abs().sum()\n",
    "        target_rdm.loc[c2, c1] = (df.loc[c1] - df.loc[c2]).abs().sum()\n",
    "\n",
    "    # --- predictor 1 ---\n",
    "    model_rdm = pd.DataFrame(\n",
    "        np.full((len(NETS_ALL), len(NETS_ALL)), np.nan),\n",
    "        columns=NETS_ALL,\n",
    "        index=NETS_ALL,\n",
    "    )\n",
    "    for combi in combinations_with_replacement(NETS_ALL, 2):\n",
    "        if (\n",
    "            combi in combinations_with_replacement(NETS_SEMANTIC, 2)\n",
    "            or combi in combinations_with_replacement(NETS_2D, 2)\n",
    "            or combi in combinations_with_replacement(NETS_3D, 2)\n",
    "        ):\n",
    "            model_rdm.loc[combi] = 1\n",
    "            model_rdm.loc[tuple(reversed(combi))] = 1\n",
    "        else:\n",
    "            model_rdm.loc[combi] = 0\n",
    "            model_rdm.loc[tuple(reversed(combi))] = 0\n",
    "\n",
    "    predictor_rdm_1 = pd.DataFrame(0, index=_models, columns=_models)\n",
    "    for c1, c2 in combinations_with_replacement(_models, 2):\n",
    "        if c1 in model_rdm.index and c2 in model_rdm.index:\n",
    "            predictor_rdm_1.loc[c1, c2] = 1 - model_rdm.loc[c1, c2]\n",
    "            predictor_rdm_1.loc[c2, c1] = 1 - model_rdm.loc[c2, c1]\n",
    "        else:\n",
    "            predictor_rdm_1.loc[c1, c2] = 0\n",
    "            predictor_rdm_1.loc[c2, c1] = 0\n",
    "    \n",
    "    # --- predictor 2---\n",
    "    bestlayers = dfibc.groupby(\"model\").idxmax().ibcorr\n",
    "    df_bestpredicting_integration = (\n",
    "        dfi.unstack(\"img\").loc[bestlayers].droplevel(\"layer\").T.droplevel(0)\n",
    "    )\n",
    "    predictor_rdm_2 = calculate_rdm(df_bestpredicting_integration, \"spearman\")\n",
    "\n",
    "    # ---predictor 3---\n",
    "    try:\n",
    "        rdms = {}\n",
    "        for mo in _models:\n",
    "            rdm = pd.read_csv(\n",
    "                os.path.join(\n",
    "                    PATH_RESULTS,\n",
    "                    \"spatial integration\",\n",
    "                    _scale,\n",
    "                    _study,\n",
    "                    mo + \".csv\",\n",
    "                ),\n",
    "                header=None,\n",
    "            )\n",
    "            rdm = rdm.iloc[1:, 1:].reset_index(drop=True).T.reset_index(drop=True).T\n",
    "            rdms[mo] = rdm\n",
    "        predictor_rdm_3 = calculate_rdm(flatten_concat_rdms(rdms))\n",
    "    except Exception as e:\n",
    "        print(\"Spatial analysis not available for these parameters, returing emptry predictor 3\")\n",
    "        predictor_rdm_3 = None\n",
    "\n",
    "    return target_rdm, predictor_rdm_1, predictor_rdm_2, predictor_rdm_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_rdms(rdm1, rdm2, correlation=\"pearson\"):\n",
    "\n",
    "    mask = np.triu(np.ones_like(rdm1.values).astype(np.bool_), k=1)\n",
    "    if correlation == \"pearson\":\n",
    "        return pearsonr(rdm1.values[mask], rdm2.values[mask])\n",
    "\n",
    "    if correlation == \"spearman\":\n",
    "        return spearmanr(rdm1.values[mask], rdm2.values[mask])\n",
    "\n",
    "    raise ValueError(\n",
    "        \"corrrelate_rdm muss für correlation pearson oder spearman bekommen\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdm2vec(rdm):\n",
    "    mask = np.triu(np.ones_like(rdm.values).astype(np.bool_), k=1)\n",
    "    return rdm.values[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"blocking:\",\n",
    "    _blocking,\n",
    "    \" | \",\n",
    "    len(_models),\n",
    "    \" | \",\n",
    "    _study,\n",
    "    \" | \",\n",
    "    _scale,\n",
    "    \" | \",\n",
    "    _layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor 1 alone\n",
    "print(\n",
    "    \"blocking:\",\n",
    "    _blocking,\n",
    "    \" | \",\n",
    "    len(_models),\n",
    "    \" | \",\n",
    "    _study,\n",
    "    \" | \",\n",
    "    _scale,\n",
    "    \" | \",\n",
    "    _layers,\n",
    ")\n",
    "correlate_rdms(target_rdm, predictor_rdm_1, correlation=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"blocking:\",\n",
    "    _blocking,\n",
    "    \" | \",\n",
    "    len(_models),\n",
    "    \" | \",\n",
    "    _study,\n",
    "    \" | \",\n",
    "    _scale,\n",
    "    \" | \",\n",
    "    _layers,\n",
    ")\n",
    "correlate_rdms(target_rdm, predictor_rdm_1, correlation=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"blocking:\",\n",
    "    _blocking,\n",
    "    \" | \",\n",
    "    len(_models),\n",
    "    \" | \",\n",
    "    _study,\n",
    "    \" | \",\n",
    "    _scale,\n",
    "    \" | \",\n",
    "    _layers,\n",
    ")\n",
    "correlate_rdms(target_rdm, predictor_rdm_1, correlation=\"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, pred1, pred2, pred3 = calculate_predictors_target_rdms(PATH_INTEGRATION, PATH_IBCORR, MODEL_NAMES, \"short presentation\", \"scale4\", slice(None))\n",
    "correlate_rdms(target, pred1), correlate_rdms(target, pred2), correlate_rdms(target, pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, pred1, pred2, pred3 = calculate_predictors_target_rdms(PATH_INTEGRATION, PATH_IBCORR, _models, \"short presentation\", \"scale4\", slice(None))\n",
    "correlate_rdms(target, pred1), correlate_rdms(target, pred2), correlate_rdms(target, pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, pred1, pred2, pred3 = calculate_predictors_target_rdms(PATH_INTEGRATION_MAXPOOL, PATH_IBCORR_MAXPOOL, MODEL_NAMES, \"short presentation\", \"scale4\", slice(None))\n",
    "correlate_rdms(target, pred1), correlate_rdms(target, pred2), correlate_rdms(target, pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, pred1, pred2, pred3 = calculate_predictors_target_rdms(PATH_INTEGRATION_MAXPOOL, PATH_IBCORR_MAXPOOL, _models, \"short presentation\", \"scale4\", slice(None))\n",
    "correlate_rdms(target, pred1), correlate_rdms(target, pred2), correlate_rdms(target, pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, pred1, pred2, pred3 = calculate_predictors_target_rdms(PATH_INTEGRATION_AVERAGE, PATH_IBCORR_AVERAGE, MODEL_NAMES, \"short presentation\", \"scale4\", slice(6,14+1))\n",
    "correlate_rdms(target, pred1), correlate_rdms(target, pred2), correlate_rdms(target, pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, pred1, pred2, pred3 = calculate_predictors_target_rdms(PATH_INTEGRATION_MAXPOOL, PATH_IBCORR_MAXPOOL, _models, \"short presentation\", \"scale4\", slice(6,14+1))\n",
    "correlate_rdms(target, pred1), correlate_rdms(target, pred2), correlate_rdms(target, pred3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for variance partitioning\n",
    "def predictors_r2(predictors, target):\n",
    "    predictors = sm.add_constant(predictors)\n",
    "    model = sm.OLS(target, predictors)\n",
    "    results = model.fit()\n",
    "    return results.rsquared    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, p1, p2, p3 = rdm2vec(target), rdm2vec(pred1), rdm2vec(pred2), rdm2vec(pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_r2(p1, t), predictors_r2(p2, t), predictors_r2(p3, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_r2(np.stack((p1, p3), axis=1), t) - predictors_r2(p3, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot full venn diagram (with 2 or 3 predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictor 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(t, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(t, p3, s=10, color=\"magenta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(pd.DataFrame((t, p3)).T, x=0, y=1, fill=True,levels=100)\n",
    "plt.scatter(t, p3, s=10, color=\"magenta\")\n",
    "plt.xlim(0,t.max())\n",
    "plt.ylim(0,p1.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(t.argsort(), p3.argsort(), s=10, color=\"magenta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(pd.DataFrame((t.argsort(), p3.argsort())).T, x=0, y=1, fill=True,levels=3)\n",
    "plt.scatter(t.argsort(), p3.argsort(), s=10, color=\"magenta\")\n",
    "plt.xlim(0,t.size)\n",
    "plt.ylim(0,t.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
