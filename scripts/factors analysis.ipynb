{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import os, sys, pickle\n",
    "from itertools import combinations_with_replacement\n",
    "from collections import OrderedDict\n",
    "\n",
    "# stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "\n",
    "# neural networks\n",
    "import torch, torch.utils.model_zoo # required to load nets\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "\n",
    "# analysis code\n",
    "from lib.transforms import VisualPriorRepresentation\n",
    "from lib.functions_second_analysis import *\n",
    "\n",
    "DATASET_NAMES               = ('places1', 'places2', 'oasis')\n",
    "SCALE_NAMES                 = ('scale2','scale4','scale8','scale16','scale32')\n",
    "STUDY_NAMES                 = (\"short presentation\", \"long presentation\", \"complexity order\")\n",
    "BEHAVIOUR_NAMES             = ('study1_places1_short.csv','study2_places1.csv','study3_places2.csv','study4_oasis.csv')\n",
    "\n",
    "PATH_IMAGES                 = '../images and ratings/imageversions_256'\n",
    "PATH_RATINGS                = '../images and ratings/ratings'\n",
    "PATH_INTEGRATION_VALUES     = '../data csv/integration'\n",
    "PATH_IB_CORRELATIONS        = '../data csv/ibcorr'\n",
    "PATH_IB_CORRELATIONS_BLOCKED= '../data csv/ibcorr blocked'\n",
    "\n",
    "PATH_RESULTS                = '../results'\n",
    "PATH_PLOTS                  = '../plots'\n",
    "\n",
    "#VisualPrior.viable_feature_tasks\n",
    "MODEL_NAMES = ('autoencoding','depth_euclidean','jigsaw','reshading',\n",
    "               'edge_occlusion','keypoints2d','room_layout', #'colorization' currently not working\n",
    "               'curvature','edge_texture','keypoints3d','segment_unsup2d',\n",
    "               'class_object','egomotion','nonfixated_pose','segment_unsup25d',\n",
    "               'class_scene','fixated_pose','normal','segment_semantic',\n",
    "               'denoising','inpainting','point_matching','vanishing_point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rdm(data: pd.DataFrame, correlation_type : str = \"pearson\"):\n",
    "    \"\"\"Calculate RDM with pearson/spearman correlation for every combination of columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "        Input with data to correlate in the columns\n",
    "\n",
    "    correlation_type: str\n",
    "        Which correlation to use. \"pearson\" (default) or \"spearman\".\n",
    "\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        representational dissimilarity matrix of inputs' columns\n",
    "    \n",
    "    \"\"\"\n",
    "    num_columns = data.shape[1]\n",
    "\n",
    "    # create empty matrix to store RDM\n",
    "    # index and column labels are in order of input columns\n",
    "    rdm = pd.DataFrame(np.full((num_columns, num_columns), np.nan), columns=data.columns, index=data.columns)\n",
    "    \n",
    "    for col1, col2 in combinations_with_replacement(data.columns, 2):\n",
    "        # there's one NaN in the autoencoding integration values, filter this here, don't know why that happens\n",
    "        co11_col2 = data[[col1,col2]].dropna()\n",
    "        \n",
    "        # calculate correlation\n",
    "        if correlation_type == \"pearson\":\n",
    "            corr = pearsonr(co11_col2.values[:,0], co11_col2.values[:,1])[0]\n",
    "        elif correlation_type == \"spearman\":\n",
    "            corr = spearmanr(co11_col2.values[:,0], co11_col2.values[:,1])[0]\n",
    "\n",
    "        # fill upper and lower triangular matrix\n",
    "        rdm.loc[col1, col2] = corr\n",
    "        rdm.loc[col2, col1] = corr\n",
    "        rdm.loc[col1, col1] = 0.0\n",
    "\n",
    "    return rdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# integration-beauty correlation in Taskonomy\n",
    "\n",
    "nets order by peak integration beauty correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import blocked integration beauty correlation for taskonomy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "data_list = []\n",
    "\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    for study_name in STUDY_NAMES:\n",
    "        for scale_name in SCALE_NAMES:\n",
    "\n",
    "            data = pd.read_csv(os.path.join(DATA_PATH, model_name, study_name, scale_name, 'ib_correlations.csv'), header=None)\n",
    "            data.insert(0, 'scale', scale_name)\n",
    "            data.insert(0, 'study',study_name)\n",
    "            data.insert(0, 'model', model_name)\n",
    "\n",
    "            data_list.append(data)\n",
    "            #selfsimilarity.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'selfsimilarity.csv'), index=False, header=False)           \n",
    "            #l2norm.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'l2norm.csv'), index=False, header=False)\n",
    "\n",
    "# add layer labels\n",
    "data_list = [data.reset_index().rename(columns={\"index\":\"layer\", 0:\"ibcorr\"}) for data in data_list]\n",
    "\n",
    "# combine into one DataFrame\n",
    "dfc = pd.concat(data_list).reset_index(drop=True).set_index(['model','study','scale','layer'])\n",
    "dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import blocked integration beauty p-values for taskonomy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "data_list = []\n",
    "\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    for study_name in STUDY_NAMES:\n",
    "        for scale_name in SCALE_NAMES:\n",
    "\n",
    "            data = pd.read_csv(os.path.join(DATA_PATH, model_name, study_name, scale_name, 'ib_correlations_pvalues.csv'), header=None)\n",
    "            data.insert(0, 'scale', scale_name)\n",
    "            data.insert(0, 'study',study_name)\n",
    "            data.insert(0, 'model', model_name)\n",
    "\n",
    "            data_list.append(data)\n",
    "            #selfsimilarity.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'selfsimilarity.csv'), index=False, header=False)           \n",
    "            #l2norm.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'l2norm.csv'), index=False, header=False)\n",
    "\n",
    "# add layer labels\n",
    "data_list = [data.reset_index().rename(columns={\"index\":\"layer\", 0:\"ibcorr pvalue\"}) for data in data_list]\n",
    "\n",
    "# combine into one DataFrame\n",
    "dfp = pd.concat(data_list).reset_index(drop=True).set_index(['model','study','scale','layer'])\n",
    "dfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc_plot = (dfc\n",
    "           .loc[:,\"short presentation\",\"scale8\",:]\n",
    "           .reset_index()\n",
    "           .pivot(index=\"model\", columns=\"layer\", values=\"ibcorr\"))\n",
    "\n",
    "dfp_plot = (dfp\n",
    "           .loc[:,\"short presentation\",\"scale8\",:]\n",
    "           .reset_index()\n",
    "           .pivot(index=\"model\", columns=\"layer\", values=\"ibcorr pvalue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortorder = dfc_plot.max(axis=1).argsort().values[-1::-1]\n",
    "dfc_plot = dfc_plot.iloc[sortorder,:]\n",
    "dfp_plot = dfp_plot.iloc[sortorder,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnames = list(dfc_plot.index.get_level_values(\"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FDR correction\n",
    "significant = dfp_plot < (0.05 / 17)\n",
    "significant = ~significant\n",
    "significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only plot significant clusters\n",
    "sns.heatmap(dfc_plot, yticklabels=modelnames, mask=significant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation differences of Taskonomy models\n",
    "\n",
    "absoloute difference in correlation in each layer, summed up, normalized with 2 (spearman correlation range) * num_layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "dfc= (dfc\n",
    "      .loc[:,\"short presentation\",\"scale8\",:]\n",
    "      .reset_index()\n",
    "      .pivot(index=\"model\", columns=\"layer\", values=\"ibcorr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dissimilarity: 17 * 2 - sum of abs diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm = pd.DataFrame(np.full((len(MODEL_NAMES), len(MODEL_NAMES)), np.nan), columns=MODEL_NAMES, index=MODEL_NAMES)\n",
    "print(rdm.shape)\n",
    "\n",
    "for model1, model2 in combinations_with_replacement(MODEL_NAMES, 2):\n",
    "    rdm.loc[model1, model2] = np.abs(dfc.loc[model1] - dfc.loc[model2]).sum() / (17 * 2)\n",
    "    rdm.loc[model2, model1] = np.abs(dfc.loc[model2] - dfc.loc[model1]).sum() / (17 * 2)\n",
    "\n",
    "rdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(rdm, xticklabels=rdm.columns, yticklabels=rdm.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor 1: semantic-2d-3d\n",
    "[Finished predictor RDM ](#predictor-rdm-semantic-2d-3d)\n",
    "\n",
    "##### TODO <br>\n",
    "> How to handle blocking of layers (take best of each block OR average) ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETS_SEMANTIC = ['class_object','class_scene','segment_semantic']\n",
    "\n",
    "# from radek paper missing: colorization (not downloadable from taskonomy)\n",
    "NETS_2D = ['autoencoding','denoising','edge_texture','inpainting','keypoints2d','segment_unsup2d']\n",
    "\n",
    "# from radek paper missing: z-depth (missing from importing as well) and distance (but this is not a network after all)\n",
    "NETS_3D = ['edge_occlusion','keypoints3d','segment_unsup25d','reshading','normal','curvature']\n",
    "\n",
    "NETS_ALL = NETS_SEMANTIC + NETS_2D + NETS_3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load integration data and beauty ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "data_list = []\n",
    "\n",
    "for model_name in NETS_ALL:\n",
    "    for dataset_name in DATASET_NAMES:\n",
    "        data = pd.read_csv(os.path.join(PATH_INTEGRATION_VALUES, model_name, dataset_name, 'scale8', 'correlations.csv'), header=None)\n",
    "        data = data.reset_index().rename({'index':'img id'}, axis=1)\n",
    "\n",
    "        #data.insert(0, 'scale', scale_name)\n",
    "        data.insert(0, 'dataset',dataset_name)\n",
    "        data.insert(0, 'model', model_name)\n",
    "\n",
    "        if model_name in NETS_SEMANTIC:\n",
    "            data.insert(0, 'class', 'semantic')\n",
    "        elif model_name in NETS_2D:\n",
    "            data.insert(0, 'class', '2d')\n",
    "        elif model_name in NETS_3D:\n",
    "            data.insert(0, 'class', '3d')\n",
    "\n",
    "        data_list.append(data)\n",
    "        #selfsimilarity.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'selfsimilarity.csv'), index=False, header=False)           \n",
    "        #l2norm.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'l2norm.csv'), index=False, header=False)\n",
    "\n",
    "# convert correlation to integration\n",
    "df_int = - pd.concat(data_list).set_index(['model','dataset','class','img id'])\n",
    "df_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beauty_ratings = {}\n",
    "for ratings_name in BEHAVIOUR_NAMES:\n",
    "    \n",
    "    data = (\n",
    "        pd.read_csv(os.path.join(PATH_BEHAVIOUR, ratings_name),header=None)\n",
    "        .mean(axis=1)\n",
    "        .to_frame()\n",
    "        .rename({0:'beauty rating'}, axis=1)\n",
    "        )\n",
    "    data.index.name = 'img_id'\n",
    "\n",
    "    # add name of study to index\n",
    "    beauty_ratings[ratings_name] = pd.concat([data], names=['dataset'], keys=[ratings_name])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize average integration of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int_netavg = df_int.groupby('model').mean().transpose()\n",
    "df_int_netavg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles, labels = df_int_netavg.plot().get_legend_handles_labels()\n",
    "\n",
    "# already order legend by classes\n",
    "order = [labels.index(netname) for netname in NETS_ALL]\n",
    "plt.legend([handles[idx] for idx in order], [labels[idx] for idx in order], loc='center right',bbox_to_anchor = (1.5, .5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grouped by semantic-2d-3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = len(NETS_SEMANTIC) * ['green'] + len(NETS_2D) * ['purple'] + len(NETS_3D) * ['orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (netname, int_netavg), color in zip(df_int_netavg.iloc[:,order].items(), colors):\n",
    "    if netname in NETS_SEMANTIC:\n",
    "        alpha = .7\n",
    "    else:\n",
    "        alpha = .3\n",
    "    plt.plot(int_netavg, label=netname, color=color, alpha=alpha)\n",
    "    plt.legend(loc='center right',bbox_to_anchor = (1.5, .5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model RDM for semantiv-2D-3D nets integration\n",
    "model_rdm = pd.DataFrame(\n",
    "        np.full((len(NETS_ALL), len(NETS_ALL)), np.nan),\n",
    "        columns=NETS_ALL, index=NETS_ALL)\n",
    "\n",
    "for combi in combinations_with_replacement(NETS_ALL,2):\n",
    "    if combi in combinations_with_replacement(NETS_SEMANTIC,2) or \\\n",
    "        combi in combinations_with_replacement(NETS_2D,2) or \\\n",
    "        combi in combinations_with_replacement(NETS_3D,2):\n",
    "        model_rdm.loc[combi] = 1\n",
    "        model_rdm.loc[tuple(reversed(combi))] = 1\n",
    "    else:\n",
    "        model_rdm.loc[combi] = 0\n",
    "        model_rdm.loc[tuple(reversed(combi))] = 0\n",
    "\n",
    "sns.heatmap(model_rdm, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlate RDM with model-RDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_id = 48\n",
    "\n",
    "# fitler relevant data\n",
    "layer_df = pd.DataFrame(df.loc[NETS_ALL,\"places1\", \"scale8\"][layer_id]).reset_index()\n",
    "# needed for pivot into wide format\n",
    "layer_df[\"img_id\"] = layer_df.groupby(\"model\").cumcount()\n",
    "\n",
    "# pivot\n",
    "layer_df = layer_df.pivot(columns=\"model\", index=\"img_id\", values=layer_id)\n",
    "\n",
    "# reorder columns according to semantic-2D-3D nets\n",
    "layer_df = layer_df[NETS_ALL]\n",
    "\n",
    "rdm = calculate_rdm(layer_df, correlation_type=\"spearman\")\n",
    "\n",
    "pearsonr(rdm.values.flatten(), model_rdm.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(rdm.values.flatten(), model_rdm.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(rdm, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdm = rdm[rdm > .142].fillna(0)\n",
    "sns.heatmap(xdm, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdm = rdm[rdm < 0].fillna(0)\n",
    "sns.heatmap(xdm, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_correlations = []\n",
    "model_pvalues = []\n",
    "# iterate layers\n",
    "for layer_name, layer_series in df.loc[:,\"places1\", \"scale8\"].items():\n",
    "\n",
    "    # put data back into DataFrame\n",
    "    layer_df = pd.DataFrame(layer_series).reset_index()\n",
    "\n",
    "    # needed for pivot into wide format\n",
    "    layer_df[\"img_id\"] = layer_df.groupby(\"model\").cumcount()\n",
    "\n",
    "    # pivot\n",
    "    layer_df = layer_df.pivot(columns=\"model\", index=\"img_id\", values=layer_name)\n",
    "\n",
    "    # reorder columns according to semantic-2D-3D nets\n",
    "    layer_df = layer_df[NETS_ALL]\n",
    "\n",
    "    rdm = calculate_rdm(layer_df, correlation_type=\"spearman\")\n",
    "\n",
    "    model_correlations.append(pearsonr(rdm.values.flatten(), model_rdm.values.flatten())[0])\n",
    "    model_pvalues.append(pearsonr(rdm.values.flatten(), model_rdm.values.flatten())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "sns.lineplot(data=model_correlations)\n",
    "plt.suptitle(\"Similarity in what is integrated\")\n",
    "plt.title(\"Correlation of taskonomy RDM with model (semantic-2D-3D) RDM\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"pearson correlation\")\n",
    "\n",
    "\n",
    "for x, layer_pvalue in enumerate(model_pvalues):\n",
    "    if layer_pvalue < alpha:\n",
    "        plt.scatter(x, 0, color='cyan', s=100, marker='o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variance partitioning of model classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average model classes\n",
    "\n",
    "average integration values for each image from each category of networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int_classes = df_int.groupby(['dataset','img id', 'class']).mean()\n",
    "df_int_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to rank data\n",
    "since the ib-correlation is the spearman correlation\n",
    "\n",
    "just do OLS variance partitioning for now an then talk to daniel about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int_classes.columns.name = 'layer'\n",
    "df_int_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int_classes_ranks = (\n",
    "    df_int_classes\n",
    "    .unstack('class')\n",
    "    .groupby('dataset')\n",
    "    .rank()\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "df_icr = df_int_classes_ranks\n",
    "\n",
    "df_icr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### beauty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beauty_ratings_rank = (\n",
    "    pd.concat(beauty_ratings.values())\n",
    "    .groupby('dataset')\n",
    "    .rank()\n",
    "    .astype(int))\n",
    "\n",
    "df_brr = df_beauty_ratings_rank\n",
    "df_brr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single study & layer\n",
    "dataset = 'places1'\n",
    "layer_idx = 48\n",
    "\n",
    "df_icr.loc[dataset,layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_id, layer in df_icr.groupby(level='layer', axis=1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_id, layer_dataset in layer.groupby(level='dataset', axis=0):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icr.columns.get_level_values('layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icr.index.get_level_values('dataset').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.nan,\n",
    "             index=df_icr.index.get_level_values('dataset').unique(),\n",
    "             columns=df_icr.columns.get_level_values('layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to present all these R2 values ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_brr.loc['study3_places2.csv',:].values\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = h.loc[:,(slice(None),'2d')].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLS(Y, X).fit().rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_variance_partitioning():\n",
    "    # do variance partitioning for one layer\n",
    "    # i.e. for all unique, shared and full combinations of the three predictors\n",
    "    # return dataframe with all R2 values\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictor RDM (semantic-2d-3d)\n",
    "\n",
    "The [model rdm](#model-rdm) is used as the predictors representing the semantic-2d-3d categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predictor 2: integration in best layer\n",
    "ordering of images by integration in best predicting layer\n",
    "\n",
    "\"what is integrated\", alternatively average of correlation between in each layer, howevery layers may not correspond to each other, therefore best predicting layer is more general <br> <br>\n",
    "\n",
    "`Interpretation`: The differences in absolout values correspond to how similar the \"integration mechanism\" in both networks are.<br> If we assume that beauty perception depends on a specific stage of processing and not the whole processing stream, then the best predicting layer of a network can be interpreted as the point, where the network best mimics the aspects of the processing that determine beauty. <br> \n",
    "\n",
    "If the a similar The value in Is there a single or are there different ways of predicting beauty ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New predictor: Image representations of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get best predicting layer in each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlate integration values of best predicting layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predictor 3:  integration profile across layers\n",
    "\n",
    "RDM of RDM's that correlate integration ratings of each different layers inside each network.\n",
    "\n",
    "[Finished predictor RDM](#predictor-rdm-layer-layer-similarity-inside-networks)\n",
    "\n",
    "?: \"how strong\".\n",
    "\n",
    "##### TODO\n",
    "> Is this essentially the same thing as absoloute correlation differences alone ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code for each models layerXlayer RDM\n",
    "# correlate correlations using daniels code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load integration data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same [integration data](#load-integration-data-and-beauty-ratings) as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df_int.droplevel('class') # don't need that here\n",
    "df_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layer X layer RDM for each network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_layer_rdms_places1 = {}\n",
    "layer_layer_rdms_places2 = {}\n",
    "layer_layer_rdms_oasis = {}\n",
    "\n",
    "for model_name, model_integration in df_int.groupby('model'):\n",
    "    layer_layer_rdms_places1[model_name] = calculate_rdm(model_integration.loc[(slice(None),'places1'),:])\n",
    "    layer_layer_rdms_places2[model_name] = calculate_rdm(model_integration.loc[(slice(None),'places2'),:])\n",
    "    layer_layer_rdms_oasis[model_name] = calculate_rdm(model_integration.loc[(slice(None),'oasis'),:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlate network RDMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put RDMs into columns\n",
    "\n",
    "throw out zeros on diagonal to avoid skewing correlation (standard RSA procedure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDMs_places1  = pd.DataFrame(columns=layer_layer_rdms_places1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for network_name, rdm in layer_layer_rdms_places1.items():\n",
    "    # mark diagonal values (all zeros)for removal\n",
    "    np.fill_diagonal(rdm.values, np.nan)\n",
    "    RDMs_places1.loc[:,network_name] = rdm.values.flatten()\n",
    "\n",
    "# removed marked diagonal values\n",
    "RDMs_places1 = RDMs_places1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Should be (2353, 15)\")\n",
    "RDMs_places1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictor RDM (layer-layer similarity inside networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rdm(RDMs_places1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predictor 4: spatial integration\n",
    "\n",
    "\"how\"\n",
    "\n",
    "\"where\" or alternatively \"what\",  which is the same because its spatial integration. Check for correlation between the what (represented by the integration ratings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## integration is localized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run integration searchlight analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize node score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize within layer heatmaps\n",
    "\n",
    "# exemplars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial correlation per image per net, correlate these netXnet\n",
    "# test if integration scores are still correlating to beauty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOK AT SEPERATE NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predictors explaining correlation differences\n",
    "do for each study and each scale, to check if there is some consistency in which factors always comes out on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance partitioning between different predictors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
