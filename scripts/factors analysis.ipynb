{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import os, sys, pickle\n",
    "from itertools import combinations_with_replacement, combinations, product\n",
    "from collections import OrderedDict\n",
    "\n",
    "# stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "\n",
    "# neural networks\n",
    "import torch, torch.utils.model_zoo  # required to load nets\n",
    "from torchvision.models.feature_extraction import (\n",
    "    get_graph_node_names,\n",
    "    create_feature_extractor,\n",
    ")\n",
    "\n",
    "# analysis code\n",
    "from lib.transforms import VisualPriorRepresentation\n",
    "from lib.functions_second_analysis import *\n",
    "from lib.functions_scripting import (\n",
    "    DATASET_NAMES,\n",
    "    SCALE_NAMES,\n",
    "    STUDY_NAMES,\n",
    "    BEHAVIOUR_NAMES,\n",
    "    MODEL_NAMES,\n",
    "    load_integration,\n",
    "    load_ibcorr,\n",
    "    load_pvalues,\n",
    "    load_ratings,\n",
    "    NETS_SEMANTIC,\n",
    "    NETS_2D,\n",
    "    NETS_3D,\n",
    "    NETS_ALL,\n",
    ")\n",
    "\n",
    "PATH_IMAGES = \"../images and ratings/imageversions_256\"\n",
    "PATH_RATINGS = \"../images and ratings/ratings\"\n",
    "PATH_INTEGRATION = (\n",
    "    \"../data csv/integration\"  # !! correlations, invert sign for integration\n",
    ")\n",
    "PATH_INTEGRATION_MAXPOOL = (\n",
    "    \"../data csv/integration maxpool\"  # !! correlations, invert sign for integration\n",
    ")\n",
    "PATH_IBCORR = \"../data csv/ibcorr\"\n",
    "PATH_IBCORR_MAXPOOL = \"../data csv/ibcorr maxpool\"\n",
    "\n",
    "PATH_RESULTS = \"../results\"\n",
    "PATH_PLOTS = \"../plots\"\n",
    "\n",
    "_models = MODEL_NAMES\n",
    "_study = \"short presentation\"\n",
    "_dataset = \"places1\"\n",
    "_scale = \"scale4\"\n",
    "_layers_unblocked = slice(None)\n",
    "_layers_blocked = slice(6,14)\n",
    "\n",
    "# load data\n",
    "dfi = load_integration(PATH_INTEGRATION)\n",
    "dfi_m = load_integration(PATH_INTEGRATION_MAXPOOL)\n",
    "dfibc = load_ibcorr(PATH_IBCORR)\n",
    "dfibc_m = load_ibcorr(PATH_IBCORR_MAXPOOL)\n",
    "beauty_ratings = load_ratings(PATH_RATINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate ibcorr for each model\n",
    "# dfi_m.unstack([\"class\", \"model\"]).groupby([\"layer\"]).aggregate(\n",
    "#     lambda i: spearmanr(i, beauty_ratings[\"study1_places1_short.csv\"])[0]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subselect data\n",
    "dfi = dfi.sort_index().loc[(slice(None), _dataset, _scale, slice(None), _layers_unblocked)]\n",
    "dfi = dfi.loc[_models] # need this to be seperate so levels drop automatically\n",
    "dfi_m = dfi_m.sort_index().loc[(slice(None), _dataset, _scale, slice(None), _layers_blocked)]\n",
    "dfi_m = dfi_m.loc[_models] # need this to be seperate so levels drop automatically\n",
    "#dfi = dfi.droplevel([\"dataset\",\"scale\"])\n",
    "dfi_m = dfi_m.droplevel([\"dataset\",\"scale\"])\n",
    "\n",
    "dfibc = dfibc.sort_index().loc[(slice(None),_study, _scale, _layers_unblocked)]\n",
    "dfibc = dfibc.loc[_models]\n",
    "dfibc_m = dfibc_m.sort_index().loc[(slice(None),_study, _scale, _layers_blocked)]\n",
    "dfibc_m = dfibc_m.loc[_models]\n",
    "#dfibc = dfibc.droplevel([\"dataset\",\"scale\"])\n",
    "dfibc_m = dfibc_m.droplevel([\"study\",\"scale\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target 01: differences in ib-corr\n",
    "\n",
    "absoloute difference in correlation in each layer, summed up. <br>\n",
    "Optional: Normalized with 2 (spearman correlation range) * num_layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 1: absoloute differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfibc.unstack(\"layer\")\n",
    "rdm = pd.DataFrame(0, index=_models, columns=_models)\n",
    "\n",
    "for c1, c2 in combinations(_models, 2):\n",
    "    # sum of absoloute differences (easier, no correlation of correlated correlation coefficients)\n",
    "    rdm.loc[c1, c2] = (df.loc[c1] - df.loc[c2]).abs().sum()\n",
    "    rdm.loc[c2, c1] = (df.loc[c1] - df.loc[c2]).abs().sum()\n",
    "\n",
    "# norming dissimilarity: sum_abs_diff / (num_layers * 2)\n",
    "sns.heatmap(rdm, xticklabels=rdm.columns, yticklabels=rdm.index)\n",
    "target_rdm = rdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm.loc[c1, c2] = (df.loc[c1] - df.loc[c2]).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfibc_m.unstack(\"layer\")\n",
    "rdm = pd.DataFrame(0, index=_models, columns=_models)\n",
    "\n",
    "for c1, c2 in combinations(_models, 2):\n",
    "    # sum of absoloute differences (easier, no correlation of correlated correlation coefficients)\n",
    "    rdm.loc[c1, c2] = (df.loc[c1] - df.loc[c2]).abs().sum()\n",
    "    rdm.loc[c2, c1] = (df.loc[c1] - df.loc[c2]).abs().sum()\n",
    "\n",
    "# norming dissimilarity: sum_abs_diff / (num_layers * 2)\n",
    "sns.heatmap(rdm, xticklabels=rdm.columns, yticklabels=rdm.index)\n",
    "target_rdm = rdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 2: correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfibc_m.unstack(\"layer\")\n",
    "rdm = pd.DataFrame(0, index=_models, columns=_models)\n",
    "\n",
    "for c1, c2 in combinations(_models, 2):\n",
    "    # sum of absoloute differences (easier, no correlation of correlated correlation coefficients)\n",
    "    rdm.loc[c1, c2] = pearsonr(df.loc[c1], df.loc[c2])[0]\n",
    "    rdm.loc[c2, c1] = pearsonr(df.loc[c1], df.loc[c2])[0]\n",
    "\n",
    "# norming dissimilarity: sum_abs_diff / (num_layers * 2)\n",
    "sns.heatmap(rdm, xticklabels=rdm.columns, yticklabels=rdm.index)\n",
    "target_rdm = rdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor 1: semantic-2d-3d\n",
    "Network grouping according to Radek paper <br>\n",
    "[Finished predictor RDM ](#predictor-rdm-semantic-2d-3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add network classes to data\n",
    "\n",
    "# integration\n",
    "dfi = dfi.loc[NETS_ALL]\n",
    "dfi[\"class\"] = dfi.reset_index(level=\"model\").model.apply(modelname2class).values\n",
    "dfi = dfi.set_index(\"class\", append=True)\n",
    "\n",
    "dfi_m = dfi_m.loc[NETS_ALL]\n",
    "dfi_m[\"class\"] = dfi_m.reset_index(level=\"model\").model.apply(modelname2class).values\n",
    "dfi_m = dfi_m.set_index(\"class\", append=True)\n",
    "\n",
    "\n",
    "\n",
    "# ibcorr\n",
    "dfibc = dfibc.loc[NETS_ALL]\n",
    "dfibc[\"class\"] = dfibc.reset_index(level=\"model\").model.apply(modelname2class).values\n",
    "dfibc = dfibc.set_index(\"class\", append=True)\n",
    "\n",
    "dfibc_m = dfibc_m.loc[NETS_ALL]\n",
    "dfibc_m[\"class\"] = dfibc_m.reset_index(level=\"model\").model.apply(modelname2class).values\n",
    "dfibc_m = dfibc_m.set_index(\"class\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model RDM\n",
    "model_rdm = pd.DataFrame(\n",
    "    np.full((len(NETS_ALL), len(NETS_ALL)), np.nan), columns=NETS_ALL, index=NETS_ALL\n",
    ")\n",
    "\n",
    "for combi in combinations_with_replacement(NETS_ALL, 2):\n",
    "    if (\n",
    "        combi in combinations_with_replacement(NETS_SEMANTIC, 2)\n",
    "        or combi in combinations_with_replacement(NETS_2D, 2)\n",
    "        or combi in combinations_with_replacement(NETS_3D, 2)\n",
    "    ):\n",
    "        model_rdm.loc[combi] = 1\n",
    "        model_rdm.loc[tuple(reversed(combi))] = 1\n",
    "    else:\n",
    "        model_rdm.loc[combi] = 0\n",
    "        model_rdm.loc[tuple(reversed(combi))] = 0\n",
    "\n",
    "sns.heatmap(model_rdm, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance partitioning\n",
    "\n",
    "MODEL RDM as target,  steps as predictors <br>\n",
    "shared variance between any of the models, subsequent added explained variance by each of the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get beauty predictions for images from best predicting model in each layer for each class\n",
    "bestnets = (\n",
    "    dfibc_m.unstack(\"layer\")\n",
    "    .groupby(\"class\")\n",
    "    .aggregate(lambda c: c.idxmax()[0])\n",
    "    .stack(\"layer\")\n",
    "    .rename({\"ibcorr\": \"net\"}, axis=1)\n",
    "    .reset_index()\n",
    "    .values.tolist()\n",
    ")\n",
    "dfi_m_best = (\n",
    "    dfi_m.unstack(\"img\")\n",
    "    .reorder_levels([\"class\", \"layer\", \"model\"])\n",
    "    .loc[bestnets, :]\n",
    "    .droplevel(\"model\")\n",
    "    .stack(\"img\")\n",
    "#    .unstack(\"layer\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi_m_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibcorr_from_classmaxpool = (\n",
    "    dfi_m_best.groupby([\"layer\", \"class\"])\n",
    "    .aggregate(lambda i: pearsonr(-i, beauty_ratings[\"study1_places1_short.csv\"])[0][0])\n",
    "    .unstack(\"class\")\n",
    ")\n",
    "ibcorr_from_classmaxpool.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average beauty prediction (i.e. integration) for each image from each class\n",
    "# since the ib-correlation is the spearman correlation. Just do OLS variance partitioning for now an then talk to daniel about it.\n",
    "ibcorr_from_classmaxpool = (\n",
    "    dfi_m_best.groupby([\"layer\", \"class\"])\n",
    "    .aggregate(lambda i: pearsonr(-i, beauty_ratings[\"study1_places1_short.csv\"])[0][0])\n",
    "    .unstack(\"class\")\n",
    ")\n",
    "ibcorr_from_classmaxpool.plot()\n",
    "\n",
    "ibcorr_from_classavg = (\n",
    "    dfi_m.groupby([\"class\", \"layer\", \"img\"])\n",
    "    .mean()\n",
    "    .groupby([\"layer\", \"class\"])\n",
    "    .aggregate(lambda i: pearsonr(-i, beauty_ratings[\"study1_places1_short.csv\"])[0][0])\n",
    "    .unstack(\"class\")\n",
    ")\n",
    "ibcorr_from_classavg.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi_m_classavg = (\n",
    "    dfi_m.groupby([\"class\", \"layer\", \"img\"])\n",
    "    .mean()\n",
    "    .unstack(\"class\")\n",
    ")\n",
    "dfi_m_classavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beauty_ratings[\"study1_places1_short.csv\"].values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi_m_classavg.loc[:,(slice(None),\"2d\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_all = dfi_m_classavg.groupby([\"layer\"]).apply(\n",
    "    lambda X: OLS(beauty_ratings[\"study1_places1_short.csv\"].values.squeeze(), X.values)\n",
    "    .fit()\n",
    "    .rsquared\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_semantic = r2_all - dfi_m_classavg.loc[:, (slice(None), [\"2d\", \"3d\"])].groupby(\n",
    "    [\"layer\"]\n",
    ").apply(\n",
    "    lambda X: OLS(beauty_ratings[\"study1_places1_short.csv\"].values.squeeze(), X.values)\n",
    "    .fit()\n",
    "    .rsquared\n",
    ")\n",
    "\n",
    "r2_2d = r2_all - dfi_m_classavg.loc[:, (slice(None), [\"semantic\", \"3d\"])].groupby(\n",
    "    [\"layer\"]\n",
    ").apply(\n",
    "    lambda X: OLS(beauty_ratings[\"study1_places1_short.csv\"].values.squeeze(), X.values)\n",
    "    .fit()\n",
    "    .rsquared\n",
    ")\n",
    "\n",
    "\n",
    "r2_3d = r2_all - dfi_m_classavg.loc[:, (slice(None), [\"semantic\", \"2d\"])].groupby(\n",
    "    [\"layer\"]\n",
    ").apply(\n",
    "    lambda X: OLS(beauty_ratings[\"study1_places1_short.csv\"].values.squeeze(), X.values)\n",
    "    .fit()\n",
    "    .rsquared\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r2_3d, label=\"3d\")\n",
    "plt.plot(r2_2d, label=\"2d\")\n",
    "plt.plot(r2_semantic, label=\"semantic\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create linear model\n",
    "# single study & layer\n",
    "dataset = \"places1\"\n",
    "layer_idx = 48\n",
    "\n",
    "df_icr.loc[dataset, layer_idx]\n",
    "\n",
    "OLS(Y, X).fit().rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_variance_partitioning():\n",
    "    # do variance partitioning for one layer\n",
    "    # i.e. for all unique, shared and full combinations of the three predictors\n",
    "    # return dataframe with all R2 values\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## semantic, 2d, 3d along layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw integration amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average integration of layers\n",
    "df_int_netavg = df1.groupby(\"model\").mean().transpose()\n",
    "handles, labels = df_int_netavg.plot().get_legend_handles_labels()\n",
    "\n",
    "# already order legend by classes\n",
    "order = [labels.index(netname) for netname in NETS_ALL]\n",
    "plt.legend(\n",
    "    [handles[idx] for idx in order],\n",
    "    [labels[idx] for idx in order],\n",
    "    loc=\"center right\",\n",
    "    bbox_to_anchor=(1.5, 0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average integration, grouped by semantic-2d-3d\n",
    "colors = (\n",
    "    len(NETS_SEMANTIC) * [\"green\"]\n",
    "    + len(NETS_2D) * [\"purple\"]\n",
    "    + len(NETS_3D) * [\"orange\"]\n",
    ")\n",
    "\n",
    "for (netname, int_netavg), color in zip(df_int_netavg.iloc[:, order].items(), colors):\n",
    "    if netname in NETS_SEMANTIC:\n",
    "        alpha = 0.7\n",
    "    else:\n",
    "        alpha = 0.3\n",
    "    plt.plot(int_netavg, label=netname, color=color, alpha=alpha)\n",
    "    plt.legend(loc=\"center right\", bbox_to_anchor=(1.5, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### integration ordering of images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dev: single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_id = 48\n",
    "\n",
    "# fitler relevant data\n",
    "layer_df = pd.DataFrame(df.loc[NETS_ALL, \"places1\", \"scale8\"][layer_id]).reset_index()\n",
    "# needed for pivot into wide format\n",
    "layer_df[\"img_id\"] = layer_df.groupby(\"model\").cumcount()\n",
    "\n",
    "# pivot\n",
    "layer_df = layer_df.pivot(columns=\"model\", index=\"img_id\", values=layer_id)\n",
    "\n",
    "# reorder columns according to semantic-2D-3D nets\n",
    "layer_df = layer_df[NETS_ALL]\n",
    "\n",
    "rdm = calculate_rdm(layer_df, correlation_type=\"spearman\")\n",
    "\n",
    "pearsonr(rdm.values.flatten(), model_rdm.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(rdm.values.flatten(), model_rdm.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(rdm, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdm = rdm[rdm > 0.142].fillna(0)\n",
    "sns.heatmap(xdm, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdm = rdm[rdm < 0].fillna(0)\n",
    "sns.heatmap(xdm, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_correlations = []\n",
    "model_pvalues = []\n",
    "# iterate layers\n",
    "for layer_name, layer_series in df.loc[:, \"places1\", \"scale8\"].items():\n",
    "\n",
    "    # put data back into DataFrame\n",
    "    layer_df = pd.DataFrame(layer_series).reset_index()\n",
    "\n",
    "    # needed for pivot into wide format\n",
    "    layer_df[\"img_id\"] = layer_df.groupby(\"model\").cumcount()\n",
    "\n",
    "    # pivot\n",
    "    layer_df = layer_df.pivot(columns=\"model\", index=\"img_id\", values=layer_name)\n",
    "\n",
    "    # reorder columns according to semantic-2D-3D nets\n",
    "    layer_df = layer_df[NETS_ALL]\n",
    "\n",
    "    rdm = calculate_rdm(layer_df, correlation_type=\"spearman\")\n",
    "\n",
    "    model_correlations.append(\n",
    "        pearsonr(rdm.values.flatten(), model_rdm.values.flatten())[0]\n",
    "    )\n",
    "    model_pvalues.append(pearsonr(rdm.values.flatten(), model_rdm.values.flatten())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "sns.lineplot(data=model_correlations)\n",
    "plt.suptitle(\"Similarity in what is integrated\")\n",
    "plt.title(\"Correlation of taskonomy RDM with model (semantic-2D-3D) RDM\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"pearson correlation\")\n",
    "\n",
    "\n",
    "for x, layer_pvalue in enumerate(model_pvalues):\n",
    "    if layer_pvalue < alpha:\n",
    "        plt.scatter(x, 0, color=\"cyan\", s=100, marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ibcorr differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inspect class average integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per class best and worst prediction from individual nets\n",
    "Y1 = dfibc_m.groupby([\"layer\", \"class\"]).agg([min, max])\n",
    "Y2 = (\n",
    "    dfi_m.groupby([\"class\", \"layer\", \"img\"])\n",
    "    .mean()\n",
    "    .groupby([\"layer\", \"class\"])\n",
    "    .aggregate(lambda i: spearmanr(i, beauty_ratings[\"study1_places1_short.csv\"])[0])\n",
    "    .unstack(\"class\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1,3, figsize=(12, 3), sharey=True)\n",
    "\n",
    "modelclass = \"semantic\"\n",
    "axes[0].fill_between(\n",
    "    Y1.loc[(slice(None), modelclass), :].index.get_level_values(\"layer\"),\n",
    "    Y1.loc[(slice(None), modelclass), (slice(None), \"min\")].values.flat,\n",
    "    Y1.loc[(slice(None), modelclass), (slice(None), \"max\")].values.flat,\n",
    ")\n",
    "axes[0].plot(-Y2.loc[:, (slice(None), modelclass)], c=\"red\")\n",
    "\n",
    "modelclass = \"2d\"\n",
    "axes[1].fill_between(\n",
    "    Y1.loc[(slice(None), modelclass), :].index.get_level_values(\"layer\"),\n",
    "    Y1.loc[(slice(None), modelclass), (slice(None), \"min\")].values.flat,\n",
    "    Y1.loc[(slice(None), modelclass), (slice(None), \"max\")].values.flat,\n",
    ")\n",
    "axes[1].plot(-Y2.loc[:, (slice(None), modelclass)], c=\"red\")\n",
    "\n",
    "modelclass = \"3d\"\n",
    "axes[2].fill_between(\n",
    "    Y1.loc[(slice(None), modelclass), :].index.get_level_values(\"layer\"),\n",
    "    Y1.loc[(slice(None), modelclass), (slice(None), \"min\")].values.flat,\n",
    "    Y1.loc[(slice(None), modelclass), (slice(None), \"max\")].values.flat,\n",
    ")\n",
    "axes[2].plot(-Y2.loc[:, (slice(None), modelclass)], c=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictor RDM (semantic-2d-3d)\n",
    "\n",
    "Extend [model rdm](#model-rdm) to contain all models to use as predictor. Fill values for models not belonging to any class with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_rdm_1 = pd.DataFrame(0, index=_models, columns=_models)\n",
    "for c1, c2 in combinations_with_replacement(_models, 2):\n",
    "    if c1 in model_rdm.index and c2 in model_rdm.index:\n",
    "        predictor_rdm_1.loc[c1, c2] = 1 - model_rdm.loc[c1, c2]\n",
    "        predictor_rdm_1.loc[c2, c1] = 1 - model_rdm.loc[c2, c1]\n",
    "    else:\n",
    "        predictor_rdm_1.loc[c1, c2] = 0\n",
    "        predictor_rdm_1.loc[c2, c1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor 2:  integration profile across layers\n",
    "\n",
    "RDM of RDM's that correlate integration ratings of each different layers inside each network.\n",
    "\n",
    "[Finished predictor RDM](#predictor-rdm-layer-layer-similarity-inside-networks)\n",
    "\n",
    "TODO: this essentially the same thing as absoloute correlation differences alone ?\n",
    "copy code for each models layerXlayer RDM\n",
    "correlate correlations using daniels code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 1: layer X layer RDM for each network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfi.loc[(slice(None), slice(None), _scale, slice(None)), :].droplevel([\"scale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_rdms_p1, v1_rdms_p2, v1_rdms_oa = {}, {}, {}\n",
    "for mo, df_mo in df1.groupby(\"model\"):\n",
    "    v1_rdms_p1[mo] = calculate_rdm(df_mo.loc[(slice(None), \"places1\"), :], \"spearman\")\n",
    "    v1_rdms_p2[mo] = calculate_rdm(df_mo.loc[(slice(None), \"places2\"), :], \"spearman\")\n",
    "    v1_rdms_oa[mo] = calculate_rdm(df_mo.loc[(slice(None), \"oasis\"), :], \"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join(PATH_RESULTS, \"layer profile\", \"version 1\")\n",
    "with open(os.path.join(PATH, \"rmds places1.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(v1_rdms_p1, f)\n",
    "\n",
    "with open(os.path.join(PATH, \"rmds places2.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(v1_rdms_p2, f)\n",
    "\n",
    "with open(os.path.join(PATH, \"rmds oasis.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(v1_rdms_oa, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join(PATH_RESULTS, \"layer profile\", \"version 1\")\n",
    "with open(os.path.join(PATH, \"rmds places1.pkl\"), \"rb\") as f:\n",
    "    v1_rdms_p1 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PATH, \"rmds places2.pkl\"), \"rb\") as f:\n",
    "    v1_rdms_p2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PATH, \"rmds oasis.pkl\"), \"rb\") as f:\n",
    "    v1_rdms_oa = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 2: integration in best layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ordering of images by integration in best predicting layer\n",
    "\n",
    "\"what is integrated\", alternatively average of correlation between in each layer, howevery layers may not correspond to each other, therefore best predicting layer is more general <br> <br>\n",
    "\n",
    "Interpretation: The differences in absolout values correspond to how similar the \"integration mechanism\" in both networks are.<br> If we assume that beauty perception depends on a specific stage of processing and not the whole processing stream, then the best predicting layer of a network can be interpreted as the point, where the network best mimics the aspects of the processing that determine beauty. <br> \n",
    "\n",
    "If the a similar The value in Is there a single or are there different ways of predicting beauty ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best layer per model\n",
    "bestlayers = dfibc.groupby(\"model\").idxmax().ibcorr\n",
    "df_bestpredicting_integration = (\n",
    "    dfi.unstack(\"img\").loc[bestlayers].droplevel(\"layer\").T.droplevel(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictor RDM: correlate network RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "# RDMs into columns\n",
    "# throw out zeros on diagonal to avoid skewing correlation (standard RSA procedure)\n",
    "RDMs_places1 = pd.DataFrame(columns=_models)\n",
    "\n",
    "for network_name, rdm in v1_rdms_p1.items():\n",
    "    # mark diagonal values (all zeros)for removal\n",
    "    np.fill_diagonal(rdm.values, np.nan)\n",
    "    RDMs_places1.loc[:, network_name] = rdm.values.flatten()\n",
    "\n",
    "# removed marked diagonal values\n",
    "RDMs_places1 = RDMs_places1.dropna()\n",
    "\n",
    "print(\"Should be (2353, 15)\")\n",
    "print(RDMs_places1.shape)\n",
    "\n",
    "predictor_rdm_2 = calculate_rdm(RDMs_places1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2\n",
    "predictor_rdm_2 = calculate_rdm(df_bestpredicting_integration, \"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor 3: spatial integration\n",
    "\n",
    "\"how\"\n",
    "\n",
    "\"where\" or alternatively \"what\",  which is the same because its spatial integration. Check for correlation between the what (represented by the integration ratings).\n",
    "\n",
    "DONE IN SEPERATE NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import results\n",
    "with open(os.path.join(PATH_RESULTS, \"spatial integration\", \"study1.pkl\"), \"rb\") as f:\n",
    "    d1 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PATH_RESULTS, \"spatial integration\", \"study2.pkl\"), \"rb\") as f:\n",
    "    d2 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PATH_RESULTS, \"spatial integration\", \"study3.pkl\"), \"rb\") as f:\n",
    "    d3 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PATH_RESULTS, \"spatial integration\", \"study4.pkl\"), \"rb\") as f:\n",
    "    d4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm_study1 = calculate_rdm(flatten_concat(d1))\n",
    "rdm_study2 = calculate_rdm(flatten_concat(d2))\n",
    "rdm_study3 = calculate_rdm(flatten_concat(d3))\n",
    "rdm_study4 = calculate_rdm(flatten_concat(d4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot RDMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(rdm_study1, xticklabels=rdm_study1.index, yticklabels=rdm_study1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(rdm_study2, xticklabels=rdm_study1.index, yticklabels=rdm_study1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(rdm_study3, xticklabels=rdm_study1.index, yticklabels=rdm_study1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(rdm_study4, xticklabels=rdm_study1.index, yticklabels=rdm_study1.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## integration is localized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize node score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize within layer heatmaps\n",
    "\n",
    "# exemplars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial correlation per image per net, correlate these netXnet\n",
    "# test if integration scores are still correlating to beauty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predctor RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_rdm_3 = rdm_study1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM (Predictors, Target)\n",
    "do for each study and each scale, to check if there is some consistency in which factors always comes out on top\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make sure all matrices are either similarity or dissimilarity matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor 1 alone\n",
    "pearsonr(target_rdm.values.flatten(), predictor_rdm_1.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor 2 alone\n",
    "pearsonr(target_rdm.values.flatten(), predictor_rdm_2.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor 3 alone\n",
    "pearsonr(target_rdm.values.flatten(), predictor_rdm_3.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance partitioning between different predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beauty_ratings[\"study1_places1_short.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dfi_m.loc[(\"edge_occlusion\", \"places1\", \"scale4\",slice(None), 10)].rank(), beauty_ratings[\"study1_places1_short.csv\"].rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi_m.loc[\"edge_occlusion\"].groupby([\"dataset\",\"scale\", \"layer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rdm.values.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, len(MODEL_NAMES) ** 2, len(MODEL_NAMES) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(target_rdm.values.flatten(), predictor_rdm_2.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(target_rdm.values.flatten(), predictor_rdm_2.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            target_rdm.values.flatten(),\n",
    "            predictor_rdm_1.values.flatten(),\n",
    "            predictor_rdm_2.values.flatten(),\n",
    "#            predictor_rdm_3.values.flatten(),\n",
    "        ],\n",
    "        index=[\"target\", \"P1\", \"P2\"],\n",
    "    )\n",
    "    .drop(columns=np.arange(0, len(MODEL_NAMES) ** 2, len(MODEL_NAMES) + 1))\n",
    "    .T\n",
    ").rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, target = df.loc[:,[\"P1\",\"P2\"]].values, df.target.values\n",
    "predictors = sm.add_constant(predictors)\n",
    "model = sm.OLS(target, predictors)\n",
    "results = model.fit()\n",
    "r2_p1_p2 = results.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors1, target = df.loc[:,[\"P1\"]].values.squeeze(), df.target.values.squeeze()\n",
    "#predictors1 = sm.add_constant(predictors1)\n",
    "model = sm.OLS(target, predictors1)\n",
    "results = model.fit()\n",
    "\n",
    "r2_p2 = r2_p1_p2 - results.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors2, target = df.loc[:,[\"P2\"]].values.squeeze(), df.target.values.squeeze()\n",
    "#predictors2 = sm.add_constant(predictors2)\n",
    "model = sm.OLS(target, predictors2)\n",
    "results = model.fit()\n",
    "\n",
    "r2_p1 = r2_p1_p2 - results.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = results.predict(predictors.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(predictors1, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predictors1, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, target = df.loc[:,[\"P1\"]].values, df.target.values\n",
    "#predictors = sm.add_constant(predictors)\n",
    "model = sm.OLS(target, predictors)\n",
    "results = model.fit()\n",
    "r2_p2 = r2_p1_p2 - results.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_p1_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_p2 / r2_p1_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_p1/ r2_p1_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(df.target, df.P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(df.target, df.P2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(df.target, df.P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(df.P1, df.P3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(predictor_rdm_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(predictor_rdm_3.values.flat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
