{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing script: analyse_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# modified visualpriors library\n",
    "from transforms import VisualPriorRepresentation\n",
    "\n",
    "from classes_analyse_nodes import (\n",
    "    ImageDataset,\n",
    "    Pattern_Generator,\n",
    "    Activation_Pattern,\n",
    "    #NetworkScorer,\n",
    "    calculate_integration_coeff,\n",
    "    taskonomy_activation_layer_shapes,\n",
    "    )\n",
    "\n",
    "import torch\n",
    "import torch.utils.model_zoo # required to load nets\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = ('places1', 'places2', 'oasis')\n",
    "SCALE_NAMES = ('scale2','scale4','scale8','scale16','scale32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data_256x256'\n",
    "BEHAVIOR_PATH = './behavior'\n",
    "RESULTS_PATH = './results_taskonomy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VisualPrior.viable_feature_tasks\n",
    "MODEL_NAMES = ('autoencoding','depth_euclidean','jigsaw','reshading',\n",
    "               'edge_occlusion','keypoints2d','room_layout', #'colorization' currently not working\n",
    "               'curvature','edge_texture','keypoints3d','segment_unsup2d',\n",
    "               'class_object','egomotion','nonfixated_pose','segment_unsup25d',\n",
    "               'class_scene','fixated_pose','normal','segment_semantic',\n",
    "               'denoising','inpainting','point_matching','vanishing_point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_TRANSFORMS = ('untransformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing a NetworkScorer object for saving backprojected scores\n",
    "# TODO needs to be adapted when analysing multiple nets\n",
    "BACKPROJECTED_SCORES_FOLDER = './backprojected_scores'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration for node subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup net\n",
    "model_name = MODEL_NAMES[18] #segment semantic\n",
    "VisualPriorRepresentation._load_unloaded_nets([model_name])\n",
    "net = VisualPriorRepresentation.feature_task_to_net[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup activation extractor\n",
    "_, eval_nodes = get_graph_node_names(net)\n",
    "return_nodes = { node:node for node in eval_nodes if \"conv\" in node or 'fc' in node}\n",
    "activation_extractor = create_feature_extractor(net, return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape of activation\n",
    "dataset = ImageDataset(os.path.join(DATA_PATH, DATASET_NAMES[0], SCALE_NAMES[0]))\n",
    "dummy_image = next(iter(dataset))\n",
    "net_activation = activation_extractor(dummy_image[0])\n",
    "net_activation = OrderedDict(net_activation)\n",
    "\n",
    "activation_shapes = taskonomy_activation_layer_shapes(net_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 250, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_subsets = 10 #10 000\n",
    "num_layers = activation_shapes.__len__()\n",
    "num_images = dataset.img_count\n",
    "\n",
    "pat = Pattern_Generator(\n",
    "    num_subsets,\n",
    "    activation_shapes,\n",
    "    frac=0.01\n",
    "    )\n",
    "\n",
    "# layer x image x subset\n",
    "integration = np.full([num_layers,num_images,num_subsets], np.nan)\n",
    "integration.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30 60 90 120 150 180 210 240 "
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "\n",
    "# iterate image set\n",
    "for img_id, (img_full, img_v1, img_v2) in enumerate(iter(dataset)):\n",
    "    if cnt % 30 == 0: print(cnt, end= ' ')\n",
    "    cnt=cnt+1\n",
    "    \n",
    "    act_full = Activation_Pattern(activation_extractor(img_full))\n",
    "    act_v1 = Activation_Pattern(activation_extractor(img_v1))\n",
    "    act_v2 = Activation_Pattern(activation_extractor(img_v2))\n",
    "    act_avg = Activation_Pattern.average(act_v1, act_v2)\n",
    "\n",
    "    # iterate node subsets\n",
    "    for subset_num in range(num_subsets):\n",
    "        subset_mask = pat.get_subset_pattern(subset_num)\n",
    "        \n",
    "        subset_act_full = act_full[subset_mask]\n",
    "        subset_act_avg = act_avg[subset_mask]\n",
    "\n",
    "        # calculate integration and store it\n",
    "        subset_integration = calculate_integration_coeff(subset_act_full, subset_act_avg)\n",
    "        integration[:,img_id, subset_num] = subset_integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score (correlate) integration-beauty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load beauty ratings\n",
    "beauty_ratings = ImageDataset(\n",
    "    os.path.join(DATA_PATH, DATASET_NAMES[0], SCALE_NAMES[0]),\n",
    "    beauty_ratings_path='behavior/ratings_study1.csv').beauty_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_integration_beauty(integration: np.ndarray, beauty_ratings: pd.Series):\n",
    "    return np.apply_along_axis(lambda c: spearmanr(c, beauty_ratings)[0], 1, integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = correlate_integration_beauty(integration, beauty_ratings)\n",
    "scores = np.abs(correlations)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map back scores onto nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49, 10), numpy.ndarray)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer x subset\n",
    "scores.shape, type(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NetworkScorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ns \u001b[38;5;241m=\u001b[39m \u001b[43mNetworkScorer\u001b[49m(activation_shapes)\n\u001b[1;32m      2\u001b[0m ns\u001b[38;5;241m.\u001b[39mmap_back_scores(scores, pat)\n\u001b[1;32m      3\u001b[0m ns\u001b[38;5;241m.\u001b[39msubset_iterations_count\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NetworkScorer' is not defined"
     ]
    }
   ],
   "source": [
    "ns = NetworkScorer(activation_shapes)\n",
    "ns.map_back_scores(scores, pat)\n",
    "ns.subset_iterations_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save NetworkScorer\n",
    "ns.save(BACKPROJECTED_SCORES_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load NetworkScorer\n",
    "loaded_ns = NetworkScorer.load('./backprojected_scores')\n",
    "ns = loaded_ns\n",
    "len(ns.scores.keys()), ns.subset_iterations_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare how much storing and loading image activations is to recalculating activations\n",
    "# implement storing and loading image activations\n",
    "\n",
    "\n",
    "IMG_ACTIVATIONS_FOLDER = './saved_image_activations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single image scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing a NetworkScorer object for saving backprojected scores\n",
    "# TODO needs to be adapted when analysing multiple nets\n",
    "BACKPROJECTED_SCORES_FOLDER = './backprojected_scores_img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image with high integration rating\n",
    "\n",
    "# calculate integration for every image in the dnn\n",
    "integration = np.full([dataset.img_count,], np.nan)\n",
    "\n",
    "for img_id, (img_full, img_v1, img_v2) in enumerate(iter(dataset)):\n",
    "    act_full = Activation_Pattern(activation_extractor(img_full))\n",
    "    act_v1 = Activation_Pattern(activation_extractor(img_v1))\n",
    "    act_v2 = Activation_Pattern(activation_extractor(img_v2))\n",
    "    act_avg = Activation_Pattern.average(act_v1, act_v2)\n",
    "\n",
    "    i = calculate_integration_coeff(act_full, act_avg)\n",
    "    integration[img_id] = i.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_integration_img_idx = integration.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Places365_val_00001647.mat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.img_list[highest_integration_img_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image with high integration rating\n",
    "\n",
    "# calculate integration for every image in the dnn\n",
    "integration = np.full([dataset.img_count,], np.nan)\n",
    "\n",
    "for img_id, (img_full, img_v1, img_v2) in enumerate(iter(dataset)):\n",
    "    act_full = Activation_Pattern(activation_extractor(img_full))\n",
    "    act_v1 = Activation_Pattern(activation_extractor(img_v1))\n",
    "    act_v2 = Activation_Pattern(activation_extractor(img_v2))\n",
    "    act_avg = Activation_Pattern.average(act_v1, act_v2)\n",
    "\n",
    "    i = calculate_integration_coeff(act_full, act_avg)\n",
    "    integration[img_id] = i.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subsets = 10 #10 000\n",
    "num_layers = activation_shapes.__len__()\n",
    "num_images = dataset.img_count\n",
    "\n",
    "pat = Pattern_Generator(\n",
    "    num_subsets,\n",
    "    activation_shapes,\n",
    "    frac=0.01\n",
    "    )\n",
    "\n",
    "# layer x image x subset\n",
    "integration = np.full([num_layers,num_images,num_subsets], np.nan)\n",
    "integration.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# track convergence of network scores\n",
    "for now just do 10 000 iterations and already save steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store progression on network scores, by saving NetworScorer every 10/100/200 subsets\n",
    "\n",
    "# ideas: normalize scores and track convergence of these scores onto stable values\n",
    "    # maybe need high numeric accuracy for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyse influence of frac\n",
    "frac represents the fraction (0, 1] of nodes in a subset of the total number of nodes in each network layer\n",
    "\n",
    "- initially determine a reasonable frac size for a initial analysis\n",
    "- later run whole analysis again for different frac sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare different choices for frac empirically, plot different subsets together and make a plot with variance bars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a frac of .33 , the results for the different subsets is nearly always identical, due to the law of large numbers.\n",
    "\n",
    "(It may be possible to estimate a reasonable subset size with Tschebytschev to estimate how much variance there is with )different subset sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mean = scores.mean(axis=1)\n",
    "scores_std = scores.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mean = pd.Series(scores.mean(axis=1))\n",
    "scores_std = pd.Series(scores.std(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(range(49), ib_corr_mean, ib_corr_std, ecolor='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 6\n",
    "plt.errorbar(range(49),\n",
    "             ib_corr_mean.rolling(window).mean(),\n",
    "             yerr=ib_corr_std.rolling(window).mean(),\n",
    "             ecolor='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run analysis for different values of frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_values = [0.0001, 0.0005, 0.001, 0.005,  0.01, 0.05, 0.1, 0.33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_subset_analysis(num_subsets:int, frac: float) -> np.ndarray:\n",
    "    \"\"\"Run analysis correlating integration with beauty for n subsets\n",
    "    with a give  subset fraction (of total network nodes).\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subset_correlation_std_dev(results: float) -> None:\n",
    "    pass\n",
    "    #run analysis\n",
    "    #reshape\n",
    "    #plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance bars for different frac sizes\n",
    "visualize variance of subset ib-correlations in different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Result: layer x subset matrix\"\"\"\n",
    "for subset_num in range(num_subsets):\n",
    "    integration = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look if results generalize to different frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dismissed: save and load images instead of recalculating activation every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## determine if saving and loading is feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sys.getsizeof(act_full.activation_pattern['conv1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.194304"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "act_full.activation_pattern['conv1'].element_size() * act_full.activation_pattern['conv1'].nelement()/1E6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size in MB:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48.627712"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate activation pattern size\n",
    "layer_sizes = np.array([t.element_size()*t.nelement() for t in act_full.activation_pattern.values()])\n",
    "print(\"Total size in MB:\")\n",
    "layer_sizes.sum()/1E6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes saving and loading look possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate image set\n",
    "for img_id, (img_full, img_v1, img_v2) in enumerate(iter(dataset)):\n",
    "    \n",
    "    act_full = Activation_Pattern(activation_extractor(img_full))\n",
    "    act_v1   = Activation_Pattern(activation_extractor(img_v1))\n",
    "    act_v2   = Activation_Pattern(activation_extractor(img_v2))\n",
    "    act_avg  = Activation_Pattern.average(act_v1, act_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate image set\n",
    "for img_id, (img_full, img_v1, img_v2) in enumerate(iter(dataset)):\n",
    "\n",
    "    with open(os.path.join(IMG_ACTIVATIONS_FOLDER, str(img_id) + '.pkl'), 'rb') as file:\n",
    "    act_full = pickle.load(file)\n",
    "\n",
    "    with open(os.path.join(IMG_ACTIVATIONS_FOLDER, str(img_id) + '.pkl'), 'rb') as file:\n",
    "    act_avg = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(IMG_ACTIVATIONS_FOLDER, str(img_id) + '.pkl'), 'wb') as file:\n",
    "    pickle.dump(act_full, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(IMG_ACTIVATIONS_FOLDER, str(img_id) + '.pkl'), 'rb') as file:\n",
    "    act_full_loaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare time to recalculating every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "for img_id, (img_full, img_v1, img_v2) in enumerate(iter(dataset)):\n",
    "    \n",
    "    act_full = Activation_Pattern(activation_extractor(img_full))\n",
    "    act_v1   = Activation_Pattern(activation_extractor(img_v1))\n",
    "    act_v2   = Activation_Pattern(activation_extractor(img_v2))\n",
    "    act_avg  = Activation_Pattern.average(act_v1, act_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new\n",
    "for img_id in range(250):\n",
    "\n",
    "    with open(os.path.join(IMG_ACTIVATIONS_FOLDER, str(249) + '.pkl'), 'rb') as file:\n",
    "        act_full = pickle.load(file)\n",
    "\n",
    "    with open(os.path.join(IMG_ACTIVATIONS_FOLDER, str(img_id) + '.pkl'), 'rb') as file:\n",
    "        act_avg = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
