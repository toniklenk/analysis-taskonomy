{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse individual integration values of each image in each layer of each network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple regression in each layer partitioning variance by three groups of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict beauty from integration in 23 taskonomy models. Do variance partitioning.\n",
    "\n",
    "Problem: Multicolinearity of individual predictors, therefore, first examing correlation of integration between different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from itertools import combinations_with_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = ('places1', 'places2', 'oasis')\n",
    "SCALE_NAMES = ('scale2','scale4','scale8','scale16','scale32')\n",
    "\n",
    "DATA_PATH = './data_256x256'\n",
    "BEHAVIOR_PATH = './behavior'\n",
    "RESULTS_PATH = './results_taskonomy'\n",
    "\n",
    "#VisualPrior.viable_feature_tasks\n",
    "MODEL_NAMES = ('autoencoding','depth_euclidean','jigsaw','reshading',\n",
    "               'edge_occlusion','keypoints2d','room_layout', #'colorization' currently not working\n",
    "               'curvature','edge_texture','keypoints3d','segment_unsup2d',\n",
    "               'class_object','egomotion','nonfixated_pose','segment_unsup25d',\n",
    "               'class_scene','fixated_pose','normal','segment_semantic',\n",
    "               'denoising','inpainting','point_matching','vanishing_point')\n",
    "\n",
    "IMAGE_TRANSFORMS = ('untransformed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETS_SEMANTIC = ['class_object','class_scene','segment_semantic']\n",
    "\n",
    "# from radek paper missing: colorization (not downloadable from taskonomy)\n",
    "NETS_2D = ['autoencoding','denoising','edge_texture','inpainting','keypoints2d','segment_unsup2d']\n",
    "\n",
    "# from radek paper missing: z-depth (missing from importing as well) and distance (but this is not a network after all)\n",
    "NETS_3D = ['edge_occlusion','keypoints3d','segment_unsup25d','reshading','normal','curvature']\n",
    "\n",
    "NETS_ALL = NETS_SEMANTIC + NETS_2D + NETS_3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load integration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "data_list = []\n",
    "\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    for dataset_name in DATASET_NAMES:\n",
    "        for scale_name in SCALE_NAMES:\n",
    "\n",
    "            data = pd.read_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'correlations.csv'), header=None)\n",
    "            data.insert(0, 'scale', scale_name)\n",
    "            data.insert(0, 'dataset',dataset_name)\n",
    "            data.insert(0, 'model', model_name)\n",
    "\n",
    "            data_list.append(data)\n",
    "            #selfsimilarity.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'selfsimilarity.csv'), index=False, header=False)           \n",
    "            #l2norm.to_csv(os.path.join(RESULTS_PATH, model_name, dataset_name, scale_name, 'l2norm.csv'), index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert correlation to integration\n",
    "df = - pd.concat(data_list).set_index(['model','dataset','scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles, labels = df.loc['autoencoding'].groupby('scale').mean().transpose().plot().get_legend_handles_labels()\n",
    "order = [1, 3, 4, 0, 2]\n",
    "plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function for calculating RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rdm(data: pd.DataFrame, correlation_type : str = \"pearson\"):\n",
    "    \"\"\"Calculate RDM with pearson/spearman correlation for every combination of columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "        Input with data to correlate in the columns\n",
    "\n",
    "    correlation_type: str\n",
    "        Which correlation to use. \"pearson\" (default) or \"spearman\".\n",
    "\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        representational dissimilarity matrix of inputs' columns\n",
    "    \n",
    "    \"\"\"\n",
    "    num_columns = data.shape[1]\n",
    "\n",
    "    # create empty matrix to store RDM\n",
    "    # index and column labels are in order of input columns\n",
    "    rdm = pd.DataFrame(np.full((num_columns, num_columns), np.nan), columns=data.columns, index=data.columns)\n",
    "    \n",
    "    for col1, col2 in combinations_with_replacement(data.columns, 2):\n",
    "        # there's one NaN in the autoencoding integration values, filter this here, don't know why that happens\n",
    "        co11_col2 = data[[col1,col2]].dropna()\n",
    "        \n",
    "        # calculate correlation\n",
    "        if correlation_type == \"pearson\":\n",
    "            corr = pearsonr(co11_col2.values[:,0], co11_col2.values[:,1])[0]\n",
    "        elif correlation_type == \"spearman\":\n",
    "            corr = spearmanr(co11_col2.values[:,0], co11_col2.values[:,1])[0]\n",
    "\n",
    "        # fill upper and lower triangular matrix\n",
    "        rdm.loc[col1, col2] = corr\n",
    "        rdm.loc[col2, col1] = corr\n",
    "        rdm.loc[col1, col1] = 0.0\n",
    "\n",
    "    return rdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do different nets integrate different things ?\n",
    "\n",
    "correlate integration of images between individual networks and groups of networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot representational dissimilarity matrix of network by network. correlating the integration to the image dataset. see if there's a clustering into the three groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model RDM for semantiv-2D-3D nets integration\n",
    "model_rdm = pd.DataFrame(\n",
    "        np.full((len(NETS_ALL), len(NETS_ALL)), np.nan),\n",
    "        columns=NETS_ALL, index=NETS_ALL)\n",
    "\n",
    "for combi in combinations_with_replacement(NETS_ALL,2):\n",
    "    if combi in combinations_with_replacement(NETS_SEMANTIC,2) or \\\n",
    "        combi in combinations_with_replacement(NETS_2D,2) or \\\n",
    "        combi in combinations_with_replacement(NETS_3D,2):\n",
    "        model_rdm.loc[combi] = 1\n",
    "        model_rdm.loc[tuple(reversed(combi))] = 1\n",
    "    else:\n",
    "        model_rdm.loc[combi] = 0\n",
    "        model_rdm.loc[tuple(reversed(combi))] = 0\n",
    "\n",
    "sns.heatmap(model_rdm, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_id = 48\n",
    "\n",
    "# fitler relevant data\n",
    "layer_df = pd.DataFrame(df.loc[NETS_ALL,\"places1\", \"scale8\"][layer_id]).reset_index()\n",
    "# needed for pivot into wide format\n",
    "layer_df[\"img_id\"] = layer_df.groupby(\"model\").cumcount()\n",
    "\n",
    "# pivot\n",
    "layer_df = layer_df.pivot(columns=\"model\", index=\"img_id\", values=layer_id)\n",
    "\n",
    "# reorder columns according to semantic-2D-3D nets\n",
    "layer_df = layer_df[NETS_ALL]\n",
    "\n",
    "rdm = calculate_rdm(layer_df, correlation_type=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(rdm.values.flatten(), model_rdm.values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visually inspect RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(rdm, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdm = rdm.abs()[rdm.abs() > .2095].fillna(0)\n",
    "sns.heatmap(xdm, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdm = rdm[rdm > .142].fillna(0)\n",
    "sns.heatmap(xdm, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdm = rdm[rdm < 0].fillna(0)\n",
    "sns.heatmap(xdm, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all layers\n",
    "\n",
    "loop layers, scales, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_correlations = []\n",
    "model_pvalues = []\n",
    "# iterate layers\n",
    "for layer_name, layer_series in df.loc[:,\"places1\", \"scale8\"].items():\n",
    "\n",
    "    # put data back into DataFrame\n",
    "    layer_df = pd.DataFrame(layer_series).reset_index()\n",
    "\n",
    "    # needed for pivot into wide format\n",
    "    layer_df[\"img_id\"] = layer_df.groupby(\"model\").cumcount()\n",
    "\n",
    "    # pivot\n",
    "    layer_df = layer_df.pivot(columns=\"model\", index=\"img_id\", values=layer_name)\n",
    "\n",
    "    # reorder columns according to semantic-2D-3D nets\n",
    "    layer_df = layer_df[NETS_ALL]\n",
    "\n",
    "    rdm = calculate_rdm(layer_df, correlation_type=\"spearman\")\n",
    "\n",
    "    model_correlations.append(pearsonr(rdm.values.flatten(), model_rdm.values.flatten())[0])\n",
    "    model_pvalues.append(pearsonr(rdm.values.flatten(), model_rdm.values.flatten())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "sns.lineplot(data=model_correlations)\n",
    "plt.suptitle(\"Similarity in what is integrated\")\n",
    "plt.title(\"Correlation of taskonomy RDM with model (semantic-2D-3D) RDM\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"pearson correlation\")\n",
    "\n",
    "\n",
    "for x, layer_pvalue in enumerate(model_pvalues):\n",
    "    if layer_pvalue < alpha:\n",
    "        plt.scatter(x, 0, color='cyan', s=100, marker='o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# integration profiles across layers\n",
    "\n",
    "for each network, perform layer x layer RSA of integration scores\n",
    "\n",
    "test each rdm for sigificance by testing against a permutation distribution with shuffeled integration values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PdfPages(\"Integratio_layer_RSA.pdf\") as pdf:\n",
    "    for network_name in NETS_ALL:\n",
    "\n",
    "        rdm = calculate_rdm(df.loc[network_name,'places1','scale8'])\n",
    "        sns.heatmap(rdm)\n",
    "\n",
    "        \n",
    "        if network_name in NETS_SEMANTIC:\n",
    "            network_class = \"(semantic)\"\n",
    "        elif network_name in NETS_2D:\n",
    "            network_class = \"(2d)\"\n",
    "        elif network_name in NETS_3D:\n",
    "            network_class = \"(3d)\"\n",
    "\n",
    "        plt.title(network_name + network_class)\n",
    "        \n",
    "        \n",
    "        pdf.savefig()\n",
    "        plt.close()      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do different scales of checkerboard patterns lead to integration of different things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
